{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Bash Script Collection","text":"<p>This guide introduces:  </p>"},{"location":"#basic-bash-commands","title":"Basic Bash Commands:","text":"<p>\u2022   Basic Bash Commands \u2022   File Management \u2022   Text Processing \u2022   Networking \u2022   Process Management </p>"},{"location":"#data-organisation","title":"Data Organisation","text":"<p>\u2022   Organise your data \u2022   Start Using Git \u2022   Advanced Git \u2022   CI/CD </p>"},{"location":"#fastq-files-treatment","title":"Fastq Files Treatment","text":"<p>\u2022   The Fastq Format \u2022   Renaming FASTQ Files Downloaded from the Short Read Archive \u2022   Fastq Quality Control with FastQC \u2022   Fastq Contamination Checking with Fastq-screen \u2022   Reads Trimming with Trimmomatic  \u2022   Reads Mapping with BWA </p>"},{"location":"#sambam-files","title":"SAM/BAM Files","text":"<p>\u2022   The SAM/BAM Format \u2022   Sorting a SAM/BAM File \u2022   How to Add or Replace ReadGroup \u2022   Deduplication \u2022   Merge and deduplicate Bam Files \u2022   Indel Realignment \u2022   Polishing Bam Files  \u2022   Bam Quality Control \u2022   Variant-Calling \u2022   GenomicsDBImport and GenotypeGVCFs in GATK4 Workflow </p>"},{"location":"#vcf-files","title":"VCF Files","text":"<p>\u2022   The VCF Format \u2022   Filtering a VCF File by Accessibility \u2022   Filtering a VCF File by Homopolymer Runs \u2022   Filtering Variants &amp; Invariants Sites \u2022   Filtering by Genotype Quality \u2022   VCF concatenation \u2022   Filtering by GT and Individuals </p>"},{"location":"Git/","title":"Git Version Control Guide","text":""},{"location":"Git/#introduction","title":"IntroductionTable of Content","text":"<p>Git is a distributed version control system that tracks changes in files and coordinates work among multiple people. Think of it as a sophisticated \"save\" system that keeps track of every change you make to your project files, allowing you to go back to previous versions, compare changes, and collaborate with others safely.</p> <p>This guide will teach you Git from the ground up, assuming you've never used version control before.</p> <ul> <li>Git Version Control Guide<ul> <li>Introduction</li> <li>Why Use Git?</li> <li>Key Concepts</li> <li>Installation and Setup</li> <li>Basic Git Workflow</li> <li>Working with Files</li> <li>Branching and Merging</li> <li>Remote Repositories</li> <li>Practical Examples</li> <li>Best Practices</li> <li>Common Commands Reference</li> <li>Troubleshooting Common Issues</li> <li>Next Steps</li> <li>Summary</li> </ul> </li> </ul>"},{"location":"Git/#why-use-git","title":"Why Use Git?","text":"<p>Imagine working on a project and accidentally deleting important code, or wanting to try a risky change without losing your working version. Git solves these problems by:</p> <ul> <li>Tracking all changes to your files over time</li> <li>Allowing you to revert to any previous version</li> <li>Enabling collaboration with multiple developers</li> <li>Backing up your work across multiple locations</li> <li>Managing different versions of your project simultaneously</li> </ul>"},{"location":"Git/#key-concepts","title":"Key Concepts","text":""},{"location":"Git/#repository-repo","title":"Repository (Repo)","text":"<p>A repository is a directory that contains your project files and the complete history of changes. Think of it as a special folder that Git monitors.</p>"},{"location":"Git/#commit","title":"Commit","text":"<p>A commit is a snapshot of your project at a specific point in time. Each commit has a unique identifier and contains information about what changed.</p>"},{"location":"Git/#branch","title":"Branch","text":"<p>A branch is a parallel version of your project. You can work on different features in separate branches without affecting the main code.</p>"},{"location":"Git/#working-directory-staging-area-and-repository","title":"Working Directory, Staging Area, and Repository","text":"<p>Git has three main areas where your files can exist:</p> <pre><code>Working Directory \u2192 Staging Area \u2192 Repository\n     (edit)           (add)        (commit)\n</code></pre> <ul> <li>Working Directory: Where you edit your files</li> <li>Staging Area: Where you prepare changes before committing</li> <li>Repository: Where Git stores committed changes permanently</li> </ul>"},{"location":"Git/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"Git/#installing-git","title":"Installing Git","text":"<p>On Ubuntu/Debian: <pre><code>sudo apt update\nsudo apt install git\n</code></pre></p> <p>On macOS: <pre><code># Using Homebrew\nbrew install git\n\n# Or download from https://git-scm.com/\n</code></pre></p> <p>On Windows: Download from https://git-scm.com/download/win</p>"},{"location":"Git/#initial-configuration","title":"Initial Configuration","text":"<p>Before using Git, set up your identity:</p> <pre><code># Set your name (replace with your actual name)\ngit config --global user.name \"Your Name\"\n\n# Set your email (replace with your actual email)\ngit config --global user.email \"your.email@example.com\"\n\n# Set default text editor (optional)\ngit config --global core.editor \"nano\"\n\n# Check your configuration\ngit config --list\n</code></pre>"},{"location":"Git/#basic-git-workflow","title":"Basic Git Workflow","text":""},{"location":"Git/#1-creating-a-repository","title":"1. Creating a Repository","text":""},{"location":"Git/#option-a-start-a-new-project","title":"Option A: Start a new project","text":"<pre><code># Create a new directory\nmkdir my-project\ncd my-project\n\n# Initialize Git repository\ngit init\n</code></pre>"},{"location":"Git/#option-b-clone-an-existing-project","title":"Option B: Clone an existing project","text":"<pre><code># Clone from a remote repository\ngit clone https://github.com/username/repository-name.git\ncd repository-name\n</code></pre>"},{"location":"Git/#2-check-repository-status","title":"2. Check Repository Status","text":"<pre><code># See what's happening in your repository\ngit status\n</code></pre> <p>Example output: <pre><code>On branch main\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    hello.txt\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre></p>"},{"location":"Git/#3-add-files-to-staging-area","title":"3. Add Files to Staging Area","text":"<pre><code># Add a specific file\ngit add filename.txt\n\n# Add all files in current directory\ngit add .\n\n# Add all files with specific extension\ngit add *.py\n\n# Add multiple specific files\ngit add file1.txt file2.py file3.html\n</code></pre> <p>Visual representation: <pre><code>Working Directory    Staging Area    Repository\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 hello.txt       \u2502  \u2502             \u2502  \u2502             \u2502\n\u2502 (modified)      \u2502  \u2502             \u2502  \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n                        git add hello.txt\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 hello.txt       \u2502  \u2502 hello.txt   \u2502  \u2502             \u2502\n\u2502 (modified)      \u2502  \u2502 (staged)    \u2502  \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"Git/#4-commit-changes","title":"4. Commit Changes","text":"<pre><code># Commit with a message\ngit commit -m \"Add hello.txt file\"\n\n# Commit with a detailed message\ngit commit -m \"Add user authentication feature\n\n- Added login form\n- Implemented password validation\n- Created user session management\"\n\n# Add and commit in one step (for tracked files only)\ngit commit -am \"Quick commit message\"\n</code></pre> <p>After committing: <pre><code>Working Directory    Staging Area    Repository\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 hello.txt       \u2502  \u2502             \u2502  \u2502 hello.txt   \u2502\n\u2502 (clean)         \u2502  \u2502             \u2502  \u2502 (committed) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"Git/#5-view-commit-history","title":"5. View Commit History","text":"<pre><code># View commit history\ngit log\n\n# View compact history\ngit log --oneline\n\n# View history with graph\ngit log --oneline --graph\n\n# View last 5 commits\ngit log -5\n</code></pre> <p>Example output: <pre><code>commit a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0\nAuthor: Your Name &lt;your.email@example.com&gt;\nDate:   Mon Oct 23 14:30:00 2023 +0200\n\n    Add hello.txt file\n\ncommit b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0a1\nAuthor: Your Name &lt;your.email@example.com&gt;\nDate:   Mon Oct 23 14:25:00 2023 +0200\n\n    Initial commit\n</code></pre></p>"},{"location":"Git/#working-with-files","title":"Working with Files","text":""},{"location":"Git/#checking-file-status","title":"Checking File Status","text":"<pre><code># Detailed status\ngit status\n\n# Short status\ngit status -s\n</code></pre> <p>Status symbols: - <code>??</code> = Untracked file - <code>A</code> = Added to staging area - <code>M</code> = Modified - <code>D</code> = Deleted - <code>R</code> = Renamed. </p>"},{"location":"Git/#viewing-changes","title":"Viewing Changes","text":"<pre><code># See changes in working directory (not staged)\ngit diff\n\n# See changes in staging area (staged but not committed)\ngit diff --staged\n\n# See changes in a specific file\ngit diff filename.txt\n\n# Compare two commits\ngit diff commit1 commit2\n</code></pre>"},{"location":"Git/#undoing-changes","title":"Undoing Changes","text":"<pre><code># Discard changes in working directory\ngit checkout -- filename.txt\n\n# Remove file from staging area (unstage)\ngit reset HEAD filename.txt\n\n# Undo last commit (keep changes in working directory)\ngit reset --soft HEAD~1\n\n# Undo last commit (discard changes completely)\ngit reset --hard HEAD~1\n</code></pre> <p>\u26a0\ufe0f Warning: <code>git reset --hard</code> permanently deletes changes!</p>"},{"location":"Git/#branching-and-merging","title":"Branching and Merging","text":""},{"location":"Git/#understanding-branches","title":"Understanding Branches","text":"<p>Branches allow you to work on different features simultaneously:</p> <pre><code>main branch:     A---B---C---D\n                      \\\nfeature branch:        E---F\n</code></pre>"},{"location":"Git/#branch-commands","title":"Branch Commands","text":"<pre><code># List all branches\ngit branch\n\n# Create a new branch\ngit branch feature-login\n\n# Switch to a branch\ngit checkout feature-login\n\n# Create and switch to a branch in one command\ngit checkout -b feature-login\n\n# Delete a branch\ngit branch -d feature-login\n\n# Force delete a branch\ngit branch -D feature-login\n</code></pre>"},{"location":"Git/#working-with-branches","title":"Working with Branches","text":"<p>Example workflow: <pre><code># 1. Create and switch to feature branch\ngit checkout -b add-user-profile\n\n# 2. Make changes and commit\necho \"User profile page\" &gt; profile.html\ngit add profile.html\ngit commit -m \"Add user profile page\"\n\n# 3. Switch back to main branch\ngit checkout main\n\n# 4. Merge feature branch into main\ngit merge add-user-profile\n\n# 5. Delete feature branch (optional)\ngit branch -d add-user-profile\n</code></pre></p>"},{"location":"Git/#merge-conflicts","title":"Merge Conflicts","text":"<p>When Git can't automatically merge changes, you'll get a conflict:</p> <pre><code># This might create a conflict\ngit merge feature-branch\n</code></pre> <p>Conflict markers in files: <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the content from the current branch\n=======\nThis is the content from the feature branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-branch\n</code></pre></p> <p>To resolve: 1. Edit the file to choose which content to keep 2. Remove the conflict markers (<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>, <code>=======</code>, <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>) 3. Add and commit the resolved file  </p> <pre><code># After resolving conflicts\ngit add conflicted-file.txt\ngit commit -m \"Resolve merge conflict\"\n</code></pre>"},{"location":"Git/#remote-repositories","title":"Remote Repositories","text":""},{"location":"Git/#understanding-remotes","title":"Understanding Remotes","text":"<p>A remote repository is a version of your project hosted on a server (like GitHub, GitLab, or Bitbucket).</p> <pre><code>Local Repository   \u2190\u2192  Remote Repository\n(your computer)         (GitHub/GitLab)\n</code></pre>"},{"location":"Git/#working-with-remotes","title":"Working with Remotes","text":"<pre><code># Add a remote repository\ngit remote add origin https://github.com/username/repository.git\n\n# List remote repositories\ngit remote -v\n\n# Push changes to remote\ngit push origin main\n\n# Pull changes from remote\ngit pull origin main\n\n# Fetch changes without merging\ngit fetch origin\n\n# Clone a remote repository\ngit clone https://github.com/username/repository.git\n</code></pre>"},{"location":"Git/#typical-remote-workflow","title":"Typical Remote Workflow","text":"<pre><code># 1. Clone a repository\ngit clone https://github.com/username/project.git\ncd project\n\n# 2. Create a feature branch\ngit checkout -b new-feature\n\n# 3. Make changes and commit\necho \"New feature\" &gt; feature.txt\ngit add feature.txt\ngit commit -m \"Add new feature\"\n\n# 4. Push branch to remote\ngit push origin new-feature\n\n# 5. Switch to main and update\ngit checkout main\ngit pull origin main\n\n# 6. Merge feature branch\ngit merge new-feature\n\n# 7. Push updated main branch\ngit push origin main\n</code></pre>"},{"location":"Git/#practical-examples","title":"Practical Examples","text":""},{"location":"Git/#example-1-starting-a-new-project","title":"Example 1: Starting a New Project","text":"<pre><code># Create project directory\nmkdir my-website\ncd my-website\n\n# Initialize Git\ngit init\n\n# Create initial files\necho \"# My Website\" &gt; README.md\necho \"Hello, World!\" &gt; index.html\n\n# Add and commit\ngit add .\ngit commit -m \"Initial commit with README and index.html\"\n\n# Check status\ngit status\ngit log --oneline\n</code></pre>"},{"location":"Git/#example-2-collaborating-on-a-project","title":"Example 2: Collaborating on a Project","text":"<pre><code># Clone existing project\ngit clone https://github.com/team/project.git\ncd project\n\n# Create feature branch\ngit checkout -b fix-bug-123\n\n# Make changes\necho \"Bug fix code\" &gt;&gt; bugfix.py\ngit add bugfix.py\ngit commit -m \"Fix bug #123: Resolve login issue\"\n\n# Push to remote\ngit push origin fix-bug-123\n\n# Switch to main and update\ngit checkout main\ngit pull origin main\n\n# Merge fix\ngit merge fix-bug-123\ngit push origin main\n\n# Clean up\ngit branch -d fix-bug-123\n</code></pre>"},{"location":"Git/#example-3-recovering-from-mistakes","title":"Example 3: Recovering from Mistakes","text":"<pre><code># Oops! Made a mistake in the last commit\ngit log --oneline\n# abc123 Wrong commit message\n# def456 Previous good commit\n\n# Fix the commit message\ngit commit --amend -m \"Correct commit message\"\n\n# Or undo the commit entirely\ngit reset --soft HEAD~1\n\n# Discard all changes in working directory\ngit checkout -- .\n</code></pre>"},{"location":"Git/#best-practices","title":"Best Practices","text":""},{"location":"Git/#1-write-good-commit-messages","title":"1. Write Good Commit Messages","text":"<p>Good: <pre><code>git commit -m \"Add user authentication system\n\n- Implement login and logout functionality\n- Add password encryption\n- Create user session management\n- Add input validation for forms\"\n</code></pre></p> <p>Bad: <pre><code>git commit -m \"stuff\"\ngit commit -m \"fixes\"\ngit commit -m \"update\"\n</code></pre></p>"},{"location":"Git/#2-commit-often-push-regularly","title":"2. Commit Often, Push Regularly","text":"<pre><code># Good practice: small, frequent commits\ngit commit -m \"Add login form HTML\"\ngit commit -m \"Add CSS styling for login form\"\ngit commit -m \"Add JavaScript form validation\"\n\n# Then push when ready\ngit push origin main\n</code></pre>"},{"location":"Git/#3-use-branches-for-features","title":"3. Use Branches for Features","text":"<pre><code># Create branches for each feature\ngit checkout -b user-registration\ngit checkout -b password-reset\ngit checkout -b email-notifications\n</code></pre>"},{"location":"Git/#4-keep-main-branch-clean","title":"4. Keep Main Branch Clean","text":"<pre><code># Always test before merging to main\ngit checkout feature-branch\n# ... test your changes ...\ngit checkout main\ngit merge feature-branch\n</code></pre>"},{"location":"Git/#common-commands-reference","title":"Common Commands Reference","text":""},{"location":"Git/#essential-commands","title":"Essential Commands","text":"<pre><code>git init                    # Initialize repository\ngit clone &lt;url&gt;            # Clone remote repository\ngit status                 # Check status\ngit add &lt;file&gt;             # Stage file\ngit commit -m \"message\"    # Commit changes\ngit push origin main       # Push to remote\ngit pull origin main       # Pull from remote\n</code></pre>"},{"location":"Git/#branch-commands_1","title":"Branch Commands","text":"<pre><code>git branch                 # List branches\ngit checkout &lt;branch&gt;      # Switch branch\ngit checkout -b &lt;branch&gt;   # Create and switch\ngit merge &lt;branch&gt;         # Merge branch\ngit branch -d &lt;branch&gt;     # Delete branch\n</code></pre>"},{"location":"Git/#history-commands","title":"History Commands","text":"<pre><code>git log                    # View commit history\ngit log --oneline         # Compact history\ngit diff                  # See changes\ngit show &lt;commit&gt;         # Show commit details\n</code></pre>"},{"location":"Git/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"Git/#issue-1-repository-not-found","title":"Issue 1: \"Repository not found\"","text":"<pre><code># Check remote URL\ngit remote -v\n\n# Update remote URL\ngit remote set-url origin https://github.com/username/repo.git\n</code></pre>"},{"location":"Git/#issue-2-merge-conflicts","title":"Issue 2: Merge Conflicts","text":"<pre><code># When you see conflict markers, edit the file\n# Remove &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt; markers\n# Keep the content you want\n\n# Then add and commit\ngit add conflicted-file.txt\ngit commit -m \"Resolve merge conflict\"\n</code></pre>"},{"location":"Git/#issue-3-accidental-commits","title":"Issue 3: Accidental Commits","text":"<pre><code># Undo last commit but keep changes\ngit reset --soft HEAD~1\n\n# Undo last commit and discard changes (careful!)\ngit reset --hard HEAD~1\n</code></pre>"},{"location":"Git/#next-steps","title":"Next Steps","text":"<p>Once you're comfortable with these basics:</p> <ol> <li>Learn about Git workflows (GitFlow, GitHub Flow)</li> <li>Explore advanced features (rebasing, cherry-picking, stashing)</li> <li>Use Git hosting platforms (GitHub, GitLab, Bitbucket)</li> <li>Integrate with development tools (IDEs, continuous integration)</li> </ol>"},{"location":"Git/#summary","title":"Summary","text":"<p>Git is an essential tool for any developer. The key concepts to remember:</p> <ul> <li>Repository: Your project folder with Git tracking</li> <li>Commit: A snapshot of your project at a point in time</li> <li>Branch: A parallel version of your project</li> <li>Remote: A copy of your repository on a server</li> </ul> <p>The basic workflow is: 1. Make changes to files 2. Stage changes with <code>git add</code> 3. Commit changes with <code>git commit</code> 4. Push to remote with <code>git push</code> </p> <p>Practice these commands regularly, and you'll become proficient with Git in no time!</p>"},{"location":"advanced-git/","title":"Advanced Git: Workflows &amp; Commands","text":""},{"location":"advanced-git/#introduction","title":"IntroductionTable of Content","text":"<p>If you have learned Git Version Control Guide basics, it's time to explore advanced concepts that professional developers use daily. This page covers sophisticated Git workflows, powerful commands that can save you time and effort. There is another part that treat how Git integrates Continuous Integration/Continuous Deployment (CI/CD) systems.</p> <p>These advanced techniques will help you work more efficiently in team environments and handle complex development scenarios.</p> <ul> <li>Advanced Git: Workflows &amp; Commands<ul> <li>Introduction</li> <li>Git Workflows</li> <li>GitFlow Workflow</li> <li>GitHub Flow</li> <li>Advanced Git Commands</li> <li>Git Workflow Best Practices</li> <li>Advanced Commands Best Practices</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"advanced-git/#git-workflows","title":"Git Workflows","text":""},{"location":"advanced-git/#what-is-a-git-workflow","title":"What is a Git Workflow?","text":"<p>A Git workflow is a recipe or recommendation for how to use Git to accomplish work in a consistent and productive manner. Different teams use different workflows based on their project size, team structure, and deployment strategy.</p>"},{"location":"advanced-git/#gitflow-workflow","title":"GitFlow Workflow","text":""},{"location":"advanced-git/#overview","title":"Overview","text":"<p>GitFlow is a branching model designed around project releases. It's ideal for projects with scheduled releases and provides a robust framework for managing larger projects.</p>"},{"location":"advanced-git/#branch-types-in-gitflow","title":"Branch Types in GitFlow","text":"<pre><code>main branch:     A---B---C---D---E---F\n                  \\                 /\n                   \\               /\nrelease branch:     G---H----I----J\n                     \\           /\ndevelop branch:       K----L----M---N---O\n                       \\       /     \\\nfeature branches:       P-----Q       R---S\n                                       \\\nhotfix branch:                          T---U\n</code></pre> <p>Branch descriptions:</p> <ul> <li>main: Production-ready code</li> <li>develop: Integration branch for features</li> <li>feature: Individual feature development</li> <li>release: Preparation for production release</li> <li>hotfix: Quick fixes for production issues</li> </ul>"},{"location":"advanced-git/#gitflow-commands","title":"GitFlow Commands","text":"<p>First, install GitFlow: <pre><code># On Ubuntu/Debian\nsudo apt install git-flow\n\n# On macOS\nbrew install git-flow-avh\n</code></pre></p>"},{"location":"advanced-git/#initialize-gitflow","title":"Initialize GitFlow","text":"<pre><code># Initialize GitFlow in your repository\ngit flow init\n\n# Follow the prompts (usually accept defaults):\n# - Production branch: main\n# - Development branch: develop\n# - Feature branch prefix: feature/\n# - Release branch prefix: release/\n# - Hotfix branch prefix: hotfix/\n</code></pre>"},{"location":"advanced-git/#working-with-features","title":"Working with Features","text":"<pre><code># Start a new feature\ngit flow feature start user-authentication\n\n# This creates and switches to: feature/user-authentication\n# Equivalent to:\n# git checkout develop\n# git checkout -b feature/user-authentication\n\n# Work on your feature (add, commit, etc.)\necho \"Login form\" &gt; login.html\ngit add login.html\ngit commit -m \"Add login form\"\n\n# Finish the feature\ngit flow feature finish user-authentication\n\n# This merges feature into develop and deletes the feature branch\n# Equivalent to:\n# git checkout develop\n# git merge feature/user-authentication\n# git branch -d feature/user-authentication\n</code></pre>"},{"location":"advanced-git/#working-with-releases","title":"Working with Releases","text":"<pre><code># Start a release\ngit flow release start 1.0.0\n\n# This creates release/1.0.0 branch from develop\n# Make final adjustments, bug fixes, update version numbers\necho \"Version 1.0.0\" &gt; VERSION\ngit add VERSION\ngit commit -m \"Bump version to 1.0.0\"\n\n# Finish the release\ngit flow release finish 1.0.0\n\n# This:\n# - Merges release into main\n# - Tags the release\n# - Merges release back into develop\n# - Deletes the release branch\n</code></pre>"},{"location":"advanced-git/#working-with-hotfixes","title":"Working with Hotfixes","text":"<pre><code># Start a hotfix (from main branch)\ngit flow hotfix start critical-bug\n\n# Fix the bug\necho \"Bug fix code\" &gt; bugfix.py\ngit add bugfix.py\ngit commit -m \"Fix critical security vulnerability\"\n\n# Finish the hotfix\ngit flow hotfix finish critical-bug\n\n# This:\n# - Merges hotfix into main\n# - Tags the hotfix\n# - Merges hotfix into develop\n# - Deletes the hotfix branch\n</code></pre>"},{"location":"advanced-git/#gitflow-advantages-and-disadvantages","title":"GitFlow Advantages and Disadvantages","text":"<p>Advantages: - Clear separation of concerns - Suitable for scheduled releases - Robust for large teams - Well-defined process for hotfixes  </p> <p>Disadvantages: - Complex for simple projects - Can slow down development - Requires discipline from team members  </p>"},{"location":"advanced-git/#github-flow","title":"GitHub Flow","text":""},{"location":"advanced-git/#overview_1","title":"Overview","text":"<p>GitHub Flow is a simpler alternative to GitFlow, designed for continuous deployment. It's perfect for web applications and projects that deploy frequently.</p>"},{"location":"advanced-git/#github-flow-process","title":"GitHub Flow Process","text":"<pre><code>main branch:     A---B---D---F-----H\n                  \\     /     \\   /\n                   \\   /       \\ /\nfeature branch:      C          E\n                                 \\\nanother feature:                  G\n</code></pre> <p>The process:</p> <ol> <li>Create a branch from main</li> <li>Add commits to your branch</li> <li>Open a Pull Request (PR)</li> <li>Discuss and review your code</li> <li>Deploy for testing (optional)</li> <li>Merge into main</li> </ol>"},{"location":"advanced-git/#github-flow-commands","title":"GitHub Flow Commands","text":"<pre><code># 1. Always start from main\ngit checkout main\ngit pull origin main\n\n# 2. Create a feature branch\ngit checkout -b feature/add-search-functionality\n\n# 3. Make changes and commit\necho \"Search functionality\" &gt; search.js\ngit add search.js\ngit commit -m \"Add search functionality\"\n\n# 4. Push the branch\ngit push origin feature/add-search-functionality\n\n# 5. Create Pull Request (done on GitHub web interface)\n# 6. After review and approval, merge via GitHub\n# 7. Clean up locally\ngit checkout main\ngit pull origin main\ngit branch -d feature/add-search-functionality\n</code></pre>"},{"location":"advanced-git/#github-flow-advantages-and-disadvantages","title":"GitHub Flow Advantages and Disadvantages","text":"<p>Advantages: - Simple and easy to understand - Perfect for continuous deployment - Fast development cycle - Less overhead than GitFlow  </p> <p>Disadvantages: - Less structured for complex projects - Requires good testing practices - Not suitable for scheduled releases. </p>"},{"location":"advanced-git/#advanced-git-commands","title":"Advanced Git Commands","text":""},{"location":"advanced-git/#stashing-temporarily-save-changes","title":"Stashing: Temporarily Save Changes","text":"<p>Stashing allows you to save your current work without committing, useful when you need to quickly switch branches or pull updates.</p> <pre><code># Save current changes to stash\ngit stash\n\n# Save with a descriptive message\ngit stash save \"Work in progress on login feature\"\n\n# List all stashes\ngit stash list\n# Output:\n# stash@{0}: WIP on main: 5c3d1a2 Add login form\n# stash@{1}: On feature-branch: 8f4e5d6 Work in progress on login feature\n\n# Apply most recent stash\ngit stash apply\n\n# Apply specific stash\ngit stash apply stash@{1}\n\n# Apply stash and remove it from stash list\ngit stash pop\n\n# Show stash contents\ngit stash show\ngit stash show -p  # Show full diff\n\n# Delete a stash\ngit stash drop stash@{1}\n\n# Delete all stashes\ngit stash clear\n</code></pre> <p>Practical example: <pre><code># You're working on a feature\necho \"Incomplete feature\" &gt; feature.txt\ngit add feature.txt\n\n# Emergency: need to fix a bug on main branch\ngit stash save \"Incomplete feature work\"\n\n# Switch to main and fix bug\ngit checkout main\necho \"Bug fix\" &gt; bugfix.txt\ngit add bugfix.txt\ngit commit -m \"Fix critical bug\"\n\n# Return to feature work\ngit checkout feature-branch\ngit stash pop  # Restore your incomplete work\n</code></pre></p>"},{"location":"advanced-git/#rebasing-rewrite-history","title":"Rebasing: Rewrite History","text":"<p>Rebasing rewrites commit history by moving commits to a new base. It creates a cleaner, linear history.</p>"},{"location":"advanced-git/#interactive-rebase","title":"Interactive Rebase","text":"<pre><code># Rebase last 3 commits interactively\ngit rebase -i HEAD~3\n\n# This opens an editor with:\n# pick abc123 First commit\n# pick def456 Second commit\n# pick ghi789 Third commit\n</code></pre> <p>Rebase commands: - <code>pick</code>: Keep the commit as-is - <code>reword</code>: Keep commit but edit message - <code>edit</code>: Keep commit but stop to make changes - <code>squash</code>: Combine with previous commit - <code>drop</code>: Remove the commit  </p> <p>Example - Squashing commits: <pre><code># Change to:\n# pick abc123 First commit\n# squash def456 Second commit\n# squash ghi789 Third commit\n\n# This combines all three commits into one\n</code></pre></p>"},{"location":"advanced-git/#rebase-onto-another-branch","title":"Rebase onto Another Branch","text":"<pre><code># Rebase current branch onto main\ngit rebase main\n\n# Rebase feature branch onto develop\ngit checkout feature-branch\ngit rebase develop\n</code></pre> <p>Visual representation: <pre><code>Before rebase:\nmain:     A---B---C\n               \\\nfeature:        D---E---F\n\nAfter rebase:\nmain:     A---B---C\n                   \\\nfeature:            D'---E'---F'\n</code></pre></p>"},{"location":"advanced-git/#handling-rebase-conflicts","title":"Handling Rebase Conflicts","text":"<pre><code># If conflicts occur during rebase\ngit rebase main\n\n# Edit conflicted files, then:\ngit add conflicted-file.txt\ngit rebase --continue\n\n# To abort rebase:\ngit rebase --abort\n</code></pre>"},{"location":"advanced-git/#cherry-picking-select-specific-commits","title":"Cherry-picking: Select Specific Commits","text":"<p>Cherry-picking allows you to apply specific commits from one branch to another.</p> <pre><code># Apply commit abc123 to current branch\ngit cherry-pick abc123\n\n# Apply multiple commits\ngit cherry-pick abc123 def456 ghi789\n\n# Apply a range of commits\ngit cherry-pick abc123..ghi789\n\n# Cherry-pick without committing (stage changes only)\ngit cherry-pick --no-commit abc123\n</code></pre> <p>Practical example: <pre><code># You have a bug fix in feature branch that's needed in main\ngit log --oneline feature-branch\n# abc123 Fix login bug\n# def456 Add new feature (not ready)\n# ghi789 Update documentation\n\n# Cherry-pick just the bug fix\ngit checkout main\ngit cherry-pick abc123  # Only applies the bug fix\n</code></pre></p> <p>Visual representation: <pre><code>Before cherry-pick:\nmain:     A---B---C\n               \\\nfeature:        D---E---F (E contains the fix we want)\n\nAfter cherry-pick:\nmain:     A---B---C---E'\n               \\\nfeature:        D---E---F\n</code></pre></p>"},{"location":"advanced-git/#advanced-git-commands-summary","title":"Advanced Git Commands Summary","text":"<pre><code># Stashing\ngit stash                    # Save current work\ngit stash pop               # Restore and remove from stash\ngit stash list              # Show all stashes\n\n# Rebasing\ngit rebase -i HEAD~3        # Interactive rebase last 3 commits\ngit rebase main             # Rebase current branch onto main\ngit rebase --continue       # Continue after resolving conflicts\n\n# Cherry-picking\ngit cherry-pick abc123      # Apply specific commit\ngit cherry-pick abc123..def456  # Apply range of commits\n</code></pre>"},{"location":"advanced-git/#git-workflow-best-practices","title":"Git Workflow Best Practices","text":"<ol> <li>Choose the right workflow for your team size and deployment frequency</li> <li>Use descriptive branch names (<code>feature/user-auth</code>, <code>hotfix/login-bug</code>)</li> <li>Keep commits small and focused</li> <li>Write clear commit messages</li> <li>Use pull requests for code review</li> </ol>"},{"location":"advanced-git/#advanced-commands-best-practices","title":"Advanced Commands Best Practices","text":"<ol> <li>Never rebase public branches (shared with others)</li> <li>Test after rebasing before pushing</li> <li>Use stash for quick context switching</li> <li>Cherry-pick sparingly (can create duplicate commits)</li> <li>Use interactive rebase to clean up history before merging</li> </ol>"},{"location":"advanced-git/#conclusion","title":"Conclusion","text":"<p>Advanced Git workflows are essential for professional software development. GitFlow provides structure for complex projects, while GitHub Flow offers simplicity for continuous deployment. Advanced commands like rebasing, cherry-picking, and stashing give you powerful tools for managing your codebase.</p> <p>Remember: start simple and gradually introduce more advanced practices as your team and project grow. The key is consistency and automation to reduce manual errors and improve code quality.</p>"},{"location":"ci-cd/","title":"Continuous Integration/Continuous Deployment (CI/CD)","text":"Table of Content <ul> <li>Continuous Integration/Continuous Deployment (CI/CD)<ul> <li>What is CI/CD?</li> <li>CI/CD Pipeline Stages</li> <li>GitHub Actions (CI/CD Example)</li> <li>GitLab CI/CD Example</li> <li>CI/CD Best Practices</li> <li>Branch Protection Rules</li> <li>Practical Integration Examples</li> <li>Integration with Development Tools</li> <li>Troubleshooting Advanced Git Issues</li> <li>CI/CD Best Practices</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"ci-cd/#what-is-cicd","title":"What is CI/CD?","text":"<p>Continuous Integration (CI) is the practice of automatically testing code changes when they're pushed to a repository.</p> <p>Continuous Deployment (CD) is the practice of automatically deploying code changes to production after they pass all tests.</p>"},{"location":"ci-cd/#cicd-pipeline-stages","title":"CI/CD Pipeline Stages","text":"<pre><code>Code Push \u2192 Build \u2192 Test \u2192 Deploy \u2192 Monitor\n     \u2193         \u2193      \u2193       \u2193        \u2193\n  GitHub   Compile  Unit   Production  Logs\n  GitLab   Install  Tests  Staging     Metrics\n  Bitbucket        Integration         Alerts\n</code></pre>"},{"location":"ci-cd/#github-actions-cicd-example","title":"GitHub Actions (CI/CD Example)","text":"<p>GitHub Actions is GitHub's built-in CI/CD platform.</p>"},{"location":"ci-cd/#basic-github-actions-workflow","title":"Basic GitHub Actions Workflow","text":"<p>Create <code>.github/workflows/ci.yml</code>:</p> <pre><code>name: CI/CD Pipeline\n\n# Trigger on push and pull requests to main\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n    # Checkout code\n    - uses: actions/checkout@v3\n\n    # Setup Node.js\n    - name: Setup Node.js\n      uses: actions/setup-node@v3\n      with:\n        node-version: '18'\n\n    # Install dependencies\n    - name: Install dependencies\n      run: npm install\n\n    # Run tests\n    - name: Run tests\n      run: npm test\n\n    # Run linting\n    - name: Run linter\n      run: npm run lint\n\n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Deploy to production\n      run: |\n        echo \"Deploying to production...\"\n        # Add your deployment commands here\n</code></pre>"},{"location":"ci-cd/#python-project-example","title":"Python Project Example","text":"<p><code>.github/workflows/python-ci.yml</code>:</p> <pre><code>name: Python CI\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.8, 3.9, 3.10]\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install pytest\n        pip install -r requirements.txt\n\n    - name: Run tests\n      run: pytest tests/\n\n    - name: Run coverage\n      run: |\n        pip install coverage\n        coverage run -m pytest\n        coverage report\n</code></pre> <p>The trigger block defines the conditions under which the action will be executed. The job block specifies additional details, including the operating system on which the action should run (runs-on) and the sequence of tasks to be performed (steps).</p> <p>You don\u2019t need to write your .yaml file from scratch \u2014 numerous examples are available on GitHub. You can simply adapt one to match your specific workflow requirements.</p> <p>Once your .yaml file is ready and the action has been triggered (for example, by pushing a commit), you can access the logs to monitor the execution. Below is an example using a test-coverage action.</p> <ol> <li>Go to the Github corresponding repository. In the \u00c0ctions`tab, select one of the workflows to display its successive runs.</li> </ol> <p> You can view the commits where the workflow was executed, along with details such as the branch, date, duration, and other relevant information. A green check mark indicates that the workflow completed successfully without errors, while a red cross signifies that the workflow failed.</p> <ol> <li>From there, select the run you\u2019re interested in to access a summary of the workflow execution. If there are any warnings, they will appear in the Annotations panel at the bottom of the screen. To view detailed logs, click on the workflow name either in the central panel or in the Jobs section on the left. Each step can be expanded to display its corresponding output logs.</li> </ol> <p></p>"},{"location":"ci-cd/#gitlab-cicd-example","title":"GitLab CI/CD Example","text":"<p>GitLab uses <code>.gitlab-ci.yml</code> for CI/CD configuration:</p> <pre><code>stages:\n  - build\n  - test\n  - deploy\n\nvariables:\n  NODE_VERSION: \"18\"\n\nbuild:\n  stage: build\n  image: node:$NODE_VERSION\n  script:\n    - npm install\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\ntest:\n  stage: test\n  image: node:$NODE_VERSION\n  script:\n    - npm install\n    - npm test\n    - npm run lint\n  coverage: '/Lines\\s*:\\s*(\\d+\\.\\d+)%/'\n\ndeploy:\n  stage: deploy\n  image: node:$NODE_VERSION\n  script:\n    - npm install\n    - npm run deploy\n  only:\n    - main\n</code></pre>"},{"location":"ci-cd/#cicd-best-practices","title":"CI/CD Best Practices","text":""},{"location":"ci-cd/#1-fail-fast","title":"1. Fail Fast","text":"<pre><code># Run fastest tests first\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run linter\n        run: npm run lint\n\n  test:\n    needs: lint  # Only run tests if linting passes\n    runs-on: ubuntu-latest\n    steps:\n      - name: Run tests\n        run: npm test\n</code></pre>"},{"location":"ci-cd/#2-use-build-matrices","title":"2. Use Build Matrices","text":"<pre><code>strategy:\n  matrix:\n    os: [ubuntu-latest, windows-latest, macos-latest]\n    node-version: [16, 18, 20]\n</code></pre>"},{"location":"ci-cd/#3-cache-dependencies","title":"3. Cache Dependencies","text":"<pre><code>- name: Cache node modules\n  uses: actions/cache@v3\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n</code></pre>"},{"location":"ci-cd/#4-secure-secrets","title":"4. Secure Secrets","text":"<pre><code>- name: Deploy\n  env:\n    API_KEY: ${{ secrets.API_KEY }}\n    DATABASE_URL: ${{ secrets.DATABASE_URL }}\n  run: deploy.sh\n</code></pre>"},{"location":"ci-cd/#branch-protection-rules","title":"Branch Protection Rules","text":"<p>Enforce quality through branch protection:</p> <pre><code># GitHub settings example (done via web interface):\n# - Require pull request reviews before merging\n# - Require status checks to pass before merging\n# - Require branches to be up to date before merging\n# - Require linear history\n</code></pre>"},{"location":"ci-cd/#practical-integration-examples","title":"Practical Integration Examples","text":""},{"location":"ci-cd/#example-1-feature-development-with-cicd","title":"Example 1: Feature Development with CI/CD","text":"<pre><code># 1. Start feature branch\ngit checkout -b feature/user-dashboard\ngit push -u origin feature/user-dashboard\n\n# 2. Make changes\necho \"Dashboard component\" &gt; dashboard.js\ngit add dashboard.js\ngit commit -m \"Add user dashboard component\"\n\n# 3. Push triggers CI\ngit push origin feature/user-dashboard\n# CI runs: lint, test, build\n\n# 4. Create Pull Request\n# CI runs again on PR\n\n# 5. After review and CI passes, merge\n# CD automatically deploys to staging\n\n# 6. After testing, merge to main\n# CD automatically deploys to production\n</code></pre>"},{"location":"ci-cd/#example-2-hotfix-with-fast-deployment","title":"Example 2: Hotfix with Fast Deployment","text":"<pre><code># 1. Create hotfix branch\ngit checkout main\ngit pull origin main\ngit checkout -b hotfix/critical-security-fix\n\n# 2. Fix the issue\necho \"Security patch\" &gt; security.patch\ngit add security.patch\ngit commit -m \"Fix critical security vulnerability\"\n\n# 3. Push and create PR\ngit push origin hotfix/critical-security-fix\n# Fast-track CI/CD pipeline for hotfixes\n\n# 4. Emergency deployment\n# Skip normal review process if critical\n# Deploy directly to production after CI passes\n</code></pre>"},{"location":"ci-cd/#example-3-release-management","title":"Example 3: Release Management","text":"<pre><code># 1. Create release branch\ngit checkout develop\ngit checkout -b release/v2.0.0\n\n# 2. Prepare release\necho \"2.0.0\" &gt; VERSION\ngit add VERSION\ngit commit -m \"Bump version to 2.0.0\"\n\n# 3. Push release branch\ngit push origin release/v2.0.0\n# CI runs full test suite\n\n# 4. Merge to main\ngit checkout main\ngit merge release/v2.0.0\ngit tag v2.0.0\ngit push origin main --tags\n\n# 5. CD deploys tagged release to production\n</code></pre>"},{"location":"ci-cd/#integration-with-development-tools","title":"Integration with Development Tools","text":""},{"location":"ci-cd/#ide-integration","title":"IDE Integration","text":"<p>Most modern IDEs support Git and CI/CD:</p> <p>VS Code Extensions: - GitLens - GitHub Pull Requests - GitLab Workflow</p> <p>Commands in VS Code: <pre><code># Git integration is built-in\n# Use Command Palette (Ctrl+Shift+P):\n# - Git: Clone\n# - Git: Commit\n# - Git: Push\n# - Git: Pull Request\n</code></pre></p>"},{"location":"ci-cd/#code-quality-tools","title":"Code Quality Tools","text":""},{"location":"ci-cd/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to run checks before commits:</p> <pre><code># Install pre-commit\npip install pre-commit\n\n# Create .pre-commit-config.yaml\ncat &gt; .pre-commit-config.yaml &lt;&lt; EOF\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n  - repo: https://github.com/pycqa/flake8\n    rev: 4.0.1\n    hooks:\n      - id: flake8\nEOF\n\n# Install hooks\npre-commit install\n</code></pre>"},{"location":"ci-cd/#code-coverage","title":"Code Coverage","text":"<p>Track code coverage in CI:</p> <pre><code># In GitHub Actions\n- name: Run tests with coverage\n  run: |\n    pip install coverage\n    coverage run -m pytest\n    coverage report --fail-under=80\n</code></pre>"},{"location":"ci-cd/#troubleshooting-advanced-git-issues","title":"Troubleshooting Advanced Git Issues","text":""},{"location":"ci-cd/#recovering-from-rebase-disasters","title":"Recovering from Rebase Disasters","text":"<pre><code># Find your commits in reflog\ngit reflog\n\n# Reset to before rebase\ngit reset --hard HEAD@{5}\n\n# Or use rebase abort if still in progress\ngit rebase --abort\n</code></pre>"},{"location":"ci-cd/#handling-large-repositories","title":"Handling Large Repositories","text":"<pre><code># Shallow clone (faster for CI)\ngit clone --depth 1 https://github.com/user/repo.git\n\n# Partial clone (Git 2.19+)\ngit clone --filter=blob:none https://github.com/user/repo.git\n</code></pre>"},{"location":"ci-cd/#fixing-merge-conflicts-in-ci","title":"Fixing Merge Conflicts in CI","text":"<pre><code># In CI configuration\n- name: Auto-merge main\n  run: |\n    git config user.name \"CI Bot\"\n    git config user.email \"ci@example.com\"\n    git fetch origin main\n    git merge origin/main || (echo \"Merge conflict detected\" &amp;&amp; exit 1)\n</code></pre>"},{"location":"ci-cd/#cicd-best-practices_1","title":"CI/CD Best Practices","text":"<ol> <li>Fail fast with quick feedback</li> <li>Test in production-like environments</li> <li>Use feature flags for safer deployments</li> <li>Monitor deployments with rollback capabilities</li> <li>Automate security scanning</li> </ol>"},{"location":"ci-cd/#conclusion","title":"Conclusion","text":"<p>CI/CD practices are essential for professional software development. CI/CD automates the process of testing and deploying code, reducing errors and increasing development velocity. By combining these practices with proper branch protection and code review processes, you create a robust development environment that scales with your team and project complexity.</p> <p>Remember: start simple and gradually introduce more advanced practices as your team and project grow. The key is consistency and automation to reduce manual errors and improve code quality.</p>"},{"location":"organize-data/","title":"Starting a new project","text":"<p>Whether you\u2019re developing a full-fledged software package or simply organising a collection of scripts, it is essential to start your project with a clear and well-structured organisation. Doing so will help keep your code clean, readable, and maintainable, while also making it easier to share and collaborate with others.</p> <p>Below are some key recommendations and considerations to keep in mind when starting a new project, with a particular focus on R and Python.</p>"},{"location":"organize-data/#organising-data-in-a-r-project","title":"Organising Data in a R project","text":"<p>Rstudio is the most widely used integrated development environment by R users and developers. It is highly recommended to use it.  </p>"},{"location":"organize-data/#creating-a-rstudio-project","title":"Creating a Rstudio project","text":"<p>Choose the menu File / New Project / New Directory / New Project. Choose a directory name without accent and blanck space. A .Rproj file will be created at the project root.</p> <p>Once the .Rproj file has been created, it is essential to get the project off to a good start, by deciding on an organisation that must be respected over time.   Choose a structure that works well for you and stick to it as much as possible. Below is a suggested way of organising the code:</p> <pre><code>my_project/             \n\u251c\u2500\u2500 code/                       # Script directory\n\u2502   \u2514\u2500\u2500 myscript.sh\n\u2502   \u2514\u2500\u2500 setup_env.R             \n\u251c\u2500\u2500 data/                       # Raw data directory\n\u2502   \u2514\u2500\u2500 data.csv\n\u251c\u2500\u2500 results/                    # Analysis results directory\n\u251c\u2500\u2500 tests/                      # Temporary test code\n\u2502   \u2514\u2500\u2500 my_temporary_script.R\n\u251c\u2500\u2500 tmp/                        \n\u251c\u2500\u2500 .gitignore \n\u251c\u2500\u2500 LICENSE \n\u2514\u2500\u2500 README.md\n</code></pre> <p>Mandatory: - <code>README.md</code> file. This file is arguably the most important file you\u2019ll create in your project. It should provide a clear overview of the code\u2019s purpose, including:     - The goal of the project     - Set-up and execution instructions     - If no separate documents exist (e.g., architecture diagram or system maintenance guide), a high-level explanation of how the code is structured and works - <code>LICENSE</code> file. It is a file that contains the license under which the package is distributed. Maybe the first file to add to your project. You can choose a license from choosealicence.  </p> <p>The <code>README.md</code> is not the place to describe individual functions in detail</p> <p>Keep it high-level and accessible.  The <code>README.md</code> is a living document. As your code evolves, make sure to keep the documentation up to date.  Don\u2019t postpone writing it until the end of the project \u2014 by then, you\u2019ll likely be racing against deadlines and already thinking about your next project.  Start documenting early. It will save you time and effort later. </p> <p>File / directories to be created: - <code>Cluster_logs</code> directory. It contains logs files redirected from standard output (<code>.out</code>) and standard error (<code>.err</code>). - <code>code/</code> directory. It contains scripts, in the form of <code>.R</code>, <code>.sh</code>, <code>.py</code> files. - <code>data/</code> directory. It contains data to be analyzed (.csv, .vcf...). - <code>results/</code> directory. It contains outputs. It is recommended to add subdirectories in this directory. - <code>tests/</code> directory. Code tests should go in here. - <code>tmp/</code>directory. It is used to store temporary files generated by the programs or workflows within this project. It serves as a local alternative to the system\u2019s default $TMPDIR, which is often saturated or unreliable on shared cluster environments. - <code>.gitignore</code>. In this file, you add everything you do not want ending up in your remote repository. As a minimum, add <code>tmp/</code> file in there.  </p> <p>Automatically created if you add renv: - <code>renv.lock</code> file, <code>renv/</code> folder and <code>.Rprofile</code> file. This is the virtual environment set up. They will be automatically created by renv when you activate it. - <code>.Rproj</code> file. This is an RStudio file containing the project specific settings. Created automatically when you create an R project with RStudio ON Demand (see further). </p> <p>To create directories: <code>mkdir -p Cluster_logs data code results tests tmp</code> in a Terminal.</p>"},{"location":"organize-data/#start-version-control-with-githubgitlab","title":"Start Version Control With Github/Gitlab","text":"<pre><code>git init\n</code></pre>"},{"location":"organize-data/#running-r-scripts-in-a-clean-reproducible-manner","title":"Running R Scripts in a Clean &amp; Reproducible Manner","text":"<p>When launching R scripts, it is highly recommended to run them in a clean environment to ensure reproducibility and avoid unexpected side effects caused by user-specific configurations. By creating a R projects, you isolate your work from other projects. Not only can you have project specific files and libraries, but also project specific settings. To do so, you just have to create <code>renv</code> environment.</p>"},{"location":"organize-data/#use-renvinit","title":"Use renv::init()","text":"<p><code>renv::init()</code> does several important things:   </p> <ol> <li>Creates the renv/ infrastructure in your project:  <ol> <li><code>renv/</code>directory with configuration files  </li> <li><code>Rprofile</code> file that activates <code>renv</code> at start-up  </li> <li><code>renv.lock</code> file (empty at start)  </li> </ol> </li> <li>Isolates the environment: creates a private package library for this project only.  </li> <li>Scans the existing code to automatically detect the packages used (via library(), require() ...)  </li> <li>Installs the detected packages in the project's private library (NOT all, unfortunatelly...)  </li> </ol>"},{"location":"organize-data/#how-to","title":"How to","text":"<ol> <li> <p>If you haven't already done so, go into the project's directory  <pre><code>cd /path/to/my_project/\n</code></pre></p> </li> <li> <p>Create a setup_env.R file  <pre><code>vim code/setup_env.R\n</code></pre> <pre><code>#!/usr/bin/env Rscript\n# setup_env.R\n\ncat(\"Setting up R environment...\\n\")\n\n# 1. Install renv first (required before)\nif (!requireNamespace(\"renv\", quietly = TRUE)) {\n    cat(\"Installing renv package...\\n\")\n    install.packages(\"renv\", repos = \"https://cran.r-project.org/\")\n}\n\n# 2. Initialize renv for this projet\ncat(\"Initializing renv environment...\\n\")\nrenv::init()\n\n# 3. List all necessary packages\nrequired_packages &lt;- c(\n    \"data.table\",\n    \"ggplot2\",\n    \"dplyr\",\n    \"parallel\",\n    \"BiocManager\"           # For Bioconductor packages if necessary\n)\n\n# 4. CRAN packages management\ncat(\"Installing CRAN packages...\\n\")\ninstall.packages(required_packages)\n\n# 5. Bioconductor packages management\nif (\"BiocManager\" %in% required_packages) {\n    cat(\"Installing Bioconductor packages...\\n\")\n    BiocManager::install(c(\"DESeq2\", \"edgeR\"))      # examples\n}\n\n# 6. Final snapshot - Freeze all\ncat(\"Creating final snapshot...\\n\")\nrenv::snapshot()\n\n# 7. Final checkup\ncat(\"Setup completed. Session info:\\n\")\nsessionInfo()\n</code></pre> Save the file and quit vim by pressing escape and entering the following command : <pre><code>:wq\n</code></pre> At this step, you should already have made a list of the packages needed for the analyses. If you need to add packages during the analysis, simply edit this file and run it again.  </p> </li> <li> <p>Run the script in RStudio</p> </li> </ol> <p>Choose File / Open File... and select setup_env.R in the code/ directory. Then select Run.</p>"},{"location":"organize-data/#useful-r-packages","title":"Useful R packages","text":"<ul> <li><code>devtools</code> : a package that provides functions to ease the development of R packages, by executing common tasks like documentation, testing, etc. e.g.:</li> <li><code>devtools::document()</code> : generates the NAMESPACE file and documentation files for your functions</li> <li><code>devtools::load_all()</code> : loads your code and functions in the R environment, so that you can test them immediately</li> <li><code>devtools::test()</code> : runs the tests you have written for your functions</li> <li><code>usethis</code> : workflow automation package for R projects for setting up and develop projects, e.g.:</li> <li><code>usethis::use_git()</code> : initializes a git repository in your project</li> <li><code>usethis::use_testthat()</code> : initializes the testthat package in your project</li> <li><code>usethis::use_package(\"package\")</code> : adds a package to the DESCRIPTION file as a dependency of your package</li> <li><code>usethis::use_r()</code> : creates a new R script in the R/ sub-repository</li> <li><code>usethis::use_test(\"myTest\")</code> : creates a new test script tests/testthat/test-myTest.R</li> <li><code>here</code> : a package that provides functions to easily refer to files in your project, e.g.:</li> <li><code>here::here()</code> : returns the path to the project root</li> <li><code>here::here(\"R\", \"my_function.R\")</code> : returns the path to the my_function.R file in the R sub-repository</li> <li><code>testthat</code> : a package that provides functions to write and run tests for your functions. Check the official documentation and the dedicated section to see how to write tests.  Once you have initiated your tests with usethis::use_testthat(), you can write tests in the tests/testthat/ sub-repository:</li> <li><code>usethis::use_test(\"my_function\")</code> : creates a new test script tests/testthat/test-my_function.R</li> <li><code>devtools::test()</code> : to run the tests.</li> </ul>"},{"location":"organize-data/#organising-data-in-a-python-project","title":"Organising Data in a Python Project","text":"<p>Python is widely used for data science, bioinformatics, and HPC workflows. On a cluster, it\u2019s essential to manage environments carefully to avoid dependency conflicts and ensure reproducibility.</p>"},{"location":"organize-data/#organize-your-code","title":"Organize your code","text":"<p>Below is a suggested way of organizing the code, with some files and directories created automatically: <pre><code>my_project/ \n\u251c\u2500\u2500 code/                       # Script directory\n\u2502   \u2514\u2500\u2500 myscript.py\n\u251c\u2500\u2500 data/                       # Raw data directory\n\u2502   \u2514\u2500\u2500 data.csv\n\u251c\u2500\u2500 results/                    # Analysis results directory\n\u251c\u2500\u2500 tests/                      # Temporary test code\n\u2502   \u2514\u2500\u2500 my_temporary_script.py\n\u251c\u2500\u2500 tmp/    \n\u251c\u2500\u2500 venv/                       # Python virtual env.\n\u2502   \u2514\u2500\u2500 bin/\n\u2502         \u2514\u2500\u2500 activate\n\u2502   \u2514\u2500\u2500 include/\n\u2502   \u2514\u2500\u2500 lib/\n\u2502   \u2514\u2500\u2500 pyenv.cfg\n\u251c\u2500\u2500 LICENSE \n\u2514\u2500\u2500 README.md\n</code></pre> - LICENSE : a file that contains the license under which the package is distributed. Maybe the first file to add to your project. You can choose a license from choosealicence.</p>"},{"location":"organize-data/#best-practices-for-running-python-jobs","title":"Best Practices for Running Python Jobs","text":"<p>Before launching a job, make sure your script runs in a controlled Python environment.</p>"},{"location":"organize-data/#with-venv-standard-python","title":"With venv (standard Python)","text":"<pre><code>python -m venv myenv\nsource myenv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>the file requirements.txt contains, for example: <pre><code>numpy==1.26.4\npandas&gt;=2.2.0\nscipy&lt;1.12\nmatplotlib\nseaborn==0.12.2\nscikit-learn&gt;=1.3\njupyterlab\nrequests\nbiopython\n</code></pre></p>"},{"location":"organize-data/#with-conda","title":"With conda:","text":"<pre><code>conda create -n myenv python=3.11\nconda activate myenv\nconda install numpy pandas scipy  # etc.\n</code></pre> <p>Once set up, activate your environment inside your IDE before running your script.</p>"},{"location":"organize-data/#start-version-control-with-githubgitlab_1","title":"Start Version Control With Github/Gitlab","text":"<pre><code>git init\n</code></pre>"},{"location":"Bam/Bam/","title":"BAM Format Specification","text":"Table of Content <ul> <li>BAM Format Specification<ul> <li>Overview</li> <li>Format Architecture</li> <li>Alignment Record Structure</li> <li>FLAG Field Interpretation</li> <li>CIGAR Operations</li> <li>Quality Scores</li> <li>Indexing and Random Access</li> <li>Compression and Performance</li> <li>File Operations and Tools</li> <li>Best Practices</li> <li>Specifications and Standards</li> <li>Related Formats</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Bam/Bam/#overview","title":"Overview","text":"<p>The Binary Alignment/Map (BAM) format is a compressed binary representation of the Sequence Alignment/Map (SAM) format, designed for efficient storage and retrieval of high-throughput sequencing alignment data. BAM files serve as the standard format for storing aligned sequence reads in computational genomics and bioinformatics pipelines.</p>"},{"location":"Bam/Bam/#format-architecture","title":"Format Architecture","text":""},{"location":"Bam/Bam/#binary-structure","title":"Binary Structure","text":"<p>BAM files employ a block-compressed format using the BGZF (Blocked GNU Zip Format) compression scheme, which combines the efficiency of gzip compression with random access capabilities. The file structure consists of:</p> <ol> <li>Header section: Contains metadata including reference sequence information, read group details, and program records</li> <li>Alignment section: Stores individual alignment records in binary format</li> <li>Index files (optional): Separate <code>.bai</code> or <code>.csi</code> files enabling rapid random access</li> </ol>"},{"location":"Bam/Bam/#header-components","title":"Header Components","text":"<p>The BAM header contains essential metadata organized into record types:</p> <ul> <li>@HD: Header line specifying format version and sort order</li> <li>@SQ: Reference sequence dictionary with sequence names and lengths</li> <li>@RG: Read group information including sequencing platform and sample identifiers</li> <li>@PG: Program records documenting the software pipeline used for alignment</li> </ul>"},{"location":"Bam/Bam/#alignment-record-structure","title":"Alignment Record Structure","text":"<p>Each alignment record in BAM format contains the following fields:</p>"},{"location":"Bam/Bam/#mandatory-fields","title":"Mandatory Fields","text":"Field Type Description QNAME String Query template name (read identifier) FLAG Integer Bitwise flags indicating alignment properties RNAME String Reference sequence name POS Integer 1-based leftmost mapping position MAPQ Integer Mapping quality score (Phred-scaled) CIGAR String Compact Idiosyncratic Gapped Alignment Report RNEXT String Reference name of the mate/next read PNEXT Integer Position of the mate/next read TLEN Integer Template length SEQ String Segment sequence QUAL String ASCII-encoded base quality scores"},{"location":"Bam/Bam/#optional-fields","title":"Optional Fields","text":"<p>BAM records support extensible optional fields (tags) following the TAG:TYPE:VALUE format, enabling storage of additional alignment information such as:</p> <ul> <li>Alignment scores and statistics</li> <li>Original quality scores</li> <li>Color space information</li> <li>Custom annotations</li> </ul>"},{"location":"Bam/Bam/#flag-field-interpretation","title":"FLAG Field Interpretation","text":"<p>The FLAG field employs bitwise encoding to represent alignment properties:</p> Bit Value Description 0x1 1 Template having multiple segments 0x2 2 Each segment properly aligned 0x4 4 Segment unmapped 0x8 8 Next segment unmapped 0x10 16 SEQ reverse complemented 0x20 32 SEQ of next segment reverse complemented 0x40 64 First segment in template 0x80 128 Last segment in template 0x100 256 Secondary alignment 0x200 512 Not passing quality controls 0x400 1024 PCR or optical duplicate 0x800 2048 Supplementary alignment"},{"location":"Bam/Bam/#cigar-operations","title":"CIGAR Operations","text":"<p>The CIGAR string describes the alignment between query and reference sequences using operation codes:</p> Operation Code Description M 0 Alignment match (can be sequence match or mismatch) I 1 Insertion to the reference D 2 Deletion from the reference N 3 Skipped region from the reference S 4 Soft clipping (clipped sequences present in SEQ) H 5 Hard clipping (clipped sequences NOT present in SEQ) P 6 Padding (silent deletion from padded reference) = 7 Sequence match X 8 Sequence mismatch"},{"location":"Bam/Bam/#quality-scores","title":"Quality Scores","text":"<p>Base quality scores in BAM files follow the Phred scale encoding: - ASCII values represent quality scores with an offset of 33 - Quality score Q relates to error probability P: Q = -10 \u00d7 log\u2081\u2080(P) - Higher scores indicate greater confidence in base calls  </p>"},{"location":"Bam/Bam/#indexing-and-random-access","title":"Indexing and Random Access","text":"<p>BAM files support efficient random access through index files:</p>"},{"location":"Bam/Bam/#bai-index-format","title":"BAI Index Format","text":"<ul> <li>Traditional coordinate-based indexing</li> <li>Supports files up to 2\u00b3\u00b9 bases per reference sequence</li> <li>Uses 16 kb linear index intervals</li> </ul>"},{"location":"Bam/Bam/#csi-index-format","title":"CSI Index Format","text":"<ul> <li>Enhanced indexing supporting longer reference sequences</li> <li>Configurable indexing parameters</li> <li>Better performance for highly variable coverage</li> </ul>"},{"location":"Bam/Bam/#compression-and-performance","title":"Compression and Performance","text":"<p>The BGZF compression scheme provides several advantages:</p> <ol> <li>Block-level compression: Independent 64 KB blocks enable parallel processing</li> <li>Random access: Seek operations without full decompression</li> <li>Compression efficiency: Typically 3-5x size reduction compared to SAM</li> <li>Error detection: Built-in CRC32 checksums for data integrity</li> </ol>"},{"location":"Bam/Bam/#file-operations-and-tools","title":"File Operations and Tools","text":"<p>Standard operations on BAM files include:</p>"},{"location":"Bam/Bam/#viewing-and-conversion","title":"Viewing and Conversion","text":"<ul> <li><code>samtools view</code>: Convert between SAM/BAM formats and filter records</li> <li><code>samtools flagstat</code>: Generate alignment statistics</li> </ul>"},{"location":"Bam/Bam/#sorting-and-indexing","title":"Sorting and Indexing","text":"<ul> <li><code>samtools sort</code>: Sort alignments by coordinate or read name</li> <li><code>samtools index</code>: Generate BAI/CSI index files</li> </ul>"},{"location":"Bam/Bam/#quality-control","title":"Quality Control","text":"<ul> <li><code>samtools stats</code>: Comprehensive alignment statistics</li> <li><code>samtools depth</code>: Calculate per-position coverage depth</li> </ul>"},{"location":"Bam/Bam/#best-practices","title":"Best Practices","text":""},{"location":"Bam/Bam/#file-handling","title":"File Handling","text":"<ul> <li>Always sort BAM files by coordinate for optimal performance</li> <li>Generate index files for random access operations</li> <li>Validate file integrity using checksum verification</li> </ul>"},{"location":"Bam/Bam/#storage-considerations","title":"Storage Considerations","text":"<ul> <li>Use appropriate compression levels based on storage vs. access speed requirements</li> <li>Consider reference-based compression for highly redundant datasets</li> <li>Implement proper backup strategies for irreplaceable sequencing data</li> </ul>"},{"location":"Bam/Bam/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Utilize parallel processing capabilities of BGZF format</li> <li>Optimize memory usage for large-scale genomic analyses</li> <li>Implement streaming approaches for memory-constrained environments</li> </ul>"},{"location":"Bam/Bam/#specifications-and-standards","title":"Specifications and Standards","text":"<p>The BAM format specification is maintained by the SAM/BAM Format Specification Working Group and follows semantic versioning. The current specification defines:</p> <ul> <li>Binary encoding schemes for all data types</li> <li>Mandatory and optional field requirements</li> <li>Compatibility requirements across different software implementations</li> <li>Extension mechanisms for format evolution</li> </ul>"},{"location":"Bam/Bam/#related-formats","title":"Related Formats","text":""},{"location":"Bam/Bam/#cram-format","title":"CRAM Format","text":"<ul> <li>Reference-based compression achieving higher compression ratios</li> <li>Maintains compatibility with BAM-based tools</li> <li>Suitable for long-term archival storage</li> </ul>"},{"location":"Bam/Bam/#sam-format","title":"SAM Format","text":"<ul> <li>Human-readable text representation of alignment data</li> <li>Useful for debugging and manual inspection</li> <li>Less efficient for storage and processing</li> </ul>"},{"location":"Bam/Bam/#conclusion","title":"Conclusion","text":"<p>The BAM format represents a cornerstone technology in modern genomics, providing efficient storage and manipulation of alignment data while maintaining compatibility across diverse bioinformatics software ecosystems. Its binary structure, combined with robust compression and indexing capabilities, enables scalable analysis of large-scale sequencing datasets essential for contemporary genomic research.</p>"},{"location":"Bam/add-or-replace-read-groups/","title":"AddOrReplaceReadGroups","text":"Table of Content <ul> <li>AddOrReplaceReadGroups<ul> <li>Overview</li> <li>Read Group Metadata Significance</li> <li>Read Group Fields Specification</li> <li>Common Read Group Issues</li> <li>Tool Functionality</li> <li>Implementation Parameters</li> <li>Practical Implementation Example</li> <li>Alternative Approaches</li> <li>Quality Control and Validation</li> <li>Integration with Analysis Pipelines</li> <li>Troubleshooting Common Issues</li> <li>Best Practices</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Bam/add-or-replace-read-groups/#overview","title":"Overview","text":"<p>The <code>AddOrReplaceReadGroups</code> tool from the Picard toolkit is a critical utility for correcting or adding read group information to existing BAM files. This tool addresses scenarios where read group metadata is missing, incomplete, or incorrectly formatted, eliminating the need to perform computationally expensive re-mapping operations. Read groups are essential metadata components that enable proper sample identification, library tracking, and downstream analysis compatibility.</p>"},{"location":"Bam/add-or-replace-read-groups/#read-group-metadata-significance","title":"Read Group Metadata Significance","text":""},{"location":"Bam/add-or-replace-read-groups/#biological-context","title":"Biological Context","text":"<p>Read groups (@RG) represent fundamental metadata units in SAM/BAM files that describe the experimental context of sequencing reads:</p> <ul> <li>Sample identification: Links reads to specific biological samples</li> <li>Library preparation: Tracks different library preparation protocols</li> <li>Sequencing platform: Records the sequencing technology used</li> <li>Flow cell information: Identifies specific sequencing runs</li> <li>Lane information: Distinguishes between different sequencing lanes</li> </ul>"},{"location":"Bam/add-or-replace-read-groups/#downstream-analysis-requirements","title":"Downstream Analysis Requirements","text":"<p>Many bioinformatics tools require properly formatted read group information:</p> <ul> <li>GATK (Genome Analysis Toolkit): Mandatory for variant calling pipelines</li> <li>Variant callers: Use read group information for quality assessment</li> <li>Population genetics tools: Require sample identification for multi-sample analysis</li> <li>Quality control software: Utilize read group metadata for batch effect detection</li> </ul>"},{"location":"Bam/add-or-replace-read-groups/#read-group-fields-specification","title":"Read Group Fields Specification","text":""},{"location":"Bam/add-or-replace-read-groups/#mandatory-fields","title":"Mandatory Fields","text":"<p>The SAM/BAM specification defines several critical read group fields:</p> Field Description Example Requirement RGID Read Group Identifier lane1_sample1 Mandatory RGSM Sample Name patient_001 Mandatory RGLB Library Name lib_prep_1 Mandatory RGPL Platform/Technology ILLUMINA Mandatory RGPU Platform Unit flowcell.lane Mandatory"},{"location":"Bam/add-or-replace-read-groups/#optional-fields","title":"Optional Fields","text":"<p>Additional metadata fields provide enhanced experimental context:</p> Field Description Example Usage RGCN Sequencing Center broad_institute Optional RGDS Description WGS_run_2023 Optional RGDT Run Date 2023-10-15 Optional RGFO Flow Order TACG Optional RGKS Key Sequence TCAG Optional RGPG Programs bwa-0.7.17 Optional RGPI Predicted Insert Size 300 Optional RGPM Platform Model HiSeq2500 Optional"},{"location":"Bam/add-or-replace-read-groups/#common-read-group-issues","title":"Common Read Group Issues","text":""},{"location":"Bam/add-or-replace-read-groups/#missing-read-groups","title":"Missing Read Groups","text":"<p>Alignment software may generate BAM files without read group information, particularly when: - Command-line parameters omit read group specification - Legacy alignment tools lack read group support - Automated pipelines have configuration errors - Manual alignment processes skip metadata inclusion  </p>"},{"location":"Bam/add-or-replace-read-groups/#incorrect-read-group-information","title":"Incorrect Read Group Information","text":"<p>Existing read groups may contain erroneous information due to: - Copy-paste errors in pipeline configuration - Inconsistent naming conventions across projects - Automated sample tracking system failures - Manual metadata entry mistakes  </p>"},{"location":"Bam/add-or-replace-read-groups/#incomplete-read-group-data","title":"Incomplete Read Group Data","text":"<p>Partial read group information may result from: - Minimal compliance with mandatory fields only - Missing platform-specific information - Absent library preparation details - Incomplete sample identification  </p>"},{"location":"Bam/add-or-replace-read-groups/#tool-functionality","title":"Tool Functionality","text":""},{"location":"Bam/add-or-replace-read-groups/#core-operations","title":"Core Operations","text":"<p>AddOrReplaceReadGroups performs the following operations:</p> <ol> <li>Validation: Checks existing read group information for completeness</li> <li>Replacement: Overwrites existing read group metadata with new values</li> <li>Addition: Adds read group information to files lacking this metadata</li> <li>Consistency: Ensures all reads within the file share identical read group information</li> </ol>"},{"location":"Bam/add-or-replace-read-groups/#processing-workflow","title":"Processing Workflow","text":"<p>The tool implements a streamlined processing workflow:</p> <ol> <li>Input validation: Verifies BAM file integrity and accessibility</li> <li>Header modification: Updates the BAM header with new read group information</li> <li>Record processing: Applies read group tags to individual alignment records</li> <li>Output generation: Creates a new BAM file with corrected metadata</li> <li>Validation: Verifies the integrity of the output file</li> </ol>"},{"location":"Bam/add-or-replace-read-groups/#implementation-parameters","title":"Implementation Parameters","text":""},{"location":"Bam/add-or-replace-read-groups/#memory-management","title":"Memory Management","text":"<p>Efficient memory utilization is crucial for processing large BAM files: - Heap allocation: JVM memory configuration for optimal performance - Buffer management: Efficient I/O buffering for large datasets - Garbage collection: Optimized memory cleanup for sustained processing  </p>"},{"location":"Bam/add-or-replace-read-groups/#performance-considerations","title":"Performance Considerations","text":"<p>Several factors influence processing performance: - Input file size: Linear relationship between file size and processing time - Compression level: Trade-off between storage efficiency and processing speed  - I/O bandwidth: Storage system performance impacts overall throughput - CPU utilization: Multi-threading capabilities for parallel processing  </p>"},{"location":"Bam/add-or-replace-read-groups/#practical-implementation-example","title":"Practical Implementation Example","text":"<p>The following example demonstrates production-ready read group correction using SLURM job scheduling:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=AddOrReplaceRG\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=128G\n#SBATCH --array=1-15\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Recover BAM file name from job array\nSAMPLELANE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/bam_files)\necho ${SAMPLELANE}\n\n# Extract sample identifier (single character at position 28)\nSAMPLE=$(echo ${SAMPLELANE:28:1})\necho ${SAMPLE}\n\n# Extract lane number from filename\nLANE=$(echo ${SAMPLELANE} | awk -F'[_]' '{print $2}')\necho ${LANE}\n\n# Define input and output directories\nINPUT_DIR=results/02_Mapping/\nOUTPUT_DIR=results/04_Polishing\nmkdir -p ${OUTPUT_DIR}\n\n# Define file paths\nSORTED_BAM=${INPUT_DIR}/${SAMPLELANE}_sorted.bam\nCORRECTED_BAM=${OUTPUT_DIR}/${SAMPLELANE}_sorted_rg.bam\n\n# Create report directory\nREPORT_DIR=results/11_Reports/addorreplacereadgroup\nmkdir -p ${REPORT_DIR}\n\n# Load required software module\nmodule load picard/2.23.5\n\n# Execute read group correction\n# Mandatory fields: RGLB (Library), RGPL (Platform), RGPU (Platform Unit), RGSM (Sample)\npicard AddOrReplaceReadGroups -Xmx128000m \\\n      -I ${SORTED_BAM} \\\n      -O ${CORRECTED_BAM} \\\n      -RGID ${LANE} \\\n      -RGLB lib1 \\\n      -RGPL illumina \\\n      -RGPU unit1 \\\n      -RGSM ${SAMPLE} \\\n      2&gt; ${REPORT_DIR}/${SAMPLELANE}_added_rg.out\n</code></pre>"},{"location":"Bam/add-or-replace-read-groups/#script-analysis","title":"Script Analysis","text":"<p>Resource Allocation: - Memory: 128 GB allocation for large BAM file processing - CPU: 2 cores per task for I/O-intensive operations - Time: Extended processing window for comprehensive datasets  </p> <p>Metadata Extraction: - Dynamic parsing: Extracts sample and lane information from filenames - Flexible naming: Supports complex filename conventions - Validation: Ensures proper identifier extraction  </p> <p>Read Group Assignment: - RGID: Uses lane identifier for unique read group identification - RGSM: Assigns extracted sample name for proper sample tracking - RGPL: Specifies Illumina platform for technology identification - RGLB/RGPU: Uses standardized library and platform unit identifiers  </p>"},{"location":"Bam/add-or-replace-read-groups/#alternative-approaches","title":"Alternative Approaches","text":""},{"location":"Bam/add-or-replace-read-groups/#samtools-implementation","title":"SAMtools Implementation","text":"<p>SAMtools provides basic read group manipulation capabilities:</p> <pre><code># Add read group information\nsamtools addreplacerg -r '@RG\\tID:lane1\\tSM:sample1\\tLB:lib1\\tPL:ILLUMINA' \\\n    input.bam output.bam\n</code></pre>"},{"location":"Bam/add-or-replace-read-groups/#custom-solutions","title":"Custom Solutions","text":"<p>For specialized requirements, custom solutions may be necessary: - Scripted approaches: Shell scripts for batch processing - Programming interfaces: Python/R implementations for complex logic - Database integration: Automated metadata retrieval from LIMS systems  </p>"},{"location":"Bam/add-or-replace-read-groups/#quality-control-and-validation","title":"Quality Control and Validation","text":""},{"location":"Bam/add-or-replace-read-groups/#pre-processing-checks","title":"Pre-Processing Checks","text":"<p>Before executing read group correction: 1. File integrity: Verify BAM file completeness and accessibility 2. Header analysis: Examine existing read group information 3. Sample identification: Confirm correct sample-to-file mapping 4. Metadata validation: Verify accuracy of proposed read group information  </p>"},{"location":"Bam/add-or-replace-read-groups/#post-processing-validation","title":"Post-Processing Validation","text":"<p>After read group correction: 1. Header verification: Confirm proper read group header integration 2. Record consistency: Validate read group tag application to all records 3. File integrity: Ensure output file completeness and accessibility 4. Downstream compatibility: Test compatibility with analysis tools  </p>"},{"location":"Bam/add-or-replace-read-groups/#error-detection","title":"Error Detection","text":"<p>Common error patterns and detection strategies: - Missing fields: Automated detection of incomplete read group information - Inconsistent naming: Validation of naming convention adherence - Duplicate identifiers: Detection of non-unique read group identifiers - Platform mismatches: Verification of platform-specific information  </p>"},{"location":"Bam/add-or-replace-read-groups/#integration-with-analysis-pipelines","title":"Integration with Analysis Pipelines","text":""},{"location":"Bam/add-or-replace-read-groups/#workflow-positioning","title":"Workflow Positioning","text":"<p>Read group correction typically occurs between alignment and variant calling: 1. Raw sequencing data: FASTQ files from sequencing platforms 2. Read alignment: Generation of initial BAM files 3. Quality control: Assessment of alignment quality 4. Sorting: Coordinate-based organization of alignments 5. Read group correction: Addition or correction of metadata \u2190 Current process 6. Duplicate marking: Identification of PCR duplicates 7. Base quality recalibration: Adjustment of quality scores 8. Variant calling: Identification of genomic variants  </p>"},{"location":"Bam/add-or-replace-read-groups/#automation-considerations","title":"Automation Considerations","text":"<ul> <li>Batch processing: Efficient handling of multiple samples</li> <li>Error handling: Robust error detection and recovery mechanisms</li> <li>Resource management: Optimal utilization of computational resources</li> <li>Monitoring: Real-time tracking of processing progress and failures</li> </ul>"},{"location":"Bam/add-or-replace-read-groups/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"Bam/add-or-replace-read-groups/#memory-related-problems","title":"Memory-Related Problems","text":"<ul> <li>Insufficient heap space: Increase JVM memory allocation</li> <li>System memory limits: Monitor system-wide memory usage</li> <li>Memory leaks: Identify and address persistent memory consumption</li> </ul>"},{"location":"Bam/add-or-replace-read-groups/#file-system-issues","title":"File System Issues","text":"<ul> <li>Disk space: Ensure adequate storage for temporary and output files</li> <li>Permissions: Verify read/write access to input and output directories</li> <li>Network latency: Minimize network-based file operations</li> </ul>"},{"location":"Bam/add-or-replace-read-groups/#metadata-inconsistencies","title":"Metadata Inconsistencies","text":"<ul> <li>Naming conventions: Standardize sample and library naming schemes</li> <li>Platform information: Ensure accurate platform and technology specification</li> <li>Date formats: Standardize date representation across projects</li> </ul>"},{"location":"Bam/add-or-replace-read-groups/#best-practices","title":"Best Practices","text":""},{"location":"Bam/add-or-replace-read-groups/#metadata-management","title":"Metadata Management","text":"<ol> <li>Standardization: Implement consistent naming conventions across projects</li> <li>Documentation: Maintain comprehensive metadata documentation</li> <li>Validation: Implement automated metadata validation procedures</li> <li>Backup: Preserve original files before metadata modification</li> </ol>"},{"location":"Bam/add-or-replace-read-groups/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Memory allocation: Optimize JVM heap size based on file size</li> <li>Parallel processing: Utilize job arrays for batch processing</li> <li>Storage optimization: Use high-performance storage for temporary files</li> <li>Monitoring: Implement resource utilization monitoring</li> </ol>"},{"location":"Bam/add-or-replace-read-groups/#quality-assurance","title":"Quality Assurance","text":"<ol> <li>Testing: Validate read group correction on representative datasets</li> <li>Verification: Implement automated output validation procedures</li> <li>Documentation: Maintain detailed processing logs and reports</li> <li>Standardization: Develop standardized operating procedures</li> </ol>"},{"location":"Bam/add-or-replace-read-groups/#conclusion","title":"Conclusion","text":"<p>The AddOrReplaceReadGroups tool provides essential functionality for correcting read group metadata in BAM files, eliminating the computational expense of re-mapping operations. Proper implementation ensures downstream analysis compatibility while maintaining data integrity and traceability. Integration into automated pipelines enables efficient processing of large-scale genomic datasets with robust error handling and quality control measures. This tool represents a critical component in modern genomics workflows, bridging the gap between raw alignment data and analysis-ready datasets.</p>"},{"location":"Bam/bam-qc/","title":"BAM File Quality Control","text":"Table of Content <ul> <li>BAM File Quality Control<ul> <li>Quick Validation with samtools quickcheck</li> <li>In-depth Quality Reports with qualimap</li> <li>Read Statistics with samtools stats</li> <li>Structural Validation with picard ValidateSamFile</li> <li>Summary Table</li> </ul> </li> </ul> <p>BAM (Binary Alignment/Map) files are a key output of read mapping processes, and ensuring their integrity is crucial before proceeding to downstream analyses such as variant calling or transcript quantification. Several tools are available to perform different types of quality checks on BAM files. This section covers the most commonly used tools and strategies, along with their associated scripts in an HPC environment using SLURM.</p>"},{"location":"Bam/bam-qc/#quick-validation-with-samtools-quickcheck","title":"Quick Validation with samtools quickcheck","text":"<p>samtools quickcheck performs a fast sanity check of BAM or CRAM files. It ensures that each file: - Is readable - Has the correct format - Is not truncated  </p> <p>It does not check file content consistency or alignment logic, but is ideal for catching incomplete or corrupted files early in a pipeline.</p> <p>This validation step can be performed just after conversion in Bam format of a sam file, following the mapping/sorting of reads, or after the deduplication step.</p> <p>A list of Bam files have to be created first:</p> <pre><code>#!/bin/bash\n\n# ls -1: display only one file per line, making the output easier to read.\nls -1 results/02_Mapping | grep -E '_sorted\\.bam' | sed 's/_sorted\\.bam//g' &gt; info_files/bam_list\n</code></pre> <p>Then, we can run <code>samtools quickcheck</code>: <pre><code>#!/bin/bash\n\nmodule load samtools/1.15.1\n\nBAM_LIST=\"info_files/bam.list\"\n\nwhile IFS= read -r bam_file; do\n    echo \"Checking $bam_file\"\n    samtools quickcheck -v \"$bam_file\"\n    if [ $? -ne 0 ]; then\n        echo \"Error found in $bam_file\"\n    else\n        echo \"$bam_file is OK\"\n    fi\ndone &lt; \"$BAM_LIST\"\n</code></pre></p> <p>This script loops through a list of BAM files and runs samtools quickcheck -v on each of them, reporting any issues.</p>"},{"location":"Bam/bam-qc/#in-depth-quality-reports-with-qualimap","title":"In-depth Quality Reports with qualimap","text":""},{"location":"Bam/bam-qc/#multi-sample-overview-qualimap-multi-bamqc","title":"Multi-sample Overview: qualimap multi-bamqc","text":"<p>qualimap multi-bamqc is used to summarize multiple BAM files in a single integrated report. It gives a global overview of: - Read coverage distribution - GC bias - Mapping quality - Insert size distribution  </p> <p>It needs an input list in a specific format, which can be created with this script:</p> <pre><code>#!/bin/bash\n\n# .bam directory\nbam_folder=\"results/04_Polishing\"\n\n# qualimap multi-bamqc output file\noutput_file=\"info_files/qualimap_sample_file\"\n\n# Pattern to be removed form filename\npattern=\"_merged_marked.bam\"\n\n# Check if directory exists\nif [ ! -d \"$bam_folder\" ]; then\n    echo \"The directory $bam_folder does not exist.\"\n    exit 1\nfi\n\n# Creates the qualimap multi-bamqc input file\nfor bam_file in \"$bam_folder\"/*merged_marked.bam; do\n    # Remove pattern\n    sample_name=$(basename \"${bam_file//$pattern}\")\n\n    # Add pathway\n    echo -e \"$sample_name\\t$bam_file\" &gt;&gt; \"$output_file\"\ndone\n\necho \"The file has been created : $output_file\"\n</code></pre> <p>Then Qualimap multi-bamqc can be performed:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=qualimap_multibamqc\n#SBATCH --cpus-per-task=24\n#SBATCH --mem=32G\n...\n\nOUTPUT_DIR=\"qc/qualimap_multibamqc/\"\nmkdir -p ${OUTPUT_DIR}\n\nmodule load qualimap/2.2.2b\n\nqualimap multi-bamqc -c -d info_files/qualimap_sample_file --run-bamqc \\\n    -outdir ${OUTPUT_DIR} -nr 10000 \\\n    --java-mem-size=32G -r\n</code></pre> <p>This script assumes that the file qualimap_sample_file contains paths and labels of all BAM files to include in the report.</p>"},{"location":"Bam/bam-qc/#per-sample-reports-qualimap-bamqc","title":"Per-sample Reports: qualimap bamqc","text":"<p>When detailed metrics are needed for each sample individually, qualimap bamqc is the tool of choice.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=qualimap\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n...\n\nBAM_FOLDER=\"results/04_Polishing\"\nOUTPUT_FOLDER=\"qc/qualimap\"\nLOG_FOLDER=\"results/11_Reports/qualimap\"\nmkdir -p \"${OUTPUT_FOLDER}\" \"${LOG_FOLDER}\"\n\nfor BAM_FILE in \"${BAM_FOLDER}\"/*_marked.bam; do\n    FILENAME_NO_EXT=$(basename \"${BAM_FILE%_marked.bam}\")\n\n    sbatch &lt;&lt;EOF\n#!/bin/bash\n#SBATCH --job-name=qlmp_${FILENAME_NO_EXT}\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n\nmodule load qualimap/2.2.2b\n\nqualimap bamqc -bam \"${BAM_FILE}\" -c -nt 16 \\\n    --java-mem-size=64G \\\n    -outdir \"${OUTPUT_FOLDER}/${FILENAME_NO_EXT}\" \\\n    &amp;&gt; \"${LOG_FOLDER}/qualimap_${FILENAME_NO_EXT}.log\"\nEOF\ndone\n</code></pre> <p>Key Differences:</p> Feature multi-bamqc bamqc Input Many BAMs via a sample file Single BAM file Output Single HTML report with comparisons One report per BAM Use case Project-level summary Per-sample diagnostics Runtime Faster overall Can be heavy for large BAMs"},{"location":"Bam/bam-qc/#read-statistics-with-samtools-stats","title":"Read Statistics with samtools stats","text":"<p>samtools stats generates a comprehensive textual summary of alignment metrics such as:     \u2022   Total reads     \u2022   Mapped reads     \u2022   Insert sizes     \u2022   Base composition     \u2022   Error rates</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=stats\n#SBATCH --cpus-per-task=16\n...\n\nBAM_FOLDER=\"results/04_Polishing\"\nOUTPUT_FOLDER=\"qc/samtools\"\nLOG_FOLDER=\"results/11_Reports/samtools\"\nmkdir -p \"$OUTPUT_FOLDER\" \"$LOG_FOLDER\"\n\nfor BAM_FILE in \"$BAM_FOLDER\"/*marked.bam; do\n    FILENAME_NO_EXT=$(basename \"${BAM_FILE%.bam}\")\n\n    sbatch &lt;&lt;EOF\n#!/bin/bash\n#SBATCH --job-name=stats_${FILENAME_NO_EXT}\n#SBATCH --cpus-per-task=16\n...\n\nmodule load samtools/1.15.1\n\nsamtools stats -@ 16 \\\n    -r resources/genomes/GCF_035046485.1_AalbF5_genomic.fna \\\n    \"$BAM_FILE\" \\\n    1&gt; \"${OUTPUT_FOLDER}/${FILENAME_NO_EXT}_stats.txt\" \\\n    2&gt; \"${LOG_FOLDER}/${FILENAME_NO_EXT}.stats.log\"\nEOF\ndone\n</code></pre> <p>The result is a plain-text .txt file with hundreds of metrics, suitable for parsing or plotting with tools like plot-bamstats.</p>"},{"location":"Bam/bam-qc/#structural-validation-with-picard-validatesamfile","title":"Structural Validation with picard ValidateSamFile","text":"<p>To detect deeper issues in file structure and metadata tags, Picard ValidateSamFile is a reference tool. It checks: - Invalid mate information - NM and MD tag consistency - Missing headers - Unexpected reference contigs  </p> <pre><code>#!/bin/bash\n#SBATCH --job-name=validatesam\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=32G\n...\n\nBAM_FOLDER=\"results/04_Polishing\"\nOUTPUT_FOLDER=\"qc/validatesam\"\nLOG_FOLDER=\"results/11_Reports/validatesamfiles\"\nmkdir -p \"${OUTPUT_FOLDER}\" \"${LOG_FOLDER}\"\n\nfor BAM_FILE in \"${BAM_FOLDER}\"/*marked.bam; do\n    FILENAME_NO_EXT=$(basename \"${BAM_FILE%.bam}\")\n\n    sbatch &lt;&lt;EOF\n#!/bin/bash\n#SBATCH --job-name=validatesam_${FILENAME_NO_EXT}\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=32G\n\nmodule load picard/2.23.5\n\npicard ValidateSamFile -Xmx32000m \\\n    -I \"${BAM_FILE}\" \\\n    -R resources/genomes/GCF_035046485.1_AalbF5_genomic.fna \\\n    -O \"${OUTPUT_FOLDER}/${FILENAME_NO_EXT}_ValidateSam.txt\" \\\n    -M SUMMARY \\\n    2&gt; ${LOG_FOLDER}/${FILENAME_NO_EXT}_validate_bam.log\nEOF\ndone\n</code></pre> <p>Output: - A text summary report of all found issues - A log file containing additional details and error traces  </p>"},{"location":"Bam/bam-qc/#summary-table","title":"Summary Table","text":"Tool Purpose Output Type Runtime Impact samtools quickcheck Basic format and integrity check Terminal messages Very fast qualimap bamqc Per-sample QC metrics HTML, PDF Medium qualimap multi-bamqc Global QC overview of many BAMs HTML Fast samtools stats Detailed read stats Text Fast ValidateSamFile Deep structure and tag validation Text Medium"},{"location":"Bam/genomicsdbimport/","title":"GenomicsDBImport and GenotypeGVCFs in GATK4 Workflow","text":"Table of Content <ul> <li>GenomicsDBImport and GenotypeGVCFs in GATK4 Workflow<ul> <li>Introduction</li> <li>The GVCF-Based Workflow</li> <li>GenomicsDBImport: Database Creation</li> <li>GenotypeGVCFs: Joint Genotyping</li> <li>Advantages of the GenomicsDB Approach</li> <li>Best Practices and Recommendations</li> <li>Integration with Downstream Analysis</li> <li>Troubleshooting Common Issues</li> <li>Future Directions</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Bam/genomicsdbimport/#introduction","title":"Introduction","text":"<p>The GATK4 variant calling workflow has evolved to incorporate a two-step approach for joint genotyping across multiple samples. This methodology addresses the computational challenges of analyzing large cohorts while maintaining accuracy in variant detection. The process involves two critical steps: GenomicsDBImport for data consolidation and GenotypeGVCFs for final variant calling across all samples simultaneously.</p> <p>This approach represents a significant improvement over earlier methods by enabling scalable joint analysis of hundreds to thousands of samples, which is essential for population-scale genomics studies and large-scale genetic association analyses.</p>"},{"location":"Bam/genomicsdbimport/#the-gvcf-based-workflow","title":"The GVCF-Based Workflow","text":""},{"location":"Bam/genomicsdbimport/#background-and-rationale","title":"Background and Rationale","text":"<p>The genomic variant call format (GVCF), output of the Variant-Caller <code>HaplotypeCaller</code>, extends the standard VCF format by including information about non-variant sites, providing a comprehensive representation of genotype confidence across the entire genome. This approach enables:</p> <ul> <li>Efficient scaling: Individual samples can be processed independently to generate GVCFs</li> <li>Joint analysis: Multiple samples can be analyzed together for improved variant detection</li> <li>Incremental analysis: New samples can be added to existing datasets without reprocessing all data</li> <li>Quality improvement: Joint analysis provides better genotype quality assessments</li> </ul>"},{"location":"Bam/genomicsdbimport/#workflow-overview","title":"Workflow Overview","text":"<p>The typical GVCF workflow consists of:</p> <ol> <li>Individual variant calling: HaplotypeCaller generates GVCF files for each sample</li> <li>Database creation: GenomicsDBImport consolidates GVCFs into a queryable database</li> <li>Joint genotyping: GenotypeGVCFs performs final variant calling across all samples</li> <li>Quality control and filtering: Standard VCF processing and filtration</li> </ol>"},{"location":"Bam/genomicsdbimport/#genomicsdbimport-database-creation","title":"GenomicsDBImport: Database Creation","text":""},{"location":"Bam/genomicsdbimport/#purpose-and-functionality","title":"Purpose and Functionality","text":"<p>GenomicsDBImport is a GATK4 tool that creates a centralized database from multiple GVCF files. This database serves as an efficient storage and query system for genomic variant data across large cohorts.</p>"},{"location":"Bam/genomicsdbimport/#key-features","title":"Key Features","text":"<ul> <li>Efficient storage: Compressed representation of variant data across samples</li> <li>Scalable architecture: Handles hundreds to thousands of samples</li> <li>Chromosomal organization: Databases are typically created per chromosome for parallel processing</li> <li>Memory optimization: Reduces memory requirements for subsequent joint genotyping</li> </ul>"},{"location":"Bam/genomicsdbimport/#technical-implementation","title":"Technical Implementation","text":"<p>The following script demonstrates a typical GenomicsDBImport implementation:</p> <pre><code>#!/bin/bash\n################### Configuration SLURM ############################\n#SBATCH -A invalbo\n#SBATCH --job-name=genomicsDBimport\n#SBATCH --time=15-23:00:00\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 6\n#SBATCH --mem=64G\n#SBATCH --array 1-4\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n######################################################################\n###########################################\n###---  Creating genomics databases  ---###\n###########################################\n#\n# This script uses GATK4 GenomicsDBImport to create a database of variants across individuals. \n# The path to the genomicsdb-workspace-path argument must be to a non-existant directory. \n# The program manual suggests this can be an empty directory, but it performed better\n# when it created a new directory.\n# \n# Requirements:\n#   - GATK4 (module: gatk4/4.2.6.1)\n#   - file 'ae_gvcf_{CHROM}_list' that specifies files and samples\n#   (use generate_sample_name_map.sh script)\n#   - This was run as an array job with a job for each scaffold\n#\n#\n#\n##########################################\nmodule load gatk4/4.2.6.1\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chroms_list)\ngatk --java-options \"-Xmx20G\" GenomicsDBImport \\\n    --genomicsdb-workspace-path results/05_Variants/genomicsdb_${CHROM}/ \\\n    -L ${CHROM} \\\n    --sample-name-map info_files/ae_gvcf_${CHROM}_list \\\n    --tmp-dir /tmp/ \\\n    --reader-threads 6 \\\n    2&gt; results/11_Reports/genomicsdb/Ae_albo_chr$CHROM-gvcf.out\n</code></pre>"},{"location":"Bam/genomicsdbimport/#critical-parameters-and-considerations","title":"Critical Parameters and Considerations","text":""},{"location":"Bam/genomicsdbimport/#workspace-path-management","title":"Workspace Path Management","text":"<p>The <code>--genomicsdb-workspace-path</code> parameter requires careful attention: - Must point to a non-existent directory: The tool creates the database structure - Directory creation: GATK4 performs optimally when creating new directories rather than using existing empty ones - Permissions: Ensure appropriate write permissions for the target directory</p>"},{"location":"Bam/genomicsdbimport/#sample-name-mapping","title":"Sample Name Mapping","text":"<p>The <code>--sample-name-map</code> parameter requires a properly formatted file that maps GVCF files to sample names: <pre><code>sample1    /path/to/sample1.g.vcf.gz\nsample2    /path/to/sample2.g.vcf.gz\nsample3    /path/to/sample3.g.vcf.gz\n</code></pre></p>"},{"location":"Bam/genomicsdbimport/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Memory allocation: Typically requires 20-64GB RAM depending on cohort size</li> <li>CPU utilization: Multi-threading through <code>--reader-threads</code> improves performance</li> <li>Temporary storage: Adequate <code>/tmp</code> space is essential for intermediate files</li> </ul>"},{"location":"Bam/genomicsdbimport/#chromosomal-parallelization","title":"Chromosomal Parallelization","text":"<p>The script utilizes SLURM job arrays to process chromosomes in parallel: - Scalability: Each chromosome processed independently - Resource optimization: Parallel processing reduces total runtime - Memory efficiency: Per-chromosome processing reduces memory requirements  </p>"},{"location":"Bam/genomicsdbimport/#genotypegvcfs-joint-genotyping","title":"GenotypeGVCFs: Joint Genotyping","text":""},{"location":"Bam/genomicsdbimport/#purpose-and-methodology","title":"Purpose and Methodology","text":"<p>GenotypeGVCFs represents the final step in the GVCF workflow, performing joint genotyping across all samples in the GenomicsDB. This tool applies sophisticated statistical models to determine the most likely genotypes for each sample at each variant site.</p>"},{"location":"Bam/genomicsdbimport/#statistical-framework","title":"Statistical Framework","text":"<p>The joint genotyping process involves:</p> <ul> <li>Likelihood calculation: Computing genotype likelihoods across all samples simultaneously</li> <li>Allele frequency estimation: Determining population-level allele frequencies</li> <li>Genotype assignment: Assigning most probable genotypes based on joint analysis</li> <li>Quality assessment: Calculating genotype quality scores and variant confidence</li> </ul>"},{"location":"Bam/genomicsdbimport/#implementation-script","title":"Implementation Script","text":"<pre><code>#!/bin/bash\n################### Configuration SLURM ############################\n#SBATCH -A invalbo\n#SBATCH --job-name=genotype_gvcfs\n#SBATCH --time=23:00:00\n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 6\n#SBATCH --mem=25G\n#SBATCH --array 1-4\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n######################################################################\nFILE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/gendb_inputs)\nFOLDER=$(echo $FILE | awk '{print $1}')\nCHROM=$(echo $FILE | awk '{print $2}')\nmodule load gatk4/4.2.6.1\ngatk --java-options \"-Xmx20G\" GenotypeGVCFs \\\n   -R resources/genomes/AalbF5_filtered.fasta \\\n   -V gendb://${FOLDER}/ \\\n   -O Ae_albo_chr_${CHROM}.vcf.gz \\\n   2&gt; results/11_Reports/genotypegvcfs/Ae_albo_chr_${CHROM}-gvcf.out\n</code></pre>"},{"location":"Bam/genomicsdbimport/#key-parameters-and-options","title":"Key Parameters and Options","text":""},{"location":"Bam/genomicsdbimport/#database-input","title":"Database Input","text":"<p>The <code>-V gendb://</code> parameter specifies the GenomicsDB created by GenomicsDBImport: - URI format: Uses the <code>gendb://</code> protocol to access the database - Path specification: Points to the workspace directory created by GenomicsDBImport - Chromosome-specific: Each database typically contains data for a single chromosome</p>"},{"location":"Bam/genomicsdbimport/#reference-genome","title":"Reference Genome","text":"<p>The <code>-R</code> parameter specifies the reference genome: - Consistency requirement: Must be identical to the reference used for alignment and initial variant calling - Indexing: Reference must be properly indexed (<code>.fai</code> and <code>.dict</code> files) - Version control: Ensure consistent reference genome versions across all analysis steps</p>"},{"location":"Bam/genomicsdbimport/#output-format","title":"Output Format","text":"<p>The <code>-O</code> parameter specifies the output VCF file: - Compression: <code>.gz</code> extension enables automatic compression - Naming convention: Typically includes chromosome or region information - Indexing: GATK automatically creates tabix indices for compressed VCF files</p>"},{"location":"Bam/genomicsdbimport/#advantages-of-the-genomicsdb-approach","title":"Advantages of the GenomicsDB Approach","text":""},{"location":"Bam/genomicsdbimport/#computational-efficiency","title":"Computational Efficiency","text":"<p>The GenomicsDB workflow provides several computational advantages:</p> <ul> <li>Memory optimization: Reduces memory requirements compared to direct multi-sample analysis</li> <li>Parallel processing: Enables efficient parallelization across genomic regions</li> <li>Incremental analysis: New samples can be added without reprocessing existing data</li> <li>Storage efficiency: Compressed database format reduces storage requirements</li> </ul>"},{"location":"Bam/genomicsdbimport/#statistical-benefits","title":"Statistical Benefits","text":"<p>Joint genotyping through GenotypeGVCFs offers statistical improvements:</p> <ul> <li>Improved variant detection: Joint analysis increases power to detect rare variants</li> <li>Better genotype quality: Population-level information improves genotype assignments</li> <li>Allele frequency accuracy: More precise estimation of population allele frequencies</li> <li>Reduced false positives: Joint analysis helps distinguish true variants from sequencing artifacts</li> </ul>"},{"location":"Bam/genomicsdbimport/#scalability-considerations","title":"Scalability Considerations","text":"<p>The workflow scales effectively for large cohorts:</p> <ul> <li>Linear scaling: Performance scales approximately linearly with sample size</li> <li>Resource predictability: Resource requirements are predictable based on cohort size</li> <li>Checkpoint capability: Database creation provides natural checkpoints for large analyses</li> <li>Distributed processing: Compatible with distributed computing environments</li> </ul>"},{"location":"Bam/genomicsdbimport/#best-practices-and-recommendations","title":"Best Practices and Recommendations","text":""},{"location":"Bam/genomicsdbimport/#resource-planning","title":"Resource Planning","text":"<p>Effective resource allocation is crucial for optimal performance:</p>"},{"location":"Bam/genomicsdbimport/#memory-requirements","title":"Memory Requirements","text":"<ul> <li>GenomicsDBImport: 20-64GB RAM depending on cohort size and chromosome length</li> <li>GenotypeGVCFs: 20-40GB RAM typically sufficient for most analyses</li> <li>Scaling factors: Memory requirements increase with sample number and genomic complexity</li> </ul>"},{"location":"Bam/genomicsdbimport/#storage-considerations","title":"Storage Considerations","text":"<ul> <li>Temporary space: Ensure adequate <code>/tmp</code> space for intermediate files</li> <li>Database storage: Plan for database sizes approximately 10-20% of input GVCF total size</li> <li>Output storage: Final VCF files typically smaller than input GVCFs due to compression</li> </ul>"},{"location":"Bam/genomicsdbimport/#processing-time","title":"Processing Time","text":"<ul> <li>GenomicsDBImport: 6-24 hours per chromosome for large cohorts</li> <li>GenotypeGVCFs: 2-12 hours per chromosome depending on variant density</li> <li>Parallelization: Chromosome-level parallelization provides optimal throughput</li> </ul>"},{"location":"Bam/genomicsdbimport/#quality-control","title":"Quality Control","text":"<p>Implementing robust quality control measures ensures reliable results:</p>"},{"location":"Bam/genomicsdbimport/#input-validation","title":"Input Validation","text":"<ul> <li>GVCF integrity: Verify all input GVCFs are properly formatted and complete</li> <li>Sample consistency: Ensure consistent sample naming and metadata</li> <li>Reference consistency: Verify identical reference genomes across all samples</li> </ul>"},{"location":"Bam/genomicsdbimport/#database-validation","title":"Database Validation","text":"<ul> <li>Completeness checks: Verify successful database creation for all chromosomes</li> <li>Sample inclusion: Confirm all expected samples are present in the database</li> <li>Genomic coverage: Validate expected genomic regions are represented</li> </ul>"},{"location":"Bam/genomicsdbimport/#output-quality-assessment","title":"Output Quality Assessment","text":"<ul> <li>Variant statistics: Examine variant counts and quality distributions</li> <li>Sample-level metrics: Assess per-sample variant counts and quality scores</li> <li>Population genetics: Evaluate allele frequency spectra and Hardy-Weinberg equilibrium</li> </ul>"},{"location":"Bam/genomicsdbimport/#integration-with-downstream-analysis","title":"Integration with Downstream Analysis","text":""},{"location":"Bam/genomicsdbimport/#variant-filtering","title":"Variant Filtering","text":"<p>The output VCF files require appropriate filtering:</p> <ul> <li>Hard filtering: Apply quality thresholds for variant and genotype quality</li> <li>Variant Quality Score Recalibration (VQSR): Recommended for large cohorts</li> <li>Population-specific filtering: Apply allele frequency and Hardy-Weinberg equilibrium filters</li> <li>Functional annotation: Add gene and transcript annotations for interpretation</li> </ul>"},{"location":"Bam/genomicsdbimport/#population-genomics-applications","title":"Population Genomics Applications","text":"<p>The joint genotyping approach is particularly valuable for:</p> <ul> <li>Genome-wide association studies (GWAS): Improved variant detection and quality</li> <li>Population structure analysis: Comprehensive variant sets for demographic inference</li> <li>Selection analysis: High-quality variant calls for detecting natural selection</li> <li>Phylogenetic studies: Reliable variant sets for evolutionary analysis</li> </ul>"},{"location":"Bam/genomicsdbimport/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"Bam/genomicsdbimport/#genomicsdbimport-challenges","title":"GenomicsDBImport Challenges","text":""},{"location":"Bam/genomicsdbimport/#memory-issues","title":"Memory Issues","text":"<ul> <li>Symptoms: Out-of-memory errors or extremely slow performance</li> <li>Solutions: Increase memory allocation, reduce batch size, or split by genomic regions</li> </ul>"},{"location":"Bam/genomicsdbimport/#directory-conflicts","title":"Directory Conflicts","text":"<ul> <li>Symptoms: Errors related to existing directories or permissions</li> <li>Solutions: Ensure target directories don't exist, verify write permissions</li> </ul>"},{"location":"Bam/genomicsdbimport/#sample-map-errors","title":"Sample Map Errors","text":"<ul> <li>Symptoms: Sample mapping failures or missing samples</li> <li>Solutions: Verify file paths, check sample name formatting, ensure file accessibility</li> </ul>"},{"location":"Bam/genomicsdbimport/#genotypegvcfs-challenges","title":"GenotypeGVCFs Challenges","text":""},{"location":"Bam/genomicsdbimport/#database-access-issues","title":"Database Access Issues","text":"<ul> <li>Symptoms: Cannot access GenomicsDB or database corruption errors</li> <li>Solutions: Verify database integrity, check file permissions, ensure complete GenomicsDBImport</li> </ul>"},{"location":"Bam/genomicsdbimport/#reference-genome-mismatches","title":"Reference Genome Mismatches","text":"<ul> <li>Symptoms: Contig errors or reference inconsistencies</li> <li>Solutions: Verify identical reference genomes, check indexing, ensure consistent naming</li> </ul>"},{"location":"Bam/genomicsdbimport/#resource-limitations","title":"Resource Limitations","text":"<ul> <li>Symptoms: Slow performance or timeout errors</li> <li>Solutions: Increase memory allocation, optimize temporary storage, consider genomic region splitting</li> </ul>"},{"location":"Bam/genomicsdbimport/#future-directions","title":"Future Directions","text":"<p>The GenomicsDB approach continues to evolve with ongoing developments:</p> <ul> <li>Cloud integration: Enhanced support for cloud-based storage and computing</li> <li>Scalability improvements: Optimizations for ultra-large cohorts (&gt;100,000 samples)</li> <li>Format enhancements: Improved database formats for better compression and access</li> <li>Tool integration: Better integration with other genomics tools and workflows</li> </ul>"},{"location":"Bam/genomicsdbimport/#conclusion","title":"Conclusion","text":"<p>The GenomicsDBImport and GenotypeGVCFs workflow represents a mature and efficient approach to joint variant calling in large genomic cohorts. By separating database creation from joint genotyping, this methodology provides computational efficiency, statistical accuracy, and scalability essential for modern population genomics studies.</p> <p>The two-step approach addresses the fundamental challenge of analyzing large numbers of samples while maintaining the statistical power of joint analysis. Proper implementation of these tools, with attention to resource allocation and quality control, enables researchers to perform robust variant calling on cohorts ranging from dozens to thousands of samples.</p> <p>Success with this workflow depends on careful attention to computational resources, data management, and quality control throughout the process. The scripts and best practices outlined here provide a foundation for implementing this approach in diverse genomic research contexts.</p>"},{"location":"Bam/indel-realignment/","title":"Indel Realignment Workflow (for UnifiedGenotyper Variant-caller)","text":"Table of Content <ul> <li>Indel Realignment Workflow (for UnifiedGenotyper Variant-caller)<ul> <li>Overview</li> <li>Scientific Rationale</li> <li>Indel Realignment Methodology</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Bam/indel-realignment/#overview","title":"Overview","text":"<p>Indel realignment represents a critical preprocessing step in genomic variant calling pipelines, particularly when employing legacy tools such as GATK's <code>UnifiedGenotyper</code>. This process addresses systematic alignment artifacts that occur around insertion and deletion (indel) polymorphisms, which are prevalent in mosquito genomes and can significantly impact variant calling accuracy.</p>"},{"location":"Bam/indel-realignment/#scientific-rationale","title":"Scientific Rationale","text":""},{"location":"Bam/indel-realignment/#alignment-artifacts-in-mosquito-genomes","title":"Alignment Artifacts in Mosquito Genomes","text":"<p>Anopheles genomes, like those of other dipteran species, exhibit substantial structural variation including frequent indel polymorphisms. Standard read alignment algorithms often produce suboptimal alignments in regions flanking true indels, leading to:</p> <ul> <li>False positive SNP calls: Misaligned reads create apparent sequence mismatches</li> <li>Mapping quality degradation: Ambiguous alignments reduce confidence scores</li> <li>Allelic bias: Preferential alignment of reference-matching reads over variant-containing reads</li> </ul>"},{"location":"Bam/indel-realignment/#legacy-tool-compatibility","title":"Legacy Tool Compatibility","text":"<p>The UnifiedGenotyper, while superseded by more recent algorithms, remains relevant for specific applications involving:</p> <ul> <li>Historical data consistency: Maintaining compatibility with existing 1000 Genomes Project mosquito datasets</li> <li>Comparative genomics: Ensuring methodological consistency across temporal datasets</li> <li>Resource-constrained environments: Lower computational requirements compared to modern alternatives</li> </ul>"},{"location":"Bam/indel-realignment/#indel-realignment-methodology","title":"Indel Realignment Methodology","text":""},{"location":"Bam/indel-realignment/#two-stage-process","title":"Two-Stage Process","text":""},{"location":"Bam/indel-realignment/#stage-1-target-identification","title":"Stage 1: Target Identification","text":"<p>The <code>RealignerTargetCreator</code> tool identifies genomic intervals requiring realignment by: - Detecting regions with elevated mismatch rates - Identifying potential indel sites based on alignment patterns - Creating target intervals for focused realignment</p>"},{"location":"Bam/indel-realignment/#stage-2-local-realignment","title":"Stage 2: Local Realignment","text":"<p>The <code>IndelRealigner</code> performs local realignment within identified target regions by: - Constructing alternative alignment hypotheses - Evaluating alignment quality scores - Selecting optimal alignments that minimize mismatches</p> <pre><code>##!/bin/bash\n\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=08:00:00\n#SBATCH --job-name=indelrealignment\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=64G\n#SBATCH --array=1-26\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nBAMNAME=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/bam_list)\n\n# R\u00e9pertoire de sortie pour les fichiers\nOUTPUT_DIR=results/04_Polishing\n\n# Fichier d'entr\u00e9e\nMARKED_BAM=${OUTPUT_DIR}/${BAMNAME}_marked.bam\n\n# Fichier filtr\u00e9\nFILTERED_BAM=${OUTPUT_DIR}/${BAMNAME}_marked_filtered.bam\n\n# Fichier bam tagg\u00e9\nTAGGED_BAM=${OUTPUT_DIR}/${BAMNAME}_marked_filtered_tagged.bam\n\n# Fichiers d'intervalles \u00e0 r\u00e9aligner\nINTERVALS_FILE=${OUTPUT_DIR}/${BAMNAME}_marked_filtered_tagged.intervals\n\n# Fichier bam r\u00e9align\u00e9\nREALIGNED_BAM=${OUTPUT_DIR}/${BAMNAME}_marked_filtered_realigned.bam\n\n# Report dir\nREPORT_DIR=results/11_Reports/indelrealignment\nmkdir -p ${REPORT_DIR}\n\n# Load module\nmodule load samtools/1.15.1\n\nsamtools view -h -q 30 ${MARKED_BAM} -o ${FILTERED_BAM}\n\n# Load module\nmodule load picard/2.23.5\n\npicard SetNmMdAndUqTags \\\n      -R resources/genome/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa \\\n      -I ${FILTERED_BAM} \\\n      -O ${TAGGED_BAM} &gt; {log} 2&gt;&amp;1\n\nsamtools index ${TAGGED_BAM}\n\n# Load module\nmodule load gatk/3.8\n\ngatk3 -T RealignerTargetCreator \\\n      --num_threads 2 \\\n      -R resources/genome/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa \\\n      -I ${TAGGED_BAM} \\\n      --defaultBaseQualities 20 --filter_reads_with_N_cigar \\\n      --out ${INTERVALS_FILE} \\\n      2&gt; ${REPORT_DIR}/${BAMNAME}_filtered_intervals.out\n\ngatk3 -Xmx64g -T IndelRealigner \\\n      -I ${TAGGED_BAM} \\\n      -R resources/genome/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa \\\n      --targetIntervals ${INTERVALS_FILE} \\\n      --defaultBaseQualities 20 --filter_reads_with_N_cigar \\\n      --out ${REALIGNED_BAM} \\\n      2&gt; ${REPORT_DIR}/${BAMNAME}_filtered_realignment.out\n\nawk -F '[:-]' 'BEGIN { OFS = \"\\t\" } { if( $3 == \"\") { print $1, $2-1, $2 } else { print $1, $2-1, $3}}' ${INTERVALS_FILE} \\\n      1&gt; ${OUTPUT_DIR}/${BAMNAME}_realignertargetcreator.bed \\\n      2&gt; ${REPORT_DIR}/${BAMNAME}_awk_filtered.out\n</code></pre>"},{"location":"Bam/indel-realignment/#conclusion","title":"Conclusion","text":"<p>Indel realignment represents an essential preprocessing step for variant calling in mosquito genomics, particularly when employing the legacy tool UnifiedGenotyper. The systematic correction of alignment artifacts around indel sites significantly improves variant calling accuracy and enables robust population genomic analyses of Anopheles species. Integration with 1000 Genomes mosquito datasets ensures methodological consistency and facilitates comparative genomic studies across different research groups and time periods.</p>"},{"location":"Bam/mark-duplicates/","title":"MarkDuplicates","text":"Table of Content <ul> <li>MarkDuplicates<ul> <li>Overview</li> <li>Biological Context of Duplicate Reads</li> <li>Duplicate Detection Algorithms</li> <li>Implementation Strategies</li> <li>Tool Parameters and Configuration</li> <li>Practical Implementation Example</li> <li>Metrics and Quality Assessment</li> <li>Alternative Tools and Approaches</li> <li>Integration with Analysis Pipelines</li> <li>Performance Optimization</li> <li>Troubleshooting Common Issues</li> <li>Quality Control and Validation</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Bam/mark-duplicates/#overview","title":"Overview","text":"<p>MarkDuplicates is a critical preprocessing tool in genomic analysis pipelines that identifies and manages duplicate reads in aligned sequencing data. Duplicate reads arise from various sources during library preparation and sequencing, potentially introducing bias in downstream analyses. This tool implements sophisticated algorithms to detect, mark, or remove duplicate reads while preserving the highest quality representatives, thereby improving the accuracy of variant calling and other genomic analyses.</p>"},{"location":"Bam/mark-duplicates/#biological-context-of-duplicate-reads","title":"Biological Context of Duplicate Reads","text":""},{"location":"Bam/mark-duplicates/#sources-of-duplication","title":"Sources of Duplication","text":"<p>Duplicate reads in next-generation sequencing data originate from multiple sources:</p>"},{"location":"Bam/mark-duplicates/#pcr-duplicates","title":"PCR Duplicates","text":"<ul> <li>Library amplification: PCR steps during library preparation create identical copies</li> <li>Cluster amplification: Bridge PCR on flow cells generates optical duplicates</li> <li>Uneven amplification: Preferential amplification of certain DNA fragments</li> </ul>"},{"location":"Bam/mark-duplicates/#optical-duplicates","title":"Optical Duplicates","text":"<ul> <li>Cluster misidentification: Sequencing instruments incorrectly separate adjacent clusters</li> <li>Signal bleeding: Fluorescent signals from adjacent clusters interfere</li> <li>Image processing artifacts: Base calling software misinterprets cluster boundaries</li> </ul>"},{"location":"Bam/mark-duplicates/#sequencing-artifacts","title":"Sequencing Artifacts","text":"<ul> <li>Index hopping: Incorrect assignment of reads to samples in multiplexed runs</li> <li>Carry-over contamination: Residual DNA from previous sequencing runs</li> <li>Template switching: Polymerase jumping between templates during amplification</li> </ul>"},{"location":"Bam/mark-duplicates/#impact-on-downstream-analysis","title":"Impact on Downstream Analysis","text":"<p>Duplicate reads can significantly affect analytical outcomes:</p>"},{"location":"Bam/mark-duplicates/#variant-calling-bias","title":"Variant Calling Bias","text":"<ul> <li>False positive variants: Duplicates artificially inflate variant allele frequencies</li> <li>Coverage distortion: Uneven duplicate distribution creates coverage artifacts</li> <li>Quality score inflation: Multiple observations of the same molecule inflate confidence</li> </ul>"},{"location":"Bam/mark-duplicates/#population-genetics-analysis","title":"Population Genetics Analysis","text":"<ul> <li>Allele frequency skewing: Duplicates bias population-level statistics</li> <li>Linkage disequilibrium: Artificial correlation between nearby variants</li> <li>Demographic inference: Incorrect population size and migration estimates</li> </ul>"},{"location":"Bam/mark-duplicates/#copy-number-analysis","title":"Copy Number Analysis","text":"<ul> <li>Depth-based calling: Duplicates create false copy number gains</li> <li>Breakpoint detection: Duplicates obscure structural variant boundaries</li> <li>Normalization issues: Uneven duplication affects read depth normalization</li> </ul>"},{"location":"Bam/mark-duplicates/#duplicate-detection-algorithms","title":"Duplicate Detection Algorithms","text":""},{"location":"Bam/mark-duplicates/#coordinate-based-detection","title":"Coordinate-Based Detection","text":"<p>The primary algorithm for duplicate identification relies on alignment coordinates:</p>"},{"location":"Bam/mark-duplicates/#single-end-reads","title":"Single-End Reads","text":"<ul> <li>Primary criteria: Identical 5' mapping coordinates on the same strand</li> <li>Secondary criteria: Identical CIGAR strings for complex alignments</li> <li>Quality assessment: Retention of highest mapping quality reads</li> </ul>"},{"location":"Bam/mark-duplicates/#paired-end-reads","title":"Paired-End Reads","text":"<ul> <li>Template coordinates: Identical 5' coordinates for both read pairs</li> <li>Orientation consistency: Proper pair orientation requirements</li> <li>Insert size validation: Consistent template length measurements</li> </ul>"},{"location":"Bam/mark-duplicates/#sequence-based-detection","title":"Sequence-Based Detection","text":"<p>Advanced algorithms incorporate sequence information: - Exact sequence matching: Identical read sequences indicate duplicates - Fuzzy matching: Sequence similarity with allowable mismatches - Barcode integration: Unique molecular identifiers (UMIs) for accurate detection  </p>"},{"location":"Bam/mark-duplicates/#optical-duplicate-detection","title":"Optical Duplicate Detection","text":"<p>Specialized algorithms for optical duplicates: - Pixel distance calculation: Physical distance between clusters on flow cells - Tile-based analysis: Localized duplicate detection within sequencing tiles - Platform-specific parameters: Instrument-specific distance thresholds  </p>"},{"location":"Bam/mark-duplicates/#implementation-strategies","title":"Implementation Strategies","text":""},{"location":"Bam/mark-duplicates/#marking-vs-removal","title":"Marking vs. Removal","text":"<p>Two primary approaches for managing duplicates:</p>"},{"location":"Bam/mark-duplicates/#marking-duplicates","title":"Marking Duplicates","text":"<ul> <li>FLAG modification: Sets bit 0x400 in BAM FLAG field</li> <li>Preservation: Maintains all reads while identifying duplicates</li> <li>Flexibility: Allows downstream tools to handle duplicates appropriately</li> <li>Reversibility: Enables recovery of original data if needed</li> </ul>"},{"location":"Bam/mark-duplicates/#removing-duplicates","title":"Removing Duplicates","text":"<ul> <li>Physical removal: Eliminates duplicate reads from output files</li> <li>Storage efficiency: Reduces file size and storage requirements</li> <li>Processing speed: Accelerates downstream analysis by reducing data volume</li> <li>Irreversibility: Permanent loss of duplicate read information</li> </ul>"},{"location":"Bam/mark-duplicates/#quality-based-selection","title":"Quality-Based Selection","text":"<p>When multiple duplicates exist, selection criteria include: - Mapping quality: Highest MAPQ score indicates best alignment - Base quality: Sum of base quality scores across read length - Alignment score: Aligner-specific scoring metrics - Read completeness: Preference for reads with fewer soft-clipped bases  </p>"},{"location":"Bam/mark-duplicates/#tool-parameters-and-configuration","title":"Tool Parameters and Configuration","text":""},{"location":"Bam/mark-duplicates/#core-parameters","title":"Core Parameters","text":"<p>Essential configuration options for MarkDuplicates:</p> Parameter Description Default Recommendation REMOVE_DUPLICATES Remove duplicate reads false true for storage optimization CREATE_INDEX Generate BAI index false true for downstream compatibility ASSUME_SORTED Input is coordinate sorted false true for sorted BAM files MAX_RECORDS_IN_RAM Memory buffer size 500000 Adjust based on available RAM TMP_DIR Temporary file directory system temp High-speed storage location"},{"location":"Bam/mark-duplicates/#advanced-parameters","title":"Advanced Parameters","text":"<p>Specialized options for specific use cases:</p> Parameter Description Usage OPTICAL_DUPLICATE_PIXEL_DISTANCE Optical duplicate threshold Platform-specific tuning READ_NAME_REGEX Custom read name parsing Non-standard naming schemes DUPLICATE_SCORING_STRATEGY Quality scoring method SUM_OF_BASE_QUALITIES CLEAR_DT Remove existing duplicate tags Reprocessing scenarios ADD_PG_TAG_TO_READS Add program group tags Pipeline tracking"},{"location":"Bam/mark-duplicates/#memory-management","title":"Memory Management","text":"<p>Optimal memory configuration strategies: - Heap allocation: 1-2 GB per million reads as baseline - Buffer sizing: Balance memory usage with I/O efficiency - Garbage collection: Tune JVM parameters for sustained performance - Temporary storage: Ensure adequate disk space for intermediate files  </p>"},{"location":"Bam/mark-duplicates/#practical-implementation-example","title":"Practical Implementation Example","text":"<p>The following example demonstrates production-ready duplicate marking using SLURM job scheduling:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=mark_bams\n#SBATCH -p long\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=128G\n#SBATCH --array=1-56\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Extract BAM filename from job array list\nBAMNAME=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/bam_list)\n\n# Define output directory structure\nOUTPUT_DIR=results/04_Polishing\nmkdir -p ${OUTPUT_DIR}\n\n# Define output file path\nMARKED_BAM=${OUTPUT_DIR}/${BAMNAME}_marked.bam\n\n# Create quality control directory\nMETRICS_DIR=\"qc/markdup\"\nmkdir -p ${METRICS_DIR}\n\n# Create reporting directory\nREPORT_DIR=results/11_Reports/markduplicates\nmkdir -p ${REPORT_DIR}\n\n# Load required software module\nmodule load picard/2.23.5\n\n# Execute duplicate marking with removal\npicard MarkDuplicates -Xmx128000m \\\n  --REMOVE_DUPLICATES true --CREATE_INDEX true \\\n  --INPUT results/02_Mapping/${BAMNAME}_sorted.bam \\\n  --OUTPUT ${MARKED_BAM} \\\n  --TMP_DIR /tmp/ \\\n  --METRICS_FILE ${METRICS_DIR}/${BAMNAME}.metrics \\\n  &amp;&gt; ${REPORT_DIR}/${BAMNAME}_marked.out\n</code></pre>"},{"location":"Bam/mark-duplicates/#script-analysis","title":"Script Analysis","text":"<p>Resource Allocation: - Memory: 128 GB allocation for processing large BAM files - CPU: 2 cores for I/O intensive operations - Time: Extended processing window for comprehensive datasets - Array processing: 56 parallel jobs for batch processing  </p> <p>File Management: - Input validation: Systematic processing of BAM file lists - Directory structure: Organized output and quality control directories - Temporary storage: Efficient use of local temporary directories  </p> <p>Quality Control Integration: - Metrics collection: Comprehensive duplicate statistics generation - Reporting: Detailed logging for process monitoring - Indexing: Automatic BAI index creation for downstream compatibility  </p> <p>Processing Configuration: - Duplicate removal: Physical elimination of duplicate reads - Index creation: Automatic generation of BAM indices - Memory optimization: Efficient memory allocation for large datasets  </p>"},{"location":"Bam/mark-duplicates/#metrics-and-quality-assessment","title":"Metrics and Quality Assessment","text":""},{"location":"Bam/mark-duplicates/#duplicate-statistics","title":"Duplicate Statistics","text":"<p>MarkDuplicates generates comprehensive metrics including:</p>"},{"location":"Bam/mark-duplicates/#library-level-metrics","title":"Library-Level Metrics","text":"<ul> <li>Total reads: Complete read count in input file  </li> <li>Duplicate reads: Number of identified duplicates  </li> <li>Duplication rate: Percentage of reads identified as duplicates  </li> <li>Optical duplicate rate: Percentage of optically duplicated reads  </li> </ul>"},{"location":"Bam/mark-duplicates/#molecular-metrics","title":"Molecular Metrics","text":"<ul> <li>Unique molecules: Estimated number of distinct DNA molecules  </li> <li>Molecules with duplicates: Count of molecules generating duplicates  </li> <li>Mean reads per molecule: Average amplification rate  </li> </ul>"},{"location":"Bam/mark-duplicates/#quality-metrics","title":"Quality Metrics","text":"<ul> <li>Mapping quality distribution: Quality scores of retained vs. duplicate reads  </li> <li>Insert size distribution: Template length statistics for paired-end reads  </li> <li>Duplicate class distribution: Breakdown by duplicate type  </li> </ul>"},{"location":"Bam/mark-duplicates/#interpretation-guidelines","title":"Interpretation Guidelines","text":""},{"location":"Bam/mark-duplicates/#acceptable-duplication-rates","title":"Acceptable Duplication Rates","text":"<ul> <li>Whole genome sequencing: 5-15% typical for well-prepared libraries  </li> <li>Exome sequencing: 10-25% due to target enrichment bias  </li> <li>RNA sequencing: 20-50% depending on library complexity  </li> <li>Amplicon sequencing: 30-70% expected for targeted approaches  </li> </ul>"},{"location":"Bam/mark-duplicates/#quality-indicators","title":"Quality Indicators","text":"<ul> <li>Low optical duplicates: &lt;5% indicates proper cluster density  </li> <li>Consistent across lanes: Similar rates suggest uniform processing  </li> <li>Insert size distribution: Proper library size selection  </li> <li>Mapping quality retention: High-quality reads preferentially retained  </li> </ul>"},{"location":"Bam/mark-duplicates/#alternative-tools-and-approaches","title":"Alternative Tools and Approaches","text":""},{"location":"Bam/mark-duplicates/#samtools-implementations","title":"SAMtools Implementations","text":"<p>Basic duplicate marking capabilities: <pre><code># Mark duplicates using SAMtools\nsamtools markdup input_sorted.bam output_marked.bam\n\n# Remove duplicates using SAMtools\nsamtools markdup -r input_sorted.bam output_deduped.bam\n</code></pre></p>"},{"location":"Bam/mark-duplicates/#specialized-tools","title":"Specialized Tools","text":""},{"location":"Bam/mark-duplicates/#umi-based-deduplication","title":"UMI-Based Deduplication","text":"<ul> <li>UMI-tools: Handles unique molecular identifiers  </li> <li>Picard MarkDuplicates with UMI: Enhanced UMI support  </li> <li>Custom solutions: Laboratory-specific UMI strategies  </li> </ul>"},{"location":"Bam/mark-duplicates/#high-performance-implementations","title":"High-Performance Implementations","text":"<ul> <li>Sambamba: Parallelized duplicate marking  </li> <li>Biobambam2: Optimized for large-scale processing  </li> <li>Custom implementations: Specialized for specific sequencing platforms  </li> </ul>"},{"location":"Bam/mark-duplicates/#cloud-based-solutions","title":"Cloud-Based Solutions","text":"<p>Modern genomics platforms offer managed duplicate detection: - Google Cloud Genomics: Integrated duplicate marking - AWS Genomics: Scalable duplicate detection - Azure Genomics: Cloud-native processing pipelines  </p>"},{"location":"Bam/mark-duplicates/#integration-with-analysis-pipelines","title":"Integration with Analysis Pipelines","text":""},{"location":"Bam/mark-duplicates/#workflow-positioning","title":"Workflow Positioning","text":"<p>Duplicate marking typically occurs after sorting and before variant calling:  </p> <ol> <li>Raw sequencing data: FASTQ files from sequencing platforms  </li> <li>Quality control: Adapter trimming and quality filtering  </li> <li>Read alignment: Mapping to reference genome  </li> <li>Coordinate sorting: Organization by genomic position  </li> <li>Read group assignment: Metadata correction and standardization  </li> <li>Duplicate marking: Identification and management \u2190 Current process </li> <li>Base quality recalibration: Systematic error correction  </li> <li>Variant calling: Genomic variant identification  </li> <li>Variant filtering: Quality-based variant selection  </li> </ol>"},{"location":"Bam/mark-duplicates/#downstream-compatibility","title":"Downstream Compatibility","text":"<p>Proper duplicate handling ensures compatibility with: - GATK Best Practices: Standard genomics pipeline requirements - Variant callers: Accurate allele frequency estimation - Population genetics tools: Unbiased allele frequency calculations - Structural variant detection: Accurate breakpoint identification  </p>"},{"location":"Bam/mark-duplicates/#performance-optimization","title":"Performance Optimization","text":""},{"location":"Bam/mark-duplicates/#computational-strategies","title":"Computational Strategies","text":""},{"location":"Bam/mark-duplicates/#memory-optimization","title":"Memory Optimization","text":"<ul> <li>Heap tuning: Optimal JVM memory allocation  </li> <li>Buffer management: Efficient I/O buffer sizing  </li> <li>Garbage collection: Appropriate GC strategy selection  </li> </ul>"},{"location":"Bam/mark-duplicates/#io-optimization","title":"I/O Optimization","text":"<ul> <li>Temporary storage: High-speed storage for intermediate files  </li> <li>Parallel processing: Multi-threaded duplicate detection  </li> <li>Streaming algorithms: Reduced memory footprint for large files  </li> </ul>"},{"location":"Bam/mark-duplicates/#scalability-considerations","title":"Scalability Considerations","text":"<ul> <li>Distributed computing: Cluster-based processing for large datasets  </li> <li>Cloud deployment: Elastic scaling based on workload  </li> <li>Resource monitoring: Real-time performance tracking  </li> </ul>"},{"location":"Bam/mark-duplicates/#best-practices","title":"Best Practices","text":""},{"location":"Bam/mark-duplicates/#pre-processing-optimization","title":"Pre-Processing Optimization","text":"<ol> <li>Coordinate sorting: Ensure proper BAM file sorting  </li> <li>Read group validation: Verify proper read group assignment  </li> <li>File integrity: Validate BAM file completeness  </li> <li>Resource planning: Estimate computational requirements  </li> </ol>"},{"location":"Bam/mark-duplicates/#processing-optimization","title":"Processing Optimization","text":"<ol> <li>Memory allocation: Size heap based on input file characteristics  </li> <li>Temporary storage: Use local, high-speed storage  </li> <li>Parallel execution: Utilize job arrays for batch processing  </li> <li>Error handling: Implement robust error detection and recovery  </li> </ol>"},{"location":"Bam/mark-duplicates/#post-processing-validation","title":"Post-Processing Validation","text":"<ol> <li>Metrics review: Analyze duplicate statistics for quality assessment  </li> <li>File validation: Verify output file integrity  </li> <li>Index generation: Ensure proper BAM indexing  </li> <li>Downstream testing: Validate compatibility with analysis tools  </li> </ol>"},{"location":"Bam/mark-duplicates/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"Bam/mark-duplicates/#memory-related-problems","title":"Memory-Related Problems","text":""},{"location":"Bam/mark-duplicates/#outofmemoryerror","title":"OutOfMemoryError","text":"<ul> <li>Symptoms: JVM heap space exhaustion during processing  </li> <li>Solutions: Increase heap size, reduce records in RAM, optimize GC  </li> <li>Prevention: Estimate memory requirements based on file size  </li> </ul>"},{"location":"Bam/mark-duplicates/#system-memory-exhaustion","title":"System Memory Exhaustion","text":"<ul> <li>Symptoms: System becomes unresponsive during processing  </li> <li>Solutions: Reduce concurrent jobs, optimize memory allocation  </li> <li>Prevention: Monitor system memory usage patterns  </li> </ul>"},{"location":"Bam/mark-duplicates/#performance-issues","title":"Performance Issues","text":""},{"location":"Bam/mark-duplicates/#slow-processing","title":"Slow Processing","text":"<ul> <li>Causes: Inadequate memory, slow storage, inefficient parameters  </li> <li>Solutions: Optimize memory allocation, use faster storage, tune parameters  </li> <li>Monitoring: Track processing rates and resource utilization  </li> </ul>"},{"location":"Bam/mark-duplicates/#io-bottlenecks","title":"I/O Bottlenecks","text":"<ul> <li>Causes: Network storage, insufficient disk bandwidth  </li> <li>Solutions: Use local storage, optimize I/O patterns  </li> <li>Prevention: Benchmark storage performance before processing  </li> </ul>"},{"location":"Bam/mark-duplicates/#data-integrity-issues","title":"Data Integrity Issues","text":""},{"location":"Bam/mark-duplicates/#incomplete-processing","title":"Incomplete Processing","text":"<ul> <li>Symptoms: Truncated output files, missing indices  </li> <li>Solutions: Verify input file integrity, check disk space  </li> <li>Prevention: Implement comprehensive error checking  </li> </ul>"},{"location":"Bam/mark-duplicates/#metrics-inconsistencies","title":"Metrics Inconsistencies","text":"<ul> <li>Symptoms: Unexpected duplication rates, anomalous statistics  </li> <li>Solutions: Validate input data quality, review processing parameters  </li> <li>Prevention: Establish quality control thresholds  </li> </ul>"},{"location":"Bam/mark-duplicates/#quality-control-and-validation","title":"Quality Control and Validation","text":""},{"location":"Bam/mark-duplicates/#pre-processing-checks","title":"Pre-Processing Checks","text":"<p>Essential validation steps before duplicate marking: 1. File integrity: Verify BAM file completeness and accessibility 2. Sort order: Confirm coordinate-based sorting 3. Read groups: Validate proper read group assignment 4. Sample consistency: Verify sample identification accuracy  </p>"},{"location":"Bam/mark-duplicates/#processing-monitoring","title":"Processing Monitoring","text":"<p>Real-time monitoring during duplicate marking: 1. Resource utilization: Track memory, CPU, and I/O usage 2. Progress tracking: Monitor processing completion rates 3. Error detection: Identify processing failures early 4. Quality metrics: Review duplicate statistics during processing  </p>"},{"location":"Bam/mark-duplicates/#post-processing-validation_1","title":"Post-Processing Validation","text":"<p>Comprehensive validation after duplicate marking: 1. Output integrity: Verify complete file generation 2. Metrics analysis: Review duplicate statistics for anomalies 3. Indexing verification: Confirm proper BAM index generation 4. Downstream compatibility: Test with analysis tools  </p>"},{"location":"Bam/mark-duplicates/#conclusion","title":"Conclusion","text":"<p>MarkDuplicates represents a fundamental component of modern genomic analysis pipelines, providing essential functionality for managing duplicate reads that can significantly impact analytical accuracy. Proper implementation requires careful consideration of computational resources, algorithm selection, and quality control measures. The tool's integration into automated pipelines enables scalable processing of large-scale genomic datasets while maintaining data integrity and analytical reliability. Understanding the biological sources of duplication, implementing appropriate detection strategies, and maintaining rigorous quality control standards are essential for generating high-quality genomic datasets suitable for downstream analysis and clinical applications.</p>"},{"location":"Bam/merge-and-mark-duplicates/","title":"BAM File Merging and Duplicate Marking Workflow","text":"Table of Content <ul> <li>BAM File Merging and Duplicate Marking Workflow<ul> <li>Overview</li> <li>Sequential Processing Strategy</li> <li>Implementation Script</li> <li>Technical Specifications</li> <li>Quality Control Outputs</li> <li>Computational Considerations</li> </ul> </li> </ul>"},{"location":"Bam/merge-and-mark-duplicates/#overview","title":"Overview","text":"<p>The processing of high-throughput sequencing data often involves multiple sequencing lanes or runs for a single biological sample to achieve adequate coverage depth or to distribute the sequencing load across multiple flow cells. This necessitates a systematic approach to consolidate the resulting alignment files and identify PCR duplicates that may introduce bias in downstream analyses.</p>"},{"location":"Bam/merge-and-mark-duplicates/#sequential-processing-strategy","title":"Sequential Processing Strategy","text":""},{"location":"Bam/merge-and-mark-duplicates/#1-bam-file-merging","title":"1. BAM File Merging","text":"<p>The initial step involves merging multiple BAM files corresponding to different sequencing lanes of the same biological sample. This consolidation is performed using Picard's <code>MergeSamFiles</code> tool, which combines aligned reads while preserving read group information and maintaining coordinate-based sorting.</p> <p>Key advantages of merging first: - Computational efficiency: Processing a single merged file is more resource-efficient than handling multiple separate files in subsequent steps - Read group preservation: Maintains lane-specific metadata essential for quality control and bias detection - Coordinate optimization: Ensures proper genomic coordinate sorting across all merged reads  </p>"},{"location":"Bam/merge-and-mark-duplicates/#2-duplicate-marking","title":"2. Duplicate Marking","text":"<p>Following the merge operation, PCR and optical duplicates are identified and marked using Picard's <code>MarkDuplicates</code> tool. This step is crucial for maintaining data quality and preventing artificial inflation of coverage metrics.</p> <p>Scientific rationale for post-merge duplicate detection: - Cross-lane duplicate identification: Duplicates may occur across different sequencing lanes, which can only be detected after merging - Improved accuracy: Global duplicate detection across the entire dataset provides more accurate duplicate identification than lane-specific processing - Standardized metrics: Consolidated duplicate statistics reflect the true duplication rate for the biological sample  </p>"},{"location":"Bam/merge-and-mark-duplicates/#implementation-script","title":"Implementation Script","text":"<p>The workflow is implemented through the following SLURM-compatible bash script:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=08:00:00\n#SBATCH --job-name=merge_bams\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=16G\n#SBATCH --array=1-22\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nBAMNAME=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/bam_list)\n\n# R\u00e9pertoire de sortie pour les fichiers fusionn\u00e9s\nOUTPUT_DIR=results/04_Polishing\nmkdir -p $OUTPUT_DIR\n\n# Nom de l'\u00e9chantillon extrait du nom du fichier BAM\nSAMPLE_NAME=$(echo $BAMNAME | awk -F'[_]' '{print $1\"_\"$2\"_\"}')\n\n# Fichiers BAM de diff\u00e9rentes lanes pour le m\u00eame \u00e9chantillon\nLANE_BAMS=($(ls results/02_Mapping/${SAMPLE_NAME}*sorted.bam))\n\n# Fichier de sortie fusionn\u00e9\nMERGED_FILE=$OUTPUT_DIR/${SAMPLE_NAME}merged.bam\n\n# QC directory\nMETRICS_DIR=\"qc/markdup\"\nmkdir -p ${METRICS_DIR}\n\n# Load module\nmodule load picard/2.23.5\n\n# Utilisation de picard MergeSamFiles pour fusionner les fichiers BAM\npicard MergeSamFiles \\\n      $(for bam in \"${LANE_BAMS[@]}\"; do echo \"-I $bam\"; done) \\\n      -O $MERGED_FILE \\\n      --USE_THREADING true \\\n      --SORT_ORDER coordinate \\\n      2&gt; results/11_Reports/mergesam/${SAMPLELANE}-SortSam.out\n\n# Suppression des fichiers temporaires si n\u00e9cessaire\n# rm \"${LANE_BAMS[@]}\"\n\necho \"Fusion des fichiers BAM pour l'\u00e9chantillon $SAMPLE_NAME termin\u00e9e.\"\n\npicard MarkDuplicates -Xmx16000M \\\n  --REMOVE_DUPLICATES true --CREATE_INDEX true \\\n  --INPUT ${MERGED_FILE} \\\n  --OUTPUT ${OUTPUT_DIR}/${SAMPLE_NAME}merged_marked.bam \\\n  --TMP_DIR /tmp/ \\\n  --METRICS_FILE ${METRICS_DIR}/${SAMPLE_NAME}-merged_marked.metrics \\\n  2&gt; results/11_Reports/markduplicates/${SAMPLELANE}_merged_marked.out\n</code></pre>"},{"location":"Bam/merge-and-mark-duplicates/#technical-specifications","title":"Technical Specifications","text":""},{"location":"Bam/merge-and-mark-duplicates/#merge-parameters","title":"Merge Parameters","text":"<ul> <li>Threading: Enabled for improved performance (<code>--USE_THREADING true</code>)</li> <li>Sort order: Maintains coordinate-based sorting for downstream compatibility</li> <li>Input handling: Dynamic input file specification using shell array expansion</li> </ul>"},{"location":"Bam/merge-and-mark-duplicates/#duplicate-marking-parameters","title":"Duplicate Marking Parameters","text":"<ul> <li>Memory allocation: 16GB heap space (<code>-Xmx16000M</code>) for large datasets</li> <li>Duplicate removal: Physical removal of duplicate reads (<code>--REMOVE_DUPLICATES true</code>)</li> <li>Index creation: Automatic BAI index generation (<code>--CREATE_INDEX true</code>)</li> <li>Quality metrics: Comprehensive duplicate statistics output</li> </ul>"},{"location":"Bam/merge-and-mark-duplicates/#quality-control-outputs","title":"Quality Control Outputs","text":"<p>The workflow generates several quality control files: - Merge logs: Detailed processing information for the merge operation - Duplicate metrics: Statistics on duplication rates and patterns - BAM indices: Binary indices for efficient random access  </p>"},{"location":"Bam/merge-and-mark-duplicates/#computational-considerations","title":"Computational Considerations","text":"<p>This workflow is optimized for high-performance computing environments with SLURM job scheduling. The array-based execution allows parallel processing of multiple samples while maintaining resource efficiency through single-threaded operations with moderate memory requirements (16GB RAM, 8-hour time limit).</p> <p>The sequential merge-then-mark approach ensures optimal data quality while minimizing computational overhead, making it suitable for large-scale genomic studies requiring robust duplicate detection across multi-lane sequencing datasets.</p>"},{"location":"Bam/polishing/","title":"Polishing of BAM Files","text":"Table of Content <ul> <li>Polishing of BAM Files<ul> <li>Step 1 \u2014 Fixing NM and MD Tags with samtools calmd</li> <li>Step 2 \u2014 Fixing Mate Pair Information with Picard</li> <li>Final Output</li> </ul> </li> </ul> <p>After mapping reads to a reference genome and marking duplicates, polishing the resulting BAM files is essential to ensure the integrity of downstream analyses. This step resolves specific errors often detected by tools such as Picard ValidateSamFile, including: - ERROR:INVALID_TAG_NM - ERROR:MATE_NOT_FOUND </p> <p>This section describes a two-step polishing pipeline that corrects these errors using samtools calmd and picard FixMateInformation.</p>"},{"location":"Bam/polishing/#step-1-fixing-nm-and-md-tags-with-samtools-calmd","title":"Step 1 \u2014 Fixing NM and MD Tags with samtools calmd","text":"<p>samtools calmd recomputes the NM (edit distance) and MD (mismatch positions) tags based on the reference genome. These tags are crucial for tools that perform variant calling or error detection, such as Picard.</p> <p>We apply this correction to all BAM files with the suffix *marked.bam:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=23:00:00\n#SBATCH --job-name=calmd\n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 1\n#SBATCH --mem=1G\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Treat and correct ERROR:INVALID_TAG_NM found by Picard ValidateSamFile\n\n# Define path to .bam files\nBAM_FOLDER=\"results/04_Polishing\"\n\n# Check if directory exists\nif [ ! -d \"$BAM_FOLDER\" ]; then\n    echo \"Le dossier $BAM_FOLDER n'existe pas.\"\n    exit 1\nfi\n\n# Create directory for log files\nLOG_FOLDER=\"results/11_Reports/samtools\"\nmkdir -p \"$LOG_FOLDER\"\n\n# Iterate through all .bam files and submit job\nfor BAM_FILE in \"$BAM_FOLDER\"/*marked.bam; do\n    # Get file name without extension\n    FILENAME_NO_EXT=$(basename \"${BAM_FILE%.bam}\")\n\n    # Submit job for each file\n    sbatch &lt;&lt;EOF\n#!/bin/bash\n#SBATCH -A invalbo\n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH --time=00:30:00\n#SBATCH --job-name=calmd_${FILENAME_NO_EXT}\n#SBATCH --output=Cluster_logs/calmd_${FILENAME_NO_EXT}_%j.out\n#SBATCH --error=Cluster_logs/calmd_${FILENAME_NO_EXT}_%j.err\n#SBATCH --cpus-per-task 4\n#SBATCH --mem=2G\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n\nmodule load samtools/1.15.1\n\nsamtools calmd -@ 4 -b \\\n    \"$BAM_FILE\" \\\n    resources/genome/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa \\\n    &gt; \"${BAM_FOLDER}\"/\"${FILENAME_NO_EXT}\".nm.bam \\\n    2&gt; \"${LOG_FOLDER}\"/\"${FILENAME_NO_EXT}\".calmd.log\n\nEOF\n\ndone\n</code></pre> <p>This script: - Iterates over all marked.bam files - Runs samtools calmd to fix NM/MD tags - Writes* a .nm.bam output file and logs errors  </p>"},{"location":"Bam/polishing/#step-2-fixing-mate-pair-information-with-picard","title":"Step 2 \u2014 Fixing Mate Pair Information with Picard","text":"<p>To resolve MATE_NOT_FOUND errors, we use Picard FixMateInformation. However, this tool requires the input BAM file to be sorted by queryname. The process is as follows:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=00:02:00\n#SBATCH --job-name=fixmate\n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 1\n#SBATCH --mem=1G\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Treat and correct ERROR:MATE_NOT_FOUND found by Picard ValidateSamFile\n# Use Picard ValidateSamFile then samtools calmd first.\n\n\n# Define path to .bam files\nBAM_FOLDER=\"results/04_Polishing\"\n\n# Check if directory exists\nif [ ! -d \"$BAM_FOLDER\" ]; then\n    echo \"Le dossier $BAM_FOLDER n'existe pas.\"\n    exit 1\nfi\n\n# Create directory for log files\nLOG_FOLDER=\"results/11_Reports/samtools\"\nmkdir -p \"$LOG_FOLDER\"\n\n# Iterate through all .bam files and submit job\nfor BAM_FILE in \"$BAM_FOLDER\"/*marked.nm.bam; do\n    # Get file name without extension\n    FILENAME_NO_EXT=$(basename \"${BAM_FILE%.nm.bam}\")\n\n    # Submit job for each file\n    sbatch &lt;&lt;EOF\n#!/bin/bash\n#SBATCH -A invalbo\n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH --time=01:00:00\n#SBATCH --job-name=fixmate_${FILENAME_NO_EXT}\n#SBATCH --output=Cluster_logs/fixmate_${FILENAME_NO_EXT}_%j.out\n#SBATCH --error=Cluster_logs/fixmate_${FILENAME_NO_EXT}_%j.err\n#SBATCH --cpus-per-task 4\n#SBATCH --mem=2G\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n\nmodule load picard/2.23.5\n\n# Sort by queryname\npicard SortSam \\\n        -I \"${BAM_FILE}\" \\\n        -O \"${BAM_FOLDER}\"/\"${FILENAME_NO_EXT}\".sorted_by_queryname.bam \\\n        --SORT_ORDER queryname --TMP_DIR tmp/ &gt; \"${LOG_FOLDER}\"/\"${FILENAME_NO_EXT}\".sorted_by_queryname.log 2&gt;&amp;1\n\n# Apply fixmate correction\npicard FixMateInformation \\\n        -I \"${BAM_FOLDER}\"/\"${FILENAME_NO_EXT}\".sorted_by_queryname.bam \\\n        -O \"${BAM_FOLDER}\"/\"${FILENAME_NO_EXT}\".fixed_mates.bam \\\n        --TMP_DIR tmp/ &gt; \"${LOG_FOLDER}\"/\"${FILENAME_NO_EXT}\".fixed_mates.log 2&gt;&amp;1\n\n# Sort by coordinates\npicard SortSam \\\n        -I \"${BAM_FOLDER}\"/\"${FILENAME_NO_EXT}\".fixed_mates.bam \\\n        -O \"${BAM_FOLDER}\"/\"${FILENAME_NO_EXT}\".fixed.bam \\\n        --SORT_ORDER coordinate --CREATE_INDEX true \\\n        --TMP_DIR tmp/ &gt; \"${LOG_FOLDER}\"/\"${FILENAME_NO_EXT}\".sorted_by_coordinates.log 2&gt;&amp;1\n\nrm -rf \"${BAM_FOLDER}\"/\"${FILENAME_NO_EXT}\".sorted_by_queryname.bam\nrm -rf \"${BAM_FOLDER}\"/\"${FILENAME_NO_EXT}\".fixed_mates.bam\n\nEOF\n\ndone\n</code></pre> <p>This script performs:     1.  Sorting by queryname     2.  Running FixMateInformation to link properly paired reads     3.  Sorting again by coordinate (required by GATK, IGV, etc.)     4.  Indexing the final BAM     5.  Cleaning up intermediate files  </p>"},{"location":"Bam/polishing/#final-output","title":"Final Output","text":"<p>After these two polishing steps, each sample has a clean and valid BAM file:</p> <p>results/04_Polishing/Sample_1.fixed.bam</p> <p>These files: - Are free of common structural and tag-based errors - Contain updated NM and MD fields - Have correctly paired reads - Are sorted and indexed</p> <p>They are now ready for high-quality downstream analysis such as variant calling, coverage analysis, or visualization in genome browsers.</p>"},{"location":"Bam/sort-bam/","title":"SAM/BAM File Sorting","text":"Table of Content <ul> <li>SAM/BAM File Sorting<ul> <li>Overview</li> <li>Sorting Strategies</li> <li>Sorting Algorithms and Performance</li> <li>Implementation Tools</li> <li>Computational Considerations</li> <li>Quality Control and Validation</li> <li>Practical Implementation Example</li> <li>Best Practices</li> <li>Integration with Analysis Pipelines</li> <li>Troubleshooting Common Issues</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Bam/sort-bam/#overview","title":"Overview","text":"<p>SAM/BAM file sorting is a fundamental preprocessing step in genomic analysis pipelines that organizes alignment records according to specific criteria. Proper sorting is essential for downstream applications including variant calling, coverage analysis, and visualization tools. The sorting process optimizes data access patterns and enables efficient indexing for random access operations.</p>"},{"location":"Bam/sort-bam/#sorting-strategies","title":"Sorting Strategies","text":""},{"location":"Bam/sort-bam/#coordinate-based-sorting","title":"Coordinate-Based Sorting","text":"<p>Coordinate sorting arranges alignment records by their genomic position, following the order: 1. Reference sequence: Sorted by reference sequence name (typically chromosomal order) 2. Position: Sorted by leftmost mapping position (POS field) 3. Strand orientation: Forward strand alignments before reverse strand at the same position  </p> <p>This sorting order is mandatory for most downstream applications and enables: - Efficient variant calling algorithms - Proper mate-pair processing - Optimized coverage calculations - Compatible indexing with BAI/CSI formats  </p>"},{"location":"Bam/sort-bam/#query-name-sorting","title":"Query Name Sorting","text":"<p>Query name sorting organizes records alphabetically by read identifier (QNAME field). This sorting strategy is particularly useful for: - Mate-pair analysis and validation - Duplicate detection algorithms - Quality control assessments - Conversion to FASTQ format  </p>"},{"location":"Bam/sort-bam/#unsorted-files","title":"Unsorted Files","text":"<p>Unsorted BAM files maintain the original alignment order, typically reflecting the input sequence order. While computationally faster to generate, unsorted files have limited utility for most analytical applications.</p>"},{"location":"Bam/sort-bam/#sorting-algorithms-and-performance","title":"Sorting Algorithms and Performance","text":""},{"location":"Bam/sort-bam/#memory-based-sorting","title":"Memory-Based Sorting","text":"<p>Modern sorting implementations utilize in-memory algorithms for optimal performance: - Quicksort variants: Efficient for smaller datasets fitting in available RAM - Merge sort: Stable sorting with predictable O(n log n) performance - Hybrid approaches: Combine multiple algorithms based on data characteristics  </p>"},{"location":"Bam/sort-bam/#external-sorting","title":"External Sorting","text":"<p>For datasets exceeding available memory, external sorting algorithms are employed: - Multi-way merge: Divides data into memory-sized chunks - Temporary file management: Utilizes disk storage for intermediate results - I/O optimization: Minimizes disk access through efficient buffering  </p>"},{"location":"Bam/sort-bam/#implementation-tools","title":"Implementation Tools","text":""},{"location":"Bam/sort-bam/#samtools","title":"SAMtools","text":"<p>SAMtools provides the standard implementation for BAM sorting:</p> <pre><code># Coordinate sorting\nsamtools sort -o output_sorted.bam input.bam\n\n# Query name sorting\nsamtools sort -n -o output_qname_sorted.bam input.bam\n\n# Memory optimization\nsamtools sort -m 2G -@ 8 -o output_sorted.bam input.bam\n</code></pre>"},{"location":"Bam/sort-bam/#picard-tools","title":"Picard Tools","text":"<p>Picard SortSam offers advanced sorting capabilities with extensive configuration options:</p> <pre><code>picard SortSam \\\n    INPUT=input.sam \\\n    OUTPUT=output_sorted.bam \\\n    SORT_ORDER=coordinate \\\n    CREATE_INDEX=true\n</code></pre>"},{"location":"Bam/sort-bam/#gnu-sort","title":"GNU Sort","text":"<p>For specialized applications, GNU sort can process SAM files directly:</p> <pre><code># Header-aware coordinate sorting\n(samtools view -H input.sam; samtools view input.sam | \\\nsort -k3,3V -k4,4n) | samtools view -b &gt; output_sorted.bam\n</code></pre>"},{"location":"Bam/sort-bam/#computational-considerations","title":"Computational Considerations","text":""},{"location":"Bam/sort-bam/#memory-requirements","title":"Memory Requirements","text":"<p>Memory allocation significantly impacts sorting performance: - Minimum requirements: 2-4 GB for typical whole-genome datasets - Optimal allocation: 8-16 GB enables in-memory sorting for most applications - Memory scaling: Linear relationship between file size and optimal memory allocation  </p>"},{"location":"Bam/sort-bam/#parallelization","title":"Parallelization","text":"<p>Modern sorting implementations support parallel processing: - Thread-level parallelism: Multiple threads for concurrent sorting operations - Process-level parallelism: Independent sorting of file segments - Distributed computing: Cluster-based sorting for extremely large datasets  </p>"},{"location":"Bam/sort-bam/#storage-considerations","title":"Storage Considerations","text":"<p>Temporary storage requirements during sorting: - Disk space: 2-3x input file size for intermediate files - I/O bandwidth: High-speed storage improves sorting performance - Network considerations: Minimize network I/O during sorting operations  </p>"},{"location":"Bam/sort-bam/#quality-control-and-validation","title":"Quality Control and Validation","text":""},{"location":"Bam/sort-bam/#sort-order-verification","title":"Sort Order Verification","text":"<p>Validation ensures proper sorting implementation: - Coordinate validation: Verify monotonic position ordering within chromosomes - Reference sequence order: Confirm proper chromosomal ordering - Flag consistency: Validate mate-pair relationships in coordinate-sorted files  </p>"},{"location":"Bam/sort-bam/#performance-monitoring","title":"Performance Monitoring","text":"<p>Key metrics for sorting performance assessment: - Processing time: Total wall-clock time for sorting operation - Memory utilization: Peak and average memory usage - I/O statistics: Read/write operations and throughput - Error rates: Validation of sorting correctness  </p>"},{"location":"Bam/sort-bam/#practical-implementation-example","title":"Practical Implementation Example","text":"<p>The following example demonstrates a production-ready BAM sorting pipeline using SLURM job scheduling:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=sortsam\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 4\n#SBATCH --mem=30G\n#SBATCH --array 1-56\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Extract sample identifier from job array\nSAMPLE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/fastq_files)\necho ${SAMPLE}\n\n# Load required software module\nmodule load picard/2.23.5\n\n# Execute coordinate-based sorting with indexing\npicard SortSam -Xmx40G \\\n  -I results/02_Mapping/${SAMPLE}.sam \\\n  -O results/02_Mapping/${SAMPLE}_sorted.bam \\\n  -SO coordinate \\\n  --CREATE_INDEX true --MAX_RECORDS_IN_RAM 500000 \\\n  2&gt; results/11_Reports/sortsam/${SAMPLE}-SortSam.out\n</code></pre>"},{"location":"Bam/sort-bam/#script-components-analysis","title":"Script Components Analysis","text":"<p>Resource Allocation: - Memory: 30 GB SLURM allocation with 40 GB Java Virtual Machine (JVM) heap - CPU: 4 cores per task for parallel processing - Time: Extended 2-day limit for large datasets  </p> <p>Processing Parameters: - Sort Order: Coordinate-based sorting for downstream compatibility - Index Creation: Automatic BAI index generation - Memory Management: 500,000 records in RAM for optimal performance  </p> <p>Array Processing: - Parallel Execution: 56 simultaneous sorting jobs - Sample Management: Dynamic sample selection from input file - Error Handling: Comprehensive logging and email notifications  </p>"},{"location":"Bam/sort-bam/#best-practices","title":"Best Practices","text":""},{"location":"Bam/sort-bam/#pre-sorting-considerations","title":"Pre-Sorting Considerations","text":"<ol> <li>Input Validation: Verify SAM/BAM file integrity (with samtools quickcheck) before sorting</li> <li>Disk Space: Ensure adequate temporary storage (3x input file size)</li> <li>Memory Planning: Allocate sufficient RAM to avoid external sorting</li> <li>Backup Strategy: Preserve original files until validation completion</li> </ol>"},{"location":"Bam/sort-bam/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Memory Tuning: Adjust heap size based on available system memory</li> <li>Parallel Processing: Utilize multiple CPU cores for enhanced performance</li> <li>I/O Optimization: Use high-speed storage for temporary files</li> <li>Compression: Consider compression level trade-offs between size and speed</li> </ol>"},{"location":"Bam/sort-bam/#post-sorting-validation","title":"Post-Sorting Validation","text":"<ol> <li>Sort Verification: Validate proper coordinate ordering</li> <li>Index Generation: Create BAI/CSI indices for random access</li> <li>Statistics Collection: Generate alignment statistics for quality assessment</li> <li>File Integrity: Verify BGZF compression and checksums</li> </ol>"},{"location":"Bam/sort-bam/#integration-with-analysis-pipelines","title":"Integration with Analysis Pipelines","text":""},{"location":"Bam/sort-bam/#workflow-integration","title":"Workflow Integration","text":"<p>Sorting typically occurs between alignment and downstream analysis: 1. Read Alignment: Generate initial SAM/BAM files 2. Quality Filtering: Remove low-quality alignments 3. Sorting: Coordinate-based organization 4. Indexing: Generate access indices 5. Analysis: Variant calling, coverage analysis, etc.  </p>"},{"location":"Bam/sort-bam/#automation-considerations","title":"Automation Considerations","text":"<ul> <li>Dependency Management: Ensure proper software module loading</li> <li>Error Handling: Implement robust error detection and recovery</li> <li>Resource Monitoring: Track computational resource utilization</li> <li>Scalability: Design for varying dataset sizes and computational environments</li> </ul>"},{"location":"Bam/sort-bam/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"Bam/sort-bam/#memory-related-problems","title":"Memory-Related Problems","text":"<ul> <li>OutOfMemoryError: Increase JVM heap size or reduce records in RAM</li> <li>System Memory: Monitor system memory usage during sorting</li> <li>Swap Usage: Avoid excessive swap file utilization</li> </ul>"},{"location":"Bam/sort-bam/#performance-bottlenecks","title":"Performance Bottlenecks","text":"<ul> <li>I/O Limitations: Optimize temporary file location and storage speed</li> <li>CPU Utilization: Balance thread count with available cores</li> <li>Network Latency: Minimize network-based file operations</li> </ul>"},{"location":"Bam/sort-bam/#data-integrity-issues","title":"Data Integrity Issues","text":"<ul> <li>Truncated Files: Verify complete file transfer before sorting</li> <li>Corruption Detection: Implement checksum validation</li> <li>Format Compliance: Ensure proper SAM/BAM format adherence</li> </ul>"},{"location":"Bam/sort-bam/#conclusion","title":"Conclusion","text":"<p>BAM file sorting represents a critical preprocessing step that significantly impacts the efficiency and accuracy of downstream genomic analyses. Proper implementation requires careful consideration of computational resources, algorithm selection, and quality control measures. The integration of sorting operations into automated pipelines enables scalable processing of large-scale genomic datasets while maintaining data integrity and analytical reproducibility.</p>"},{"location":"Bam/variant-calling/","title":"Variant Calling in Genomics","text":"Table of Content <ul> <li>Variant Calling in Genomics<ul> <li>Introduction</li> <li>GATK HaplotypeCaller</li> <li>GATK UnifiedGenotyper</li> <li>BCFtools for Variant Calling</li> <li>Comparative Considerations</li> <li>Best Practices</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Bam/variant-calling/#introduction","title":"Introduction","text":"<p>Variant calling is a fundamental step in genomics that identifies differences between sequenced genomes and a reference genome. These differences, or variants, include single nucleotide polymorphisms (SNPs), insertions, deletions (indels), and structural variants. Accurate variant calling is crucial for understanding genetic diversity, disease susceptibility, and evolutionary relationships.</p> <p>The process involves aligning sequencing reads to a reference genome and then identifying positions where the aligned reads differ from the reference. Various computational tools have been developed to perform this task, each with specific strengths and applications.</p>"},{"location":"Bam/variant-calling/#gatk-haplotypecaller","title":"GATK HaplotypeCaller","text":"<p>The Genome Analysis Toolkit (GATK) HaplotypeCaller is currently the gold standard for variant calling in human genomics and is widely adopted across various species. This tool uses a sophisticated approach that assembles haplotypes in regions with variation, making it particularly effective at calling variants in complex genomic regions.</p>"},{"location":"Bam/variant-calling/#key-features","title":"Key Features","text":"<p>HaplotypeCaller employs a local de novo assembly approach that: - Identifies active regions where variation is likely to occur - Assembles possible haplotypes in these regions - Realigns reads to the most likely haplotypes - Calls variants based on the assembled haplotypes  </p> <p>This methodology provides superior accuracy compared to position-based callers, especially for indels and complex variants.</p>"},{"location":"Bam/variant-calling/#implementation-example","title":"Implementation Example","text":"<p>The following script demonstrates a typical HaplotypeCaller workflow for generating genomic variant call format (GVCF) files:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --job-name=haplotype-gvcf\n#SBATCH --time=15-23:00:00\n#SBATCH -p long\n#SBATCH -N 1 # Nodes\n#SBATCH -n 1 # ntasks\n#SBATCH --cpus-per-task 4\n#SBATCH --mem=48G\n#SBATCH -o Cluster_logs/%x-%A-%a.out\n#SBATCH -e Cluster_logs/%x-%A-%a.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n#SBATCH --array=1-284 # 71 samples x 4 chromosomes\n###################################################################\nmodule load gatk4/4.2.6.1\nFILE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/ae_sample_scaffolds)\nSAMPLE=$(echo $FILE | awk '{print $1}')\nSCAFFOLD=$(echo $FILE | awk '{print $2}')\n# Define path to .bam files\nBAM_DIR=\"results/04_Polishing\"\n# Check if directory exists\nif [ ! -d \"$BAM_DIR\" ]; then\n    echo \"Le dossier $BAM_DIR n'existe pas.\"\n    exit 1\nfi\nOUTPUT_DIR=\"results/05_Variants\"/${SCAFFOLD}\nmkdir -p \"${OUTPUT_DIR}\"\n# Create directory for log files\nLOG_FOLDER=\"results/11_Reports/haplotypecaller\"\nmkdir -p \"${LOG_FOLDER}\"\n# Run HaplotyCaller: \n# -Xmx64g: defines the maximum memory size that the JVM can allocate to the application during execution\ngatk --java-options \"-Xmx48g\" HaplotypeCaller \\\n  -R resources/genomes/AalbF5_filtered.fasta \\\n  -I ${BAM_DIR}/${SAMPLE}_marked.bam \\\n  -O ${OUTPUT_DIR}/${SAMPLE}-${SCAFFOLD}.g.vcf.gz \\\n  -ERC GVCF \\\n  -L ${SCAFFOLD} \\\n  2&gt; ${LOG_FOLDER}/${SAMPLE}-${SCAFFOLD}-hapcall-gvcf.out\n</code></pre> <p>This script utilizes SLURM job arrays to process multiple samples and chromosomes in parallel, demonstrating the scalability required for large-scale genomic projects.</p> <p>The output of HaplotypeCaller in <code>-ERC GVCF</code> mode is a <code>genomic VCF</code> (gVCF) file that records genotype likelihoods and variant calls for every genomic position, including non-variant sites. It enables joint genotyping by capturing reference confidence information. </p> <p>Unfortunately, the GVCF format is not always supported by downstream analysis tools, which often require a <code>standard VCF</code> file. To convert GVCF files into a conventional VCF format, the GATK pipeline provides the <code>GenomicsDBImport</code> tool to aggregate multiple GVCFs into a GenomicsDB datastore, followed by <code>GenotypeGVCFs</code>, which performs joint genotyping across samples and produces a final multisample VCF suitable for most variant analysis workflows. </p> <p>See GenomicsDBImport and GenotypeGVCFs in GATK4 Workflow</p>"},{"location":"Bam/variant-calling/#gatk-unifiedgenotyper","title":"GATK UnifiedGenotyper","text":"<p>The UnifiedGenotyper was one of the earlier variant callers in the GATK suite and served as a workhorse for many genomic studies. However, it has been largely superseded by HaplotypeCaller due to several limitations in its algorithm.</p>"},{"location":"Bam/variant-calling/#current-status-and-applications","title":"Current Status and Applications","text":"<p>UnifiedGenotyper is increasingly deprecated and is now rarely used in modern genomics workflows. Its usage has been largely discontinued for most applications, with one notable exception: projects involving the 1000 Genomes Anopheles gambiae dataset. This specific use case maintains UnifiedGenotyper compatibility due to:</p> <ul> <li>Historical continuity with existing analyses</li> <li>Standardized protocols established for malaria vector genomics</li> <li>Specific parameter optimizations for Anopheles gambiae genetic characteristics</li> <li>Integration with the Ag1000G (Anopheles gambiae 1000 Genomes) project pipeline</li> </ul>"},{"location":"Bam/variant-calling/#implementation-for-anopheles-gambiae-projects","title":"Implementation for Anopheles gambiae Projects","text":"<p>The following script shows a typical UnifiedGenotyper implementation for Anopheles gambiae genomic data:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --job-name=UnifiedGenotyper\n#SBATCH --time=2-23:00:00\n#SBATCH -p long\n#SBATCH -N 1 # Nodes\n#SBATCH -n 1 # ntasks\n#SBATCH --cpus-per-task 4\n#SBATCH --mem=48G\n#SBATCH --array=1-4\n#SBATCH -o Cluster_logs/%x-%A-%a.out\n#SBATCH -e Cluster_logs/%x-%A-%a.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n###################################################################\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chrom.list)\n# Load module\nmodule load gatk/3.8\nBAM_LIST=\"/shared/projects/invalbo/AG3/SN/info_files/bam.list\"\nREF=\"/shared/projects/invalbo/AG3/SN/resources/genome/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa\"\nOUTPUT_DIR=\"results/05_Variants\"\nmkdir -p \"${OUTPUT_DIR}\"\n# Create directory for log files\nLOG=\"results/11_Reports/unifiedgenotyper\"\nmkdir -p \"${LOG}\"\ngatk3 -Xmx48G -T UnifiedGenotyper --num_threads 4 --num_cpu_threads_per_data_thread 4 -I ${BAM_LIST} -R ${REF} -L ${CHROM} \\\n      --out ${OUTPUT_DIR}/SN.ag3.${CHROM}.vcf --genotype_likelihoods_model BOTH --genotyping_mode DISCOVERY --heterozygosity 0.015 \\\n      --heterozygosity_stdev 0.05 --indel_heterozygosity 0.001 --downsampling_type BY_SAMPLE -dcov 250 --output_mode EMIT_ALL_SITES \\\n      --min_base_quality_score 17 -stand_call_conf 0.0 -contamination 0.0 -A DepthPerAlleleBySample \\\n      -A RMSMappingQuality -A Coverage -A FisherStrand -A StrandOddsRatio -A BaseQualityRankSumTest -A MappingQualityRankSumTest -A QualByDepth \\\n      -A ReadPosRankSumTest -XA ExcessHet -XA InbreedingCoeff -XA MappingQualityZero -XA HaplotypeScore -XA SpanningDeletions -XA ChromosomeCounts \\\n      &amp;&gt; ${LOG}/SN.ag3.${CHROM}.log\n</code></pre> <p>Note the specialized parameters optimized for Anopheles gambiae genetics, including heterozygosity rates and quality thresholds tailored to this species.</p>"},{"location":"Bam/variant-calling/#bcftools-for-variant-calling","title":"BCFtools for Variant Calling","text":"<p>BCFtools, part of the SAMtools suite, provides an alternative approach to variant calling that is particularly valued for its computational efficiency and integration with other bioinformatics tools.</p>"},{"location":"Bam/variant-calling/#algorithm-and-approach","title":"Algorithm and Approach","text":"<p>BCFtools uses a Bayesian approach to variant calling through its <code>mpileup</code> and <code>call</code> commands. The process involves:</p> <ol> <li>Pileup generation: Creating a summary of read alignments at each position</li> <li>Likelihood calculation: Computing genotype likelihoods based on base qualities and alignment characteristics</li> <li>Variant calling: Applying statistical models to identify the most likely genotypes</li> </ol>"},{"location":"Bam/variant-calling/#advantages-of-bcftools","title":"Advantages of BCFtools","text":"<ul> <li>Computational efficiency: Generally faster than GATK tools, especially for large datasets</li> <li>Memory efficiency: Lower memory requirements make it suitable for resource-constrained environments</li> <li>Flexibility: Extensive command-line options for fine-tuning calling parameters</li> <li>Integration: Seamless integration with SAMtools and other tools in the suite</li> </ul>"},{"location":"Bam/variant-calling/#typical-bcftools-workflow","title":"Typical BCFtools Workflow","text":"<pre><code># Generate pileup and call variants\nbcftools mpileup -f reference.fasta input.bam | bcftools call -mv -O z -o output.vcf.gz\n\n# Alternative approach with separate steps\nbcftools mpileup -f reference.fasta input.bam &gt; pileup.vcf\nbcftools call -mv pileup.vcf -O z -o variants.vcf.gz\n</code></pre>"},{"location":"Bam/variant-calling/#when-to-choose-bcftools","title":"When to Choose BCFtools","text":"<p>BCFtools is particularly suitable for: - High-throughput projects requiring computational efficiency - Population genomics studies with large sample sizes - Projects with limited computational resources - Analyses requiring tight integration with SAMtools workflows</p>"},{"location":"Bam/variant-calling/#comparative-considerations","title":"Comparative Considerations","text":""},{"location":"Bam/variant-calling/#accuracy-vs-speed-trade-offs","title":"Accuracy vs. Speed Trade-offs","text":"<ul> <li>GATK HaplotypeCaller: Highest accuracy, especially for complex variants, but computationally intensive</li> <li>BCFtools: Good balance of accuracy and speed, particularly efficient for SNP calling</li> <li>UnifiedGenotyper: Historical tool with limited current applications</li> </ul>"},{"location":"Bam/variant-calling/#project-specific-recommendations","title":"Project-Specific Recommendations","text":"<ul> <li>Human clinical genomics: GATK HaplotypeCaller is the standard</li> <li>Population genomics: BCFtools or HaplotypeCaller depending on accuracy requirements</li> <li>Anopheles gambiae 1000 Genomes projects: UnifiedGenotyper for continuity with established protocols</li> <li>Resource-limited environments: BCFtools for efficiency</li> </ul>"},{"location":"Bam/variant-calling/#best-practices","title":"Best Practices","text":""},{"location":"Bam/variant-calling/#quality-control","title":"Quality Control","text":"<p>Regardless of the chosen variant caller, implementing robust quality control measures is essential:</p> <ul> <li>Base quality filtering: Remove low-quality bases from consideration</li> <li>Mapping quality assessment: Filter poorly mapped reads</li> <li>Depth filtering: Apply appropriate coverage thresholds</li> <li>Allele frequency filtering: Consider population-specific allele frequencies</li> </ul>"},{"location":"Bam/variant-calling/#post-calling-processing","title":"Post-Calling Processing","text":"<p>All variant calling workflows should include:</p> <ul> <li>Variant quality score recalibration (VQSR): Particularly important for GATK workflows</li> <li>Hard filtering: Apply quality thresholds when VQSR is not feasible</li> <li>Annotation: Add functional and population annotations</li> <li>Validation: Confirm critical variants through independent methods</li> </ul>"},{"location":"Bam/variant-calling/#conclusion","title":"Conclusion","text":"<p>The choice of variant calling tool depends on specific project requirements, computational resources, and accuracy needs. While GATK HaplotypeCaller has become the gold standard for most applications, BCFtools remains valuable for efficient processing of large datasets. UnifiedGenotyper, though largely deprecated, continues to serve specific niche applications, particularly in Anopheles gambiae genomics where protocol continuity is essential.</p> <p>As sequencing technologies and analytical methods continue to evolve, the field of variant calling will likely see further innovations in accuracy, efficiency, and specialized applications for different genomic contexts.</p>"},{"location":"Basic/basic-bash-commands/","title":"Introduction to Basic Bash Commands","text":"<p>Welcome to the world of UNIX command-line interfaces! This guide will introduce you to essential bash commands that form the foundation of working in a UNIX environment. As a Master's student, mastering these commands will significantly enhance your productivity and open up powerful possibilities for file management, data processing, and system administration.</p> Table of Content <ul> <li>Introduction to Basic Bash Commands<ul> <li>Getting Help: Your First Tools (--help &amp; man)</li> <li>File and Directory Operations (pwd, ls, cd, mkdir, rmdir)</li> <li>File Operations (touch, cp, mv, rm)</li> <li>Viewing File Contents (cat, less, head, tail)</li> <li>File Permissions and Information (chmod, chown)</li> <li>Process Management (ps, kill)</li> <li>System Information (whoami, date, df, du)</li> <li>Best Practices and Tips</li> <li>Next Steps</li> </ul> </li> </ul>"},{"location":"Basic/basic-bash-commands/#getting-help-your-first-tools-help-man","title":"Getting Help: Your First Tools (--help &amp; man)","text":"<p>Before diving into specific commands, let's learn how to get help when you're stuck or need more information about a command.</p>"},{"location":"Basic/basic-bash-commands/#the-help-option","title":"The <code>--help</code> Option","text":"<p>Most commands in bash support the <code>--help</code> option, which provides a quick overview of the command's usage and available options.</p> <p>Syntax: <pre><code>command --help\n</code></pre></p> <p>Example: <pre><code>ls --help\n</code></pre></p> <p>This will display a concise summary of how to use the <code>ls</code> command, including all its options and their meanings. The <code>--help</code> option is perfect for quick reference when you remember the command but need to check its options.</p>"},{"location":"Basic/basic-bash-commands/#the-man-command-manual-pages","title":"The <code>man</code> Command (Manual Pages)","text":"<p>The <code>man</code> command provides access to the system's manual pages, which contain comprehensive documentation for commands, system calls, and configuration files.</p> <p>Syntax: <pre><code>man command_name\n</code></pre></p> <p>Example: <pre><code>man ls\n</code></pre></p> <p>This opens the complete manual page for the <code>ls</code> command. Manual pages are more detailed than <code>--help</code> and include:</p> <ul> <li>NAME: Brief description of the command  </li> <li>SYNOPSIS: Usage syntax  </li> <li>DESCRIPTION: Detailed explanation of what the command does  </li> <li>OPTIONS: Complete list of all available options  </li> <li>EXAMPLES: Usage examples  </li> <li>SEE ALSO: Related commands  </li> </ul> <p>Navigation in man pages: - Use arrow keys or <code>j</code>/<code>k</code> to scroll up and down - Press <code>Space</code> to scroll down one page - Press <code>q</code> to quit the manual page - Press <code>/</code> followed by a search term to search within the page  </p> <p>What does <code>tr</code> command?</p> <p>This exercise is your first introduction to the man command. You will discover   a command you are not familiar with. Let's take the example of the <code>tr</code> command.   1. Using the man command, explain what the <code>tr</code> command does.    2. Must at least one option (short or long) be passed as an argument to the <code>tr</code> command?   3. Can more than one option (short or long) be passed as an argument to the <code>tr</code> command?   4. Must at least one argument be passed to the <code>tr</code> command?   5. How many arguments (other than options) can be passed to the <code>tr</code> command?   6. How are arguments (other than options) passed to the tr command?  </p> Answers <ol> <li>From the NAME part of the command <code>tr</code>, we learn that it is used to \u2018Convert or delete characters\u2019.  </li> <li>The SYNOPSIS part of the man of the tr command contains the text below: <code>SYNOPSIS</code> <code>tr</code> <code>[OPTION]</code> <code>...</code> <code>ENSEMBLE1</code> <code>[ENSEMBLE2]</code>      The notation with square brackets [ ] around OPTION indicates that the argument marked OPTION is optional. It is therefore not necessary to pass an option as an argument to the <code>tr</code> command.</li> <li>The three dots after <code>[OPTION]</code> indicate that you can specify more than one.</li> <li>The absence of a bracket around ENSEMBLE1 indicates that this argument is necessary, so must pass at least one argument to the tr command.</li> <li><code>[ENSEMBLE2]</code> indicates that a second set can be specified, and as it is not followed by three dots, this means that a maximum of two sets can be passed as arguments to the tr command.</li> <li>The <code>DESCRIPTION</code> part of the <code>tr</code> command man contains the text below: <pre><code>DESCRIPTION  \n      Convert, compress and/or eliminate characters read from the standard input and write them to the standard output.\n\n      -c, -C, --complement\n                     use the complement of the ENSEMBLE1\n(...)\n      --version\n                     Display software name and version and exit\n</code></pre></li> </ol>"},{"location":"Basic/basic-bash-commands/#file-and-directory-operations-pwd-ls-cd-mkdir-rmdir","title":"File and Directory Operations (pwd, ls, cd, mkdir, rmdir)","text":""},{"location":"Basic/basic-bash-commands/#pwd-print-working-directory","title":"<code>pwd</code> - Print Working Directory","text":"<p>The <code>pwd</code> command shows you exactly where you are in the filesystem hierarchy.</p> <p>Syntax: <pre><code>pwd\n</code></pre></p> <p>Example: <pre><code>pwd\n# Output: /home/username/documents\n</code></pre></p> <p>This command is essential for understanding your current location before performing other operations.</p>"},{"location":"Basic/basic-bash-commands/#ls-list-directory-contents","title":"<code>ls</code> - List Directory Contents","text":"<p>The <code>ls</code> command displays the contents of directories. It's one of the most frequently used commands.</p> <p>Basic syntax: <pre><code>ls [options] [directory]  \n</code></pre></p> <p>Common options: - <code>-l</code>: Long format (detailed information including permissions, owner, size, date) - <code>-a</code>: Show all files, including hidden files (those starting with <code>.</code>) - <code>-h</code>: Human-readable file sizes (when used with <code>-l</code>) - <code>-t</code>: Sort by modification time - <code>-r</code>: Reverse the order of sorting  </p> <p>Examples: <pre><code>ls                    # List current directory\nls -l                 # Detailed listing\nls -la                # Detailed listing including hidden files\nls -lh                # Detailed listing with human-readable sizes\nls /home/username     # List contents of specific directory\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#cd-change-directory","title":"<code>cd</code> - Change Directory","text":"<p>The <code>cd</code> command allows you to navigate between directories.</p> <p>Syntax: <pre><code>cd [directory]\n</code></pre></p> <p>Special directory references: - <code>~</code>: Home directory - <code>.</code>: Current directory - <code>..</code>: Parent directory - <code>-</code>: Previous directory  </p> <p>Examples: <pre><code>cd /home/username     # Navigate to specific directory\ncd ~                  # Go to home directory\ncd ..                 # Go up one level\ncd -                  # Go back to previous directory\ncd                    # Go to home directory (same as cd ~)\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#mkdir-make-directory","title":"<code>mkdir</code> - Make Directory","text":"<p>The <code>mkdir</code> command creates new directories.</p> <p>Syntax: <pre><code>mkdir [options] directory_name\n</code></pre></p> <p>Common options: - <code>-p</code>: Create parent directories as needed - <code>-m</code>: Set permissions for the new directory  </p> <p>Examples: <pre><code>mkdir new_folder                    # Create a single directory\nmkdir -p documents/projects/2024    # Create nested directories\nmkdir folder1 folder2 folder3       # Create multiple directories\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#rmdir-remove-directory","title":"<code>rmdir</code> - Remove Directory","text":"<p>The <code>rmdir</code> command removes empty directories.</p> <p>Syntax: <pre><code>rmdir directory_name\n</code></pre></p> <p>Example: <pre><code>rmdir empty_folder\n</code></pre></p> <p>Note: This command only works on empty directories. For directories with content, use <code>rm -r</code> (covered later).  </p>"},{"location":"Basic/basic-bash-commands/#file-operations-touch-cp-mv-rm","title":"File Operations (touch, cp, mv, rm)","text":""},{"location":"Basic/basic-bash-commands/#touch-create-empty-files-or-update-timestamps","title":"<code>touch</code> - Create Empty Files or Update Timestamps","text":"<p>The <code>touch</code> command creates new empty files or updates the timestamp of existing files.</p> <p>Syntax: <pre><code>touch filename\n</code></pre></p> <p>Examples: <pre><code>touch new_file.txt                    # Create a new empty file\ntouch file1.txt file2.txt file3.txt   # Create multiple files\ntouch existing_file.txt               # Update timestamp of existing file\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#cp-copy-files-and-directories","title":"<code>cp</code> - Copy Files and Directories","text":"<p>The <code>cp</code> command copies files and directories from one location to another.</p> <p>Syntax: <pre><code>cp [options] source destination\n</code></pre></p> <p>Common options: - <code>-r</code> or <code>-R</code>: Copy directories recursively - <code>-i</code>: Interactive mode (ask before overwriting) - <code>-v</code>: Verbose mode (show what's being copied)  </p> <p>Examples: <pre><code>cp file.txt backup.txt              # Copy file to new name\ncp file.txt /home/username/         # Copy file to different directory\ncp -r documents/ backup_documents/  # Copy directory and all contents\ncp -i file.txt existing_file.txt    # Copy with confirmation prompt\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#mv-moverename-files-and-directories","title":"<code>mv</code> - Move/Rename Files and Directories","text":"<p>The <code>mv</code> command moves files and directories, and can also rename them.</p> <p>Syntax: <pre><code>mv [options] source destination\n</code></pre></p> <p>Common options: - <code>-i</code>: Interactive mode (ask before overwriting) - <code>-v</code>: Verbose mode (show what's being moved)  </p> <p>Examples: <pre><code>mv old_name.txt new_name.txt        # Rename a file\nmv file.txt /home/username/         # Move file to different directory\nmv documents/ /backup/              # Move directory\nmv -i file.txt existing_file.txt    # Move with confirmation prompt\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#rm-remove-files-and-directories","title":"<code>rm</code> - Remove Files and Directories","text":"<p>The <code>rm</code> command deletes files and directories. Use with caution!</p> <p>Syntax: <pre><code>rm [options] file_or_directory\n</code></pre></p> <p>Common options: - <code>-r</code> or <code>-R</code>: Remove directories and their contents recursively - <code>-i</code>: Interactive mode (ask for confirmation) - <code>-f</code>: Force deletion without prompting - <code>-v</code>: Verbose mode (show what's being deleted)  </p> <p>Examples: <pre><code>rm file.txt                   # Delete a file\nrm -i important_file.txt      # Delete with confirmation\nrm -r old_directory/          # Delete directory and all contents\nrm -rf temp_folder/           # Force delete directory (be very careful!)\n</code></pre></p> <p>Warning: The <code>rm</code> command permanently deletes files. There's no \"trash\" or \"recycle bin\" in the command line. Always double-check before using <code>rm</code>, especially with the <code>-r</code> and <code>-f</code> options.</p>"},{"location":"Basic/basic-bash-commands/#viewing-file-contents-cat-less-head-tail","title":"Viewing File Contents (cat, less, head, tail)","text":""},{"location":"Basic/basic-bash-commands/#cat-display-file-contents","title":"<code>cat</code> - Display File Contents","text":"<p>The <code>cat</code> command displays the entire contents of a file.</p> <p>Syntax: <pre><code>cat filename\n</code></pre></p> <p>Examples: <pre><code>cat document.txt              # Display file contents\ncat file1.txt file2.txt       # Display multiple files\ncat &gt; new_file.txt            # Create file and enter content (Ctrl+D to finish)\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#less-zless-view-file-contents-page-by-page","title":"<code>less / zless</code> - View File Contents Page by Page","text":"<p>The <code>less</code> command allows you to view file contents one page at a time, which is useful for large files. <code>zless</code> is the same command but for compressed files, finishing by <code>.gz</code></p> <p>Syntax: <pre><code>less filename\nzless -S filename.gz\n</code></pre></p> <p>Navigation in less: - Arrow keys or <code>j</code>/<code>k</code>: Move up and down - <code>Space</code>: Move forward one page - <code>b</code>: Move backward one page - <code>g</code>: Go to beginning of file - <code>G</code>: Go to end of file - <code>S</code>: Chop long lines rather than folding them - <code>/pattern</code>: Search for pattern - <code>q</code>: Quit  </p> <p>Example: <pre><code>zless -S sample1_R1.fastq.gz\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#head-display-first-lines-of-a-file","title":"<code>head</code> - Display First Lines of a File","text":"<p>The <code>head</code> command shows the first few lines of a file (default is 10 lines).</p> <p>Syntax: <pre><code>head [options] filename\n</code></pre></p> <p>Common options: - <code>-n number</code>: Show specific number of lines</p> <p>Examples: <pre><code>head document.txt        # Show first 10 lines\nhead -n 5 document.txt   # Show first 5 lines\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#tail-display-last-lines-of-a-file","title":"<code>tail</code> - Display Last Lines of a File","text":"<p>The <code>tail</code> command shows the last few lines of a file (default is 10 lines).</p> <p>Syntax: <pre><code>tail [options] filename\n</code></pre></p> <p>Common options: - <code>-n number</code>: Show specific number of lines - <code>-f</code>: Follow the file (useful for log files)  </p> <p>Examples: <pre><code>tail document.txt        # Show last 10 lines\ntail -n 20 document.txt  # Show last 20 lines\ntail -f logfile.txt      # Follow log file updates (Ctrl+C to stop)\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#file-permissions-and-information-chmod-chown","title":"File Permissions and Information (chmod, chown)","text":""},{"location":"Basic/basic-bash-commands/#chmod-change-file-permissions","title":"<code>chmod</code> - Change File Permissions","text":"<p>The <code>chmod</code> command modifies file and directory permissions.</p> <p>Syntax: <pre><code>chmod [options] permissions filename\n</code></pre></p> <p>Permission notation: In Unix, file permissions are represented by a 10-character string like <code>-rwxr-xr--</code>, where the first character indicates the file type (-: file; d: directory; l: symbolic link...), and the next nine specify read (r), write (w), and execute (x) permissions for the <code>owner</code>, <code>group</code>, and <code>others</code> (thus, each coded on three characters). </p> <pre><code>ls -l document.txt\n# -rwxr-xr-- 1 username group 1024 Jan 15 10:30 document.txt\n# \u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\n# \u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2514\u2514\u2514\u2500 others: read\n# \u2502\u2502\u2502\u2502\u2502\u2502\u2502\n# \u2502\u2502\u2502\u2502\u2514\u2514\u2514\u2500\u2500\u2500\u2500 group: read, execute\n# \u2502\u2502\u2502\u2502\n# \u2502\u2514\u2514\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500 owner: read, write\n# \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 regular file\n</code></pre> <p>These permissions can also be represented numerically (e.g., 751) using octal notation: </p> <ul> <li><code>r</code> (read) = 4  </li> <li><code>w</code> (write) = 2  </li> <li><code>x</code> (execute) = 1  </li> </ul> <p>This system controls who can access or modify a file. In the example above, the type is a file the owner can read, write and execute (4+2+1=7). The group can read and execute (4+1=5) and others can only read (1). The octal notation in this case is 751.</p> <p>How to change permissions on a file: <pre><code>chmod 755 script.sh      # rwxr-xr-x (owner: all, group/others: read+execute)\nchmod 644 document.txt   # rw-r--r-- (owner: read+write, group/others: read only)\nchmod +x script.sh       # Add execute permission\nchmod -w document.txt    # Remove write permission\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#chown-change-file-ownership","title":"<code>chown</code> - Change File Ownership","text":"<p>The <code>chown</code> command changes the owner and group of files and directories.</p> <p>Syntax: <pre><code>chown [options] owner:group filename\n</code></pre></p> <p>Example: <pre><code>chown username:usergroup file.txt\n\n# Change owner to 'alice' and group to 'researchers' for a single file\nsudo chown alice:researchers my_project/file1.txt\n\n# Change owner only (group remains unchanged)\nsudo chown bob my_project/file2.txt\n\n# Recursively change owner and group for the entire directory\nsudo chown -R alice:researchers my_project/\n</code></pre></p> <p>Note: You typically need administrator privileges to change ownership.</p>"},{"location":"Basic/basic-bash-commands/#process-management-ps-kill","title":"Process Management (ps, kill)","text":""},{"location":"Basic/basic-bash-commands/#ps-display-running-processes","title":"<code>ps</code> - Display Running Processes","text":"<p>The <code>ps</code> command shows information about running processes.</p> <p>Syntax: <pre><code>ps [options]\n</code></pre></p> <p>Common options: - <code>aux</code>: Show all processes with detailed information - <code>ef</code>: Show all processes in full format  </p> <p>Examples: <pre><code>ps                # Show processes in current session\nps aux            # Show all processes with details\nps aux | grep python  # Show only Python processes\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#kill-terminate-processes","title":"<code>kill</code> - Terminate Processes","text":"<p>The <code>kill</code> command sends signals to processes, typically to terminate them.</p> <p>Syntax: <pre><code>kill [options] process_id\n</code></pre></p> <p>Common options: - <code>-9</code>: Force kill (SIGKILL) - <code>-15</code>: Graceful termination (SIGTERM, default)  </p> <p>Examples: <pre><code>kill 1234         # Terminate process with ID 1234\nkill -9 1234      # Force terminate process 1234\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#system-information-whoami-date-df-du","title":"System Information (whoami, date, df, du)","text":""},{"location":"Basic/basic-bash-commands/#whoami-display-current-username","title":"<code>whoami</code> - Display Current Username","text":"<p>The <code>whoami</code> command shows your current username.</p> <p>Syntax: <pre><code>whoami\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#date-display-or-set-date","title":"<code>date</code> - Display or Set Date","text":"<p>The <code>date</code> command shows the current date and time.</p> <p>Syntax: <pre><code>date [options]\n</code></pre></p> <p>Example: <pre><code>date                    # Show current date and time\ndate +\"%Y-%m-%d %H:%M\"  # Show formatted date\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#df-display-filesystem-disk-space","title":"<code>df</code> - Display Filesystem Disk Space","text":"<p>The <code>df</code> command shows disk space usage for filesystems.</p> <p>Syntax: <pre><code>df [options]\n</code></pre></p> <p>Common options: - <code>-h</code>: Human-readable format  </p> <p>Example: <pre><code>df -h              # Show disk usage in human-readable format\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#du-display-directory-space-usage","title":"<code>du</code> - Display Directory Space Usage","text":"<p>The <code>du</code> command shows disk space usage for directories.</p> <p>Syntax: <pre><code>du [options] directory\n</code></pre></p> <p>Common options: - <code>-h</code>: Human-readable format - <code>-s</code>: Summary (total size only)  </p> <p>Examples: <pre><code>du -h documents/        # Show space usage of documents directory\ndu -sh documents/       # Show total size of documents directory\n</code></pre></p>"},{"location":"Basic/basic-bash-commands/#best-practices-and-tips","title":"Best Practices and Tips","text":"<ol> <li> <p>Always use <code>--help</code> or <code>man</code> when uncertain about a command's options or usage.  </p> </li> <li> <p>Be careful with destructive commands like <code>rm</code>, especially with options like <code>-r</code> and <code>-f</code>.  </p> </li> <li> <p>Use tab completion to avoid typing errors and speed up your work. Press Tab while typing to auto-complete filenames and commands.</p> </li> <li> <p>Use relative and absolute paths appropriately: </p> </li> <li>Relative paths: <code>documents/file.txt</code> (relative to current directory)</li> <li> <p>Absolute paths: <code>/home/username/documents/file.txt</code> (full path from root)</p> </li> <li> <p>Practice in a safe environment before using commands on important files.  </p> </li> <li> <p>Use descriptive names for files and directories to make navigation easier.  </p> </li> <li> <p>Combine commands with pipes (<code>|</code>) to create powerful workflows: <pre><code>ls -la | grep \"document\"    # List files and filter for those containing \"document\"\n</code></pre></p> </li> </ol>"},{"location":"Basic/basic-bash-commands/#next-steps","title":"Next Steps","text":"<p>Now that you've learned these basic commands, practice using them regularly. Try creating directories, copying files, and exploring your filesystem. As you become more comfortable, you can learn about:</p> <ul> <li>Text processing commands (<code>grep</code>, <code>sed</code>, <code>awk</code>)</li> <li>File compression (<code>tar</code>, <code>gzip</code>)</li> <li>Network commands (<code>curl</code>, <code>wget</code>)</li> <li>Environment variables and shell scripting</li> <li>Advanced file operations and regular expressions</li> </ul> <p>Remember, the command line is a powerful tool that becomes more intuitive with practice. Don't hesitate to use <code>--help</code> and <code>man</code> pages whenever you need clarification!</p>"},{"location":"Basic/file-management/","title":"File Management in Bash","text":"<p>Effective file management is crucial for productivity in UNIX environments. This guide will teach you advanced techniques for organizing, searching, and manipulating files using bash commands. These skills will help you handle large datasets, maintain organized project structures, and automate repetitive tasks.</p> Table of Content <ul> <li>File Management in Bash<ul> <li>Understanding File Systems and Paths</li> <li>Advanced Directory Operations</li> <li>Advanced File Operations</li> <li>File Search and Discovery</li> <li>File Content Operations</li> <li>File Permissions and Ownership</li> <li>File Compression and Archives</li> <li>Symbolic Links and Hard Links</li> <li>File Monitoring and Watching</li> <li>Batch File Operations</li> <li>File System Navigation and Organization</li> <li>Practical Examples and Use Cases</li> <li>Troubleshooting Common Issues</li> <li>Best Practices for File Management</li> <li>Summary</li> </ul> </li> </ul>"},{"location":"Basic/file-management/#understanding-file-systems-and-paths","title":"Understanding File Systems and Paths","text":""},{"location":"Basic/file-management/#absolute-vs-relative-paths","title":"Absolute vs Relative Paths","text":"<p>Understanding paths is fundamental to file management:</p> <p>Absolute paths start from the root directory (<code>/</code>) and specify the complete location: <pre><code>/home/username/documents/project/data.txt\n</code></pre></p> <p>Relative paths are relative to your current working directory: <pre><code>documents/project/data.txt    # From home directory\n../project/data.txt          # From documents directory\n./data.txt                   # From project directory\n</code></pre></p> <p>Special path symbols: - <code>~</code>: Home directory (<code>/home/username</code>) - <code>.</code>: Current directory - <code>..</code>: Parent directory - <code>-</code>: Previous directory (used with <code>cd</code>)  </p>"},{"location":"Basic/file-management/#file-extensions-and-types","title":"File Extensions and Types","text":"<p>Unlike Windows, UNIX systems don't rely on file extensions to determine file types. However, extensions are still useful for organization:</p> <pre><code>file document.txt        # Determine actual file type\nfile image.jpg          # Check if it's really an image\nfile script.py          # Verify file content type\n</code></pre>"},{"location":"Basic/file-management/#advanced-directory-operations","title":"Advanced Directory Operations","text":""},{"location":"Basic/file-management/#creating-complex-directory-structures","title":"Creating Complex Directory Structures","text":"<p>The <code>mkdir</code> command with the <code>-p</code> option can create entire directory trees:</p> <pre><code># Create a project structure\nmkdir -p project/{src,docs,tests,data/{raw,processed}}\n\n# This creates:\n# project/\n# \u251c\u2500\u2500 src/\n# \u251c\u2500\u2500 docs/\n# \u251c\u2500\u2500 tests/\n# \u2514\u2500\u2500 data/\n#     \u251c\u2500\u2500 raw/\n#     \u2514\u2500\u2500 processed/\n</code></pre> <p>Verify the structure: <pre><code>tree project/           # If tree is available\n# or\nfind project/ -type d   # List all directories\n</code></pre></p>"},{"location":"Basic/file-management/#bulk-directory-operations","title":"Bulk Directory Operations","text":"<p>Create multiple directories at once: <pre><code>mkdir week{1..10}                    # Creates week1, week2, ..., week10\nmkdir -p courses/{math,physics,chemistry}/{homework,notes,exams}\n</code></pre></p>"},{"location":"Basic/file-management/#directory-navigation-shortcuts","title":"Directory Navigation Shortcuts","text":"<pre><code>cd -                    # Switch to previous directory\npushd /path/to/dir     # Save current directory and go to new one\npopd                   # Return to saved directory\ndirs                   # Show directory stack\n</code></pre>"},{"location":"Basic/file-management/#advanced-file-operations","title":"Advanced File Operations","text":""},{"location":"Basic/file-management/#copying-files-and-directories","title":"Copying Files and Directories","text":"<p>The <code>cp</code> command has many powerful options:</p> <pre><code># Basic copying\ncp source.txt destination.txt\n\n# Copy with preservation of attributes\ncp -p file.txt backup.txt           # Preserve timestamps and permissions\ncp -a directory/ backup_directory/  # Archive mode (preserves everything)\n\n# Interactive and verbose copying\ncp -iv source.txt destination.txt   # Ask before overwriting, show progress\n\n# Copy multiple files to directory\ncp file1.txt file2.txt file3.txt target_directory/\n\n# Copy with pattern matching\ncp *.txt backup_directory/          # Copy all .txt files\ncp data_*.csv analysis/             # Copy files matching pattern\n</code></pre>"},{"location":"Basic/file-management/#advanced-moving-and-renaming","title":"Advanced Moving and Renaming","text":"<pre><code># Rename multiple files (simple cases)\nmv old_name.txt new_name.txt\n\n# Move multiple files\nmv *.log logs_directory/\n\n# Rename with pattern (using bash parameter expansion)\nfor file in *.txt; do\n    mv \"$file\" \"${file%.txt}.backup\"\ndone\n\n# Move with backup of existing files\nmv -b source.txt destination.txt    # Creates destination.txt~\n</code></pre>"},{"location":"Basic/file-management/#safe-file-deletion","title":"Safe File Deletion","text":"<pre><code># Always use -i for interactive deletion\nrm -i unwanted_file.txt\n\n# Remove empty directories\nrmdir empty_directory/\n\n# Remove directory and contents (be very careful!)\nrm -ri directory_to_delete/         # Interactive recursive deletion\n\n# Remove files older than a certain date\nfind . -name \"*.tmp\" -mtime +30 -delete    # Delete .tmp files older than 30 days\n</code></pre>"},{"location":"Basic/file-management/#file-search-and-discovery","title":"File Search and Discovery","text":""},{"location":"Basic/file-management/#using-find-for-file-search","title":"Using <code>find</code> for File Search","text":"<p>The <code>find</code> command is incredibly powerful for locating files:</p> <p>Basic syntax: <pre><code>find [path] [expression]\n</code></pre></p> <p>Search by name: <pre><code>find . -name \"*.txt\"                # Find all .txt files  \nfind . -name \"data*\"                # Find files starting with \"data\"  \nfind . -iname \"*.PDF\"               # Case-insensitive search  \nfind /home -name \"config.yml\"       # Search in specific directory\n</code></pre></p> <p>Search by type: <pre><code>find . -type f                      # Find files only\nfind . -type d                      # Find directories only\nfind . -type l                      # Find symbolic links\n</code></pre></p> <p>Search by size: <pre><code>find . -size +100M                  # Files larger than 100MB\nfind . -size -1k                    # Files smaller than 1KB\nfind . -size 50c                    # Files exactly 50 bytes\n</code></pre></p> <p>Search by modification time: <pre><code>find . -mtime -7                    # Modified in last 7 days\nfind . -mtime +30                   # Modified more than 30 days ago\nfind . -mmin -60                    # Modified in last 60 minutes\n</code></pre></p> <p>Search by permissions: <pre><code>find . -perm 755                    # Files with exact permissions\nfind . -perm -644                   # Files with at least these permissions\nfind . -executable                  # Executable files\n</code></pre></p> <p>Combining search criteria: <pre><code>find . -name \"*.log\" -size +10M     # Large log files\nfind . -type f -name \"*.tmp\" -mtime +7 -delete    # Delete old temp files\n</code></pre></p>"},{"location":"Basic/file-management/#using-locate-for-fast-search","title":"Using <code>locate</code> for Fast Search","text":"<p>The <code>locate</code> command uses a database for fast searching:</p> <pre><code>locate filename.txt                 # Fast search (requires updatedb)\nlocate -i filename                  # Case-insensitive\nlocate \"*.pdf\"                      # Search with patterns\n</code></pre> <p>Update the locate database: <pre><code>sudo updatedb                       # Update database (run as administrator)\n</code></pre></p>"},{"location":"Basic/file-management/#using-which-and-whereis","title":"Using <code>which</code> and <code>whereis</code>","text":"<p>Find executable files and their locations:</p> <pre><code>which python                        # Find location of python executable\nwhich -a python                     # Find all python executables in PATH\nwhereis python                      # Find binary, source, and manual pages\n</code></pre>"},{"location":"Basic/file-management/#file-content-operations","title":"File Content Operations","text":""},{"location":"Basic/file-management/#viewing-and-analyzing-files","title":"Viewing and Analyzing Files","text":"<p>Get file information: <pre><code>file document.txt                   # Determine file type\nstat document.txt                   # Detailed file information\nls -lh document.txt                 # Human-readable file size\n</code></pre></p> <p>Count lines, words, and characters: <pre><code>wc document.txt                     # Lines, words, characters\nwc -l document.txt                  # Count lines only\nwc -w document.txt                  # Count words only\nwc -c document.txt                  # Count characters only\n</code></pre></p>"},{"location":"Basic/file-management/#comparing-files","title":"Comparing Files","text":"<pre><code>diff file1.txt file2.txt            # Show differences between files\ndiff -u file1.txt file2.txt         # Unified diff format\ndiff -r dir1/ dir2/                 # Compare directories recursively\n</code></pre>"},{"location":"Basic/file-management/#splitting-and-joining-files","title":"Splitting and Joining Files","text":"<p>Split large files: <pre><code>split -l 1000 large_file.txt        # Split into files of 1000 lines each\nsplit -b 100M large_file.bin        # Split into 100MB chunks\nsplit -d -l 1000 data.txt data_     # Split with numeric suffixes\n</code></pre></p> <p>Join files back together: <pre><code>cat data_00 data_01 data_02 &gt; combined_data.txt\n</code></pre></p>"},{"location":"Basic/file-management/#file-permissions-and-ownership","title":"File Permissions and Ownership","text":""},{"location":"Basic/file-management/#understanding-file-permissions","title":"Understanding File Permissions","text":"<p>File permissions are displayed in the format: <code>drwxrwxrwx</code></p> <ul> <li>First character: file type (<code>d</code> = directory, <code>-</code> = regular file, <code>l</code> = link)</li> <li>Next 9 characters: permissions for owner, group, and others</li> <li>Each group of 3: read (<code>r</code>), write (<code>w</code>), execute (<code>x</code>)</li> </ul> <p>Example permission breakdown: <pre><code>ls -l document.txt\n# -rw-r--r-- 1 username group 1024 Jan 15 10:30 document.txt\n# \u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\n# \u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2514\u2514\u2514\u2500 others: read\n# \u2502\u2502\u2502\u2502\u2502\u2502\u2502\n# \u2502\u2502\u2502\u2502\u2514\u2514\u2514\u2500\u2500\u2500\u2500 group: read, execute\n# \u2502\u2502\u2502\u2502\n# \u2502\u2514\u2514\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500 owner: read, write\n# \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 regular file\n</code></pre></p>"},{"location":"Basic/file-management/#changing-permissions","title":"Changing Permissions","text":"<p>Numeric method (octal): <pre><code>chmod 755 script.sh                 # rwxr-xr-x (owner: all, group/others: read+execute)\nchmod 644 document.txt              # rw-r--r-- (owner: read+write, others: read)\nchmod 600 private_file.txt          # rw------- (owner only)\n</code></pre></p> <p>Symbolic method: <pre><code>chmod u+x script.sh                 # Add execute for owner\nchmod g-w document.txt              # Remove write for group\nchmod o+r public_file.txt           # Add read for others\nchmod a+r readme.txt                # Add read for all (owner, group, others)\nchmod u=rw,g=r,o=r document.txt     # Set specific permissions\n</code></pre></p> <p>Recursive permission changes: <pre><code>chmod -R 755 directory/             # Apply to directory and all contents\nchmod -R u+x scripts/               # Make all files in scripts/ executable for owner\n</code></pre></p>"},{"location":"Basic/file-management/#changing-ownership","title":"Changing Ownership","text":"<pre><code>chown username file.txt             # Change owner\nchown username:groupname file.txt   # Change owner and group\nchown -R username:group directory/  # Change ownership recursively\n</code></pre>"},{"location":"Basic/file-management/#file-compression-and-archives","title":"File Compression and Archives","text":""},{"location":"Basic/file-management/#creating-archives-with-tar","title":"Creating Archives with <code>tar</code>","text":"<p>The <code>tar</code> command creates and extracts archives:</p> <p>Create archives: <pre><code>tar -cf archive.tar files/          # Create archive\ntar -czf archive.tar.gz files/      # Create compressed archive (gzip)\ntar -cjf archive.tar.bz2 files/     # Create archive with bzip2 compression\n</code></pre></p> <p>Extract archives: <pre><code>tar -xf archive.tar                 # Extract archive\ntar -xzf archive.tar.gz             # Extract gzip archive\ntar -xjf archive.tar.bz2            # Extract bzip2 archive\n</code></pre></p> <p>List archive contents: <pre><code>tar -tf archive.tar                 # List contents without extracting\ntar -tzf archive.tar.gz             # List contents of compressed archive\n</code></pre></p> <p>Common tar options: - <code>-c</code>: Create archive  - <code>-x</code>: Extract archive - <code>-t</code>: List contents - <code>-f</code>: Specify filename - <code>-z</code>: Use gzip compression - <code>-j</code>: Use bzip2 compression - <code>-v</code>: Verbose output - <code>-C</code>: Change to directory before operation  </p>"},{"location":"Basic/file-management/#working-with-compressed-files","title":"Working with Compressed Files","text":"<pre><code># Gzip compression\ngzip file.txt                       # Compress file (creates file.txt.gz)\ngunzip file.txt.gz                  # Decompress file\nzcat file.txt.gz                    # View compressed file without extracting\n\n# Zip archives\nzip archive.zip file1.txt file2.txt # Create zip archive\nzip -r archive.zip directory/       # Create zip archive recursively\nunzip archive.zip                   # Extract zip archive\nunzip -l archive.zip                # List zip contents\n</code></pre>"},{"location":"Basic/file-management/#symbolic-links-and-hard-links","title":"Symbolic Links and Hard Links","text":""},{"location":"Basic/file-management/#creating-and-managing-links","title":"Creating and Managing Links","text":"<p>Symbolic links (soft links): <pre><code>ln -s /path/to/original linkname    # Create symbolic link\nln -s ../data/file.txt shortcut.txt # Relative symbolic link\n</code></pre></p> <p>Hard links: <pre><code>ln original.txt hardlink.txt        # Create hard link\n</code></pre></p> <p>Managing links: <pre><code>ls -l linkname                      # Check where symbolic link points\nreadlink linkname                   # Show target of symbolic link\nfind . -type l                      # Find all symbolic links\n</code></pre></p>"},{"location":"Basic/file-management/#differences-between-link-types","title":"Differences Between Link Types","text":"<p>Symbolic links: - Point to a path (can be relative or absolute) - Can link to files or directories - Can link across filesystems - Become broken if target is moved or deleted  </p> <p>Hard links: - Point to the same inode as the original file - Cannot link to directories - Cannot link across filesystems - Remain valid even if original filename is deleted  </p>"},{"location":"Basic/file-management/#file-monitoring-and-watching","title":"File Monitoring and Watching","text":""},{"location":"Basic/file-management/#real-time-file-monitoring","title":"Real-time File Monitoring","text":"<pre><code>tail -f logfile.txt                 # Follow file changes in real-time\ntail -F logfile.txt                 # Follow file, handle rotation\nwatch -n 1 \"ls -la\"                 # Watch command output every second\n</code></pre>"},{"location":"Basic/file-management/#monitoring-directory-changes","title":"Monitoring Directory Changes","text":"<pre><code># If inotify-tools is available\ninotifywait -m -e create,delete,modify directory/\n</code></pre>"},{"location":"Basic/file-management/#batch-file-operations","title":"Batch File Operations","text":""},{"location":"Basic/file-management/#using-loops-for-batch-processing","title":"Using Loops for Batch Processing","text":"<p>Process multiple files: <pre><code># Rename multiple files\nfor file in *.txt; do\n    mv \"$file\" \"${file%.txt}.backup\"\ndone\n\n# Convert file formats (example)\nfor file in *.md; do\n    pandoc \"$file\" -o \"${file%.md}.pdf\"\ndone\n\n# Process files in subdirectories\nfind . -name \"*.log\" -exec gzip {} \\;\n</code></pre></p>"},{"location":"Basic/file-management/#using-parameter-expansion","title":"Using Parameter Expansion","text":"<p>Bash parameter expansion provides powerful string manipulation:</p> <pre><code>filename=\"document.txt\"\necho ${filename%.*}         # Remove extension: \"document\"\necho ${filename##*.}        # Get extension: \"txt\"\necho ${filename/.txt/.bak}  # Replace extension: \"document.bak\"\n\n# Useful for batch renaming\nfor file in *.jpeg; do\n    mv \"$file\" \"${file%.jpeg}.jpg\"\ndone\n</code></pre>"},{"location":"Basic/file-management/#file-system-navigation-and-organization","title":"File System Navigation and Organization","text":""},{"location":"Basic/file-management/#organizing-project-files","title":"Organizing Project Files","text":"<p>Recommended directory structure: <pre><code>project/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/                    # Source code\n\u251c\u2500\u2500 docs/                   # Documentation\n\u251c\u2500\u2500 tests/                  # Test files\n\u251c\u2500\u2500 data/                   # Data files\n\u2502   \u251c\u2500\u2500 raw/               # Original data\n\u2502   \u2514\u2500\u2500 processed/         # Processed data\n\u251c\u2500\u2500 scripts/               # Utility scripts\n\u251c\u2500\u2500 results/               # Output files\n\u2514\u2500\u2500 archive/               # Old versions\n</code></pre></p> <p>Create the structure: <pre><code>mkdir -p project/{src,docs,tests,data/{raw,processed},scripts,results,archive}\n</code></pre></p>"},{"location":"Basic/file-management/#file-naming-conventions","title":"File Naming Conventions","text":"<p>Good practices: - Use descriptive names - Avoid spaces (use underscores or hyphens) - Include dates in format YYYY-MM-DD - Use consistent extensions  </p> <p>Examples: <pre><code># Good\ndata_2024-01-15_experiment_results.csv\nanalysis_script_v2.py\nmeeting_notes_2024-01-15.md\n\n# Avoid\ndata file.csv\nscript.py\nnotes.txt\n</code></pre></p>"},{"location":"Basic/file-management/#practical-examples-and-use-cases","title":"Practical Examples and Use Cases","text":""},{"location":"Basic/file-management/#example-1-organizing-downloaded-files","title":"Example 1: Organizing Downloaded Files","text":"<pre><code># Create organization structure\nmkdir -p ~/Downloads/{documents,images,videos,archives,software}\n\n# Move files by type\nmv ~/Downloads/*.{pdf,doc,docx,txt} ~/Downloads/documents/\nmv ~/Downloads/*.{jpg,jpeg,png,gif} ~/Downloads/images/\nmv ~/Downloads/*.{mp4,avi,mkv} ~/Downloads/videos/\nmv ~/Downloads/*.{zip,tar,gz} ~/Downloads/archives/\nmv ~/Downloads/*.{deb,rpm,dmg,exe} ~/Downloads/software/\n</code></pre>"},{"location":"Basic/file-management/#example-2-backup-script","title":"Example 2: Backup Script","text":"<pre><code>#!/bin/bash\n# Simple backup script\n\nbackup_date=$(date +%Y-%m-%d)\nbackup_dir=\"/backup/daily_$backup_date\"\n\nmkdir -p \"$backup_dir\"\ntar -czf \"$backup_dir/documents_backup.tar.gz\" ~/Documents/\ntar -czf \"$backup_dir/projects_backup.tar.gz\" ~/Projects/\n\necho \"Backup completed: $backup_dir\"\n</code></pre>"},{"location":"Basic/file-management/#example-3-finding-and-cleaning-old-files","title":"Example 3: Finding and Cleaning Old Files","text":"<pre><code># Find files older than 30 days\nfind ~/Downloads -type f -mtime +30\n\n# Find large files (&gt;100MB)\nfind ~ -size +100M -type f\n\n# Clean temporary files\nfind /tmp -name \"*.tmp\" -mtime +7 -delete\nfind ~ -name \"*~\" -delete                    # Remove backup files\nfind ~ -name \".DS_Store\" -delete             # Remove macOS metadata\n</code></pre>"},{"location":"Basic/file-management/#example-4-duplicate-file-detection","title":"Example 4: Duplicate File Detection","text":"<pre><code># Find duplicate files by size\nfind . -type f -exec ls -la {} \\; | sort -k5 -n | uniq -d -f4\n\n# Find files with same name in different directories\nfind . -type f -printf \"%f\\n\" | sort | uniq -d\n</code></pre>"},{"location":"Basic/file-management/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"Basic/file-management/#permission-denied-errors","title":"Permission Denied Errors","text":"<pre><code># Check file permissions\nls -la filename\n\n# Fix common permission issues\nchmod 644 document.txt              # For regular files\nchmod 755 script.sh                 # For executable files\nchmod 755 directory/                # For directories\n</code></pre>"},{"location":"Basic/file-management/#disk-space-issues","title":"Disk Space Issues","text":"<pre><code># Check disk usage\ndf -h                               # Show disk space usage\ndu -sh *                            # Show directory sizes\ndu -sh * | sort -hr                 # Sort by size (largest first)\n\n# Find large files\nfind . -size +100M -type f\n</code></pre>"},{"location":"Basic/file-management/#broken-symbolic-links","title":"Broken Symbolic Links","text":"<pre><code># Find broken symbolic links\nfind . -type l -exec test ! -e {} \\; -print\n\n# Remove broken symbolic links\nfind . -type l -exec test ! -e {} \\; -delete\n</code></pre>"},{"location":"Basic/file-management/#best-practices-for-file-management","title":"Best Practices for File Management","text":"<ol> <li>Use descriptive names that indicate the file's purpose and content  </li> <li>Organize files hierarchically with logical directory structures  </li> <li>Regular backups of important files  </li> <li>Clean up regularly to remove unnecessary files  </li> <li>Use version control for code and important documents  </li> <li>Set appropriate permissions to protect sensitive files  </li> <li>Document your organization system so others can understand it  </li> <li>Use archives for long-term storage of completed projects  </li> <li>Test commands on sample files before applying to important data  </li> <li>Keep track of file locations using consistent naming and organization  </li> </ol>"},{"location":"Basic/file-management/#summary","title":"Summary","text":"<p>Effective file management in bash involves mastering a variety of commands and techniques. Key skills include:</p> <ul> <li>Understanding file system navigation and paths  </li> <li>Using <code>find</code> for powerful file searching  </li> <li>Managing permissions and ownership appropriately  </li> <li>Creating and extracting archives  </li> <li>Implementing batch operations for efficiency  </li> <li>Organizing files systematically  </li> <li>Monitoring file changes when needed  </li> </ul> <p>Practice these techniques regularly, and you'll develop the skills needed to manage files efficiently in any UNIX environment. Remember to always test commands on sample data before applying them to important files, and maintain regular backups of critical data.</p>"},{"location":"Basic/networking/","title":"Networking in Bash","text":""},{"location":"Basic/networking/#introduction","title":"IntroductionTable of Content","text":"<p>Networking is a fundamental aspect of modern computing, and the bash shell provides powerful tools for network troubleshooting, monitoring, and administration. This guide introduces essential networking commands that every system administrator and developer should know.</p> <ul> <li>Networking in Bash<ul> <li>Introduction</li> <li>Basic Network Information Commands</li> <li>Network Connectivity Testing</li> <li>DNS and Name Resolution</li> <li>Port and Service Information</li> <li>Network Testing and Troubleshooting</li> <li>Network Monitoring</li> <li>Common Network Troubleshooting Workflow</li> <li>Practical Examples</li> <li>Security Considerations</li> <li>Tips for Beginners</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Basic/networking/#basic-network-information-commands","title":"Basic Network Information Commands","text":""},{"location":"Basic/networking/#ifconfig-network-interface-configuration","title":"<code>ifconfig</code> - Network Interface Configuration","text":"<p>The <code>ifconfig</code> command displays and configures network interfaces on your system.</p> <p>Basic usage: <pre><code>ifconfig\n</code></pre></p> <p>This shows all active network interfaces with their IP addresses, subnet masks, and other configuration details.</p> <p>Example output: <pre><code>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 192.168.1.100  netmask 255.255.255.0  broadcast 192.168.1.255\n        ether 08:00:27:12:34:56  txqueuelen 1000  (Ethernet)\n</code></pre></p> <p>Specific interface: <pre><code>ifconfig eth0\n</code></pre></p> <p>Configure an interface (requires root privileges): <pre><code>sudo ifconfig eth0 192.168.1.50 netmask 255.255.255.0\n</code></pre></p>"},{"location":"Basic/networking/#ip-modern-network-configuration-tool","title":"<code>ip</code> - Modern Network Configuration Tool","text":"<p>The <code>ip</code> command is the modern replacement for several networking tools including <code>ifconfig</code>.</p> <p>Show all interfaces: <pre><code>ip addr show\n# or shorter:\nip a\n</code></pre></p> <p>Show specific interface: <pre><code>ip addr show eth0\n</code></pre></p> <p>Show routing table: <pre><code>ip route show\n# or shorter:\nip r\n</code></pre></p> <p>Add a static route: <pre><code>sudo ip route add 192.168.2.0/24 via 192.168.1.1\n</code></pre></p>"},{"location":"Basic/networking/#network-connectivity-testing","title":"Network Connectivity Testing","text":""},{"location":"Basic/networking/#ping-test-network-connectivity","title":"<code>ping</code> - Test Network Connectivity","text":"<p>The <code>ping</code> command sends ICMP echo requests to test connectivity to remote hosts.</p> <p>Basic ping: <pre><code>ping google.com\n</code></pre></p> <p>Ping with count limit: <pre><code>ping -c 4 google.com\n</code></pre></p> <p>Ping with specific interval: <pre><code>ping -i 2 google.com  # ping every 2 seconds\n</code></pre></p> <p>Ping IPv6: <pre><code>ping6 google.com\n</code></pre></p> <p>Example output: <pre><code>PING google.com (142.250.191.14) 56(84) bytes of data.\n64 bytes from lhr35s10-in-f14.1e100.net (142.250.191.14): icmp_seq=1 ttl=119 time=12.3 ms\n64 bytes from lhr35s10-in-f14.1e100.net (142.250.191.14): icmp_seq=2 ttl=119 time=11.8 ms\n</code></pre></p>"},{"location":"Basic/networking/#traceroute-trace-network-path","title":"<code>traceroute</code> - Trace Network Path","text":"<p>Shows the route packets take to reach a destination.</p> <p>Basic traceroute: <pre><code>traceroute google.com\n</code></pre></p> <p>Traceroute with specific interface: <pre><code>traceroute -i eth0 google.com\n</code></pre></p> <p>Example output: <pre><code>traceroute to google.com (142.250.191.14), 30 hops max, 60 byte packets\n 1  192.168.1.1 (192.168.1.1)  1.234 ms  1.198 ms  1.167 ms\n 2  10.0.0.1 (10.0.0.1)  5.432 ms  5.401 ms  5.378 ms\n 3  203.0.113.1 (203.0.113.1)  12.567 ms  12.534 ms  12.501 ms\n</code></pre></p>"},{"location":"Basic/networking/#dns-and-name-resolution","title":"DNS and Name Resolution","text":""},{"location":"Basic/networking/#nslookup-dns-lookup-tool","title":"<code>nslookup</code> - DNS Lookup Tool","text":"<p>Query DNS servers to resolve domain names.</p> <p>Basic lookup: <pre><code>nslookup google.com\n</code></pre></p> <p>Reverse lookup (IP to domain): <pre><code>nslookup 8.8.8.8\n</code></pre></p> <p>Query specific DNS server: <pre><code>nslookup google.com 8.8.8.8\n</code></pre></p> <p>Query specific record type: <pre><code>nslookup -type=MX google.com  # Mail exchange records\nnslookup -type=NS google.com  # Name server records\n</code></pre></p>"},{"location":"Basic/networking/#dig-advanced-dns-lookup","title":"<code>dig</code> - Advanced DNS Lookup","text":"<p>More powerful and flexible than <code>nslookup</code>.</p> <p>Basic dig: <pre><code>dig google.com\n</code></pre></p> <p>Short answer: <pre><code>dig +short google.com\n</code></pre></p> <p>Query specific record types: <pre><code>dig MX google.com      # Mail exchange records\ndig NS google.com      # Name server records\ndig AAAA google.com    # IPv6 addresses\n</code></pre></p> <p>Reverse lookup: <pre><code>dig -x 8.8.8.8\n</code></pre></p>"},{"location":"Basic/networking/#host-simple-dns-lookup","title":"<code>host</code> - Simple DNS Lookup","text":"<p>Basic lookup: <pre><code>host google.com\n</code></pre></p> <p>All record types: <pre><code>host -a google.com\n</code></pre></p>"},{"location":"Basic/networking/#port-and-service-information","title":"Port and Service Information","text":""},{"location":"Basic/networking/#netstat-network-statistics","title":"<code>netstat</code> - Network Statistics","text":"<p>Shows network connections, routing tables, and network interface statistics.</p> <p>Show all connections: <pre><code>netstat -a\n</code></pre></p> <p>Show listening ports: <pre><code>netstat -l\n</code></pre></p> <p>Show TCP connections: <pre><code>netstat -t\n</code></pre></p> <p>Show UDP connections: <pre><code>netstat -u\n</code></pre></p> <p>Show processes using ports: <pre><code>netstat -p\n</code></pre></p> <p>Combined useful options: <pre><code>netstat -tulpn\n# t = TCP, u = UDP, l = listening, p = process, n = numerical addresses\n</code></pre></p> <p>Example output: <pre><code>Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1234/sshd\ntcp        0      0 127.0.0.1:3306          0.0.0.0:*               LISTEN      5678/mysqld\n</code></pre></p>"},{"location":"Basic/networking/#ss-socket-statistics-modern-alternative","title":"<code>ss</code> - Socket Statistics (Modern Alternative)","text":"<p>Show all sockets: <pre><code>ss -a\n</code></pre></p> <p>Show listening sockets: <pre><code>ss -l\n</code></pre></p> <p>Show TCP sockets: <pre><code>ss -t\n</code></pre></p> <p>Show processes: <pre><code>ss -p\n</code></pre></p> <p>Combined options: <pre><code>ss -tulpn\n</code></pre></p>"},{"location":"Basic/networking/#lsof-list-open-files","title":"<code>lsof</code> - List Open Files","text":"<p>Can show network connections by process.</p> <p>Show network connections: <pre><code>lsof -i\n</code></pre></p> <p>Show connections on specific port: <pre><code>lsof -i :80\nlsof -i :22\n</code></pre></p> <p>Show connections by specific process: <pre><code>lsof -i -p 1234\n</code></pre></p>"},{"location":"Basic/networking/#network-testing-and-troubleshooting","title":"Network Testing and Troubleshooting","text":""},{"location":"Basic/networking/#telnet-test-port-connectivity","title":"<code>telnet</code> - Test Port Connectivity","text":"<p>Test if a port is open: <pre><code>telnet google.com 80\ntelnet localhost 22\n</code></pre></p> <p>If successful, you'll see: <pre><code>Trying 142.250.191.14...\nConnected to google.com.\nEscape character is '^]'.\n</code></pre></p>"},{"location":"Basic/networking/#nc-netcat-network-swiss-army-knife","title":"<code>nc</code> (netcat) - Network Swiss Army Knife","text":"<p>Test port connectivity: <pre><code>nc -zv google.com 80\n</code></pre></p> <p>Listen on a port: <pre><code>nc -l 8080\n</code></pre></p> <p>Send data to a port: <pre><code>echo \"Hello\" | nc localhost 8080\n</code></pre></p> <p>Port scanning: <pre><code>nc -zv google.com 80-90\n</code></pre></p>"},{"location":"Basic/networking/#curl-transfer-data-from-servers","title":"<code>curl</code> - Transfer Data from Servers","text":"<p>Basic HTTP request: <pre><code>curl http://google.com\n</code></pre></p> <p>Show headers: <pre><code>curl -I http://google.com\n</code></pre></p> <p>Follow redirects: <pre><code>curl -L http://google.com\n</code></pre></p> <p>Download a file: <pre><code>curl -O http://example.com/file.txt\n</code></pre></p> <p>POST data: <pre><code>curl -X POST -d \"key=value\" http://example.com/api\n</code></pre></p>"},{"location":"Basic/networking/#wget-download-files","title":"<code>wget</code> - Download Files","text":"<p>Download a file: <pre><code>wget http://example.com/file.txt\n</code></pre></p> <p>Download recursively: <pre><code>wget -r http://example.com/\n</code></pre></p> <p>Download in background: <pre><code>wget -b http://example.com/largefile.zip\n</code></pre></p>"},{"location":"Basic/networking/#network-monitoring","title":"Network Monitoring","text":""},{"location":"Basic/networking/#iftop-real-time-network-usage","title":"<code>iftop</code> - Real-time Network Usage","text":"<p>Shows bandwidth usage by connection.</p> <pre><code>sudo iftop\n</code></pre>"},{"location":"Basic/networking/#netstat-for-monitoring","title":"<code>netstat</code> for Monitoring","text":"<p>Show network statistics: <pre><code>netstat -s\n</code></pre></p> <p>Monitor connections continuously: <pre><code>watch netstat -tulpn\n</code></pre></p>"},{"location":"Basic/networking/#arp-arp-table-management","title":"<code>arp</code> - ARP Table Management","text":"<p>Show ARP table: <pre><code>arp -a\n</code></pre></p> <p>Add static ARP entry: <pre><code>sudo arp -s 192.168.1.100 00:11:22:33:44:55\n</code></pre></p> <p>Delete ARP entry: <pre><code>sudo arp -d 192.168.1.100\n</code></pre></p>"},{"location":"Basic/networking/#common-network-troubleshooting-workflow","title":"Common Network Troubleshooting Workflow","text":"<p>When troubleshooting network issues, follow this systematic approach:</p> <ol> <li> <p>Check local network configuration: <pre><code>ip addr show\nip route show\n</code></pre></p> </li> <li> <p>Test local connectivity: <pre><code>ping 127.0.0.1        # Test loopback\nping 192.168.1.1      # Test gateway\n</code></pre></p> </li> <li> <p>Test DNS resolution: <pre><code>nslookup google.com\ndig google.com\n</code></pre></p> </li> <li> <p>Test external connectivity: <pre><code>ping 8.8.8.8          # Test external IP\nping google.com       # Test with DNS\n</code></pre></p> </li> <li> <p>Check service ports: <pre><code>netstat -tulpn | grep :80\ntelnet localhost 80\n</code></pre></p> </li> <li> <p>Trace network path: <pre><code>traceroute google.com\n</code></pre></p> </li> </ol>"},{"location":"Basic/networking/#practical-examples","title":"Practical Examples","text":""},{"location":"Basic/networking/#example-1-check-if-a-web-server-is-running","title":"Example 1: Check if a Web Server is Running","text":"<pre><code># Check if port 80 is listening\nnetstat -tulpn | grep :80\n\n# Test connectivity to the port\ntelnet localhost 80\n\n# Check with curl\ncurl -I http://localhost\n</code></pre>"},{"location":"Basic/networking/#example-2-troubleshoot-ssh-connection","title":"Example 2: Troubleshoot SSH Connection","text":"<pre><code># Check if SSH service is running\nnetstat -tulpn | grep :22\n\n# Test SSH port connectivity\ntelnet server.example.com 22\n\n# Check SSH with verbose output\nssh -v user@server.example.com\n</code></pre>"},{"location":"Basic/networking/#example-3-monitor-network-usage","title":"Example 3: Monitor Network Usage","text":"<pre><code># Show current connections\nnetstat -i\n\n# Monitor bandwidth usage\nsudo iftop\n\n# Show network statistics\nnetstat -s\n</code></pre>"},{"location":"Basic/networking/#security-considerations","title":"Security Considerations","text":"<ul> <li>Always be cautious when using networking commands on production systems</li> <li>Some commands require root privileges (use <code>sudo</code>)</li> <li>Network scanning tools like <code>nmap</code> should only be used on systems you own or have permission to test</li> <li>Be aware that some network tools generate significant traffic</li> </ul>"},{"location":"Basic/networking/#tips-for-beginners","title":"Tips for Beginners","text":"<ol> <li>Start with basic commands: Begin with <code>ping</code>, <code>ifconfig</code>, and <code>netstat</code></li> <li>Use help options: Most commands support <code>-h</code> or <code>--help</code> flags</li> <li>Read man pages: Use <code>man command</code> for detailed documentation</li> <li>Practice in safe environments: Use virtual machines or isolated networks for learning</li> <li>Combine commands: Use pipes (<code>|</code>) and redirection (<code>&gt;</code>) to combine tools effectively</li> </ol>"},{"location":"Basic/networking/#conclusion","title":"Conclusion","text":"<p>These networking commands form the foundation of network troubleshooting and administration in Unix-like systems. Practice these commands regularly, and gradually incorporate more advanced options as you become comfortable with the basics. Remember that networking is both an art and a science - systematic troubleshooting combined with experience will make you proficient in diagnosing and resolving network issues.</p>"},{"location":"Basic/process-management/","title":"Process Management in Bash","text":""},{"location":"Basic/process-management/#introduction","title":"IntroductionTable of Content","text":"<p>In UNIX-like systems, a process is a running instance of a program. When you execute a command in the terminal, you create a process. Understanding how to manage these processes is crucial for effective system administration and development work.</p> <p>This guide will teach you the essential commands and concepts for managing processes in bash, even if you've never used a UNIX environment before.</p> <ul> <li>Process Management in Bash<ul> <li>Introduction</li> <li>What is a Process?</li> <li>Basic Process Commands</li> <li>Process States</li> <li>Background and Foreground Processes</li> <li>Process Termination</li> <li>Process Monitoring and Information</li> <li>Practical Examples</li> <li>Process Priority and Nice Values</li> <li>Best Practices</li> <li>Common Scenarios and Solutions</li> <li>Summary</li> </ul> </li> </ul>"},{"location":"Basic/process-management/#what-is-a-process","title":"What is a Process?","text":"<p>A process is simply a program that is currently running on your system. Every time you run a command, open an application, or execute a script, you create a new process. Each process has:</p> <ul> <li>A unique Process ID (PID) - a number that identifies the process</li> <li>A Parent Process ID (PPID) - the ID of the process that created it</li> <li>Memory allocation - the amount of RAM the process uses</li> <li>CPU time - how much processor time the process has consumed</li> </ul>"},{"location":"Basic/process-management/#basic-process-commands","title":"Basic Process Commands","text":""},{"location":"Basic/process-management/#ps-display-running-processes","title":"<code>ps</code> - Display Running Processes","text":"<p>The <code>ps</code> command shows you information about currently running processes.</p> <p>Basic syntax: <pre><code>ps [options]\n</code></pre></p> <p>Common examples:</p> <pre><code># Show processes for current user\nps\n\n# Show all processes with detailed information\nps aux\n\n# Show processes in a tree format (shows parent-child relationships)\nps -ef --forest\n</code></pre> <p>Example output of <code>ps aux</code>: <pre><code>USER       PID  %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\njohn      1234  0.5  2.1  12345  4567 pts/0    Ss   09:30   0:01 bash\njohn      5678  0.0  0.8   8901  2345 pts/0    R+   09:35   0:00 ps aux\n</code></pre></p> <p>Column explanations: - USER: Who owns the process - PID: Process ID - %CPU: Percentage of CPU usage - %MEM: Percentage of memory usage - COMMAND: The command that started the process  </p>"},{"location":"Basic/process-management/#top-real-time-process-monitor","title":"<code>top</code> - Real-time Process Monitor","text":"<p>The <code>top</code> command displays processes in real-time, updating every few seconds.</p> <pre><code># Start top (press 'q' to quit)\ntop\n\n# Show processes for a specific user\ntop -u username\n</code></pre> <p>Useful <code>top</code> commands while running: - <code>q</code>: Quit top - <code>k</code>: Kill a process (you'll be prompted for the PID) - <code>h</code>: Show help - <code>M</code>: Sort by memory usage - <code>P</code>: Sort by CPU usage  </p>"},{"location":"Basic/process-management/#htop-enhanced-process-monitor","title":"<code>htop</code> - Enhanced Process Monitor","text":"<p><code>htop</code> is an improved version of <code>top</code> with a more user-friendly interface.</p> <pre><code># Start htop (may need to install first)\nhtop\n</code></pre> <p>Navigation in htop: - Use arrow keys to navigate - <code>F9</code>: Kill selected process - <code>F10</code>: Quit htop  </p>"},{"location":"Basic/process-management/#process-states","title":"Process States","text":"<p>Processes can be in different states:</p> <ul> <li>Running (R): Currently executing  </li> <li>Sleeping (S): Waiting for an event (like user input)  </li> <li>Stopped (T): Process has been stopped (paused)  </li> <li>Zombie (Z): Process has finished but parent hasn't cleaned it up yet  </li> </ul>"},{"location":"Basic/process-management/#background-and-foreground-processes","title":"Background and Foreground Processes","text":""},{"location":"Basic/process-management/#running-commands-in-background","title":"Running Commands in Background","text":"<p>You can run commands in the background using the <code>&amp;</code> symbol:</p> <pre><code># Run a command in the background\nlong_running_command &amp;\n\n# Example: copy a large file in the background\ncp large_file.txt backup_file.txt &amp;\n</code></pre>"},{"location":"Basic/process-management/#jobs-list-active-jobs","title":"<code>jobs</code> - List Active Jobs","text":"<p>The <code>jobs</code> command shows processes started from your current shell:</p> <pre><code># List all jobs\njobs\n\n# List jobs with process IDs\njobs -l\n</code></pre> <p>Example output: <pre><code>[1]+  Running                 cp large_file.txt backup_file.txt &amp;\n[2]-  Stopped                 vim document.txt\n</code></pre></p>"},{"location":"Basic/process-management/#fg-and-bg-foreground-and-background-control","title":"<code>fg</code> and <code>bg</code> - Foreground and Background Control","text":"<pre><code># Bring job number 1 to foreground\nfg %1\n\n# Send job number 1 to background\nbg %1\n\n# Bring the most recent job to foreground\nfg\n</code></pre>"},{"location":"Basic/process-management/#stopping-and-resuming-processes","title":"Stopping and Resuming Processes","text":"<ul> <li>Ctrl+Z: Stop (pause) the current foreground process  </li> <li>Ctrl+C: Terminate the current foreground process  </li> </ul> <p>Example workflow: <pre><code># Start editing a file\nvim document.txt\n\n# Press Ctrl+Z to stop vim and return to shell\n# [1]+  Stopped                 vim document.txt\n\n# Continue editing in the background\nbg %1\n\n# Bring it back to foreground when needed\nfg %1\n</code></pre></p>"},{"location":"Basic/process-management/#process-termination","title":"Process Termination","text":""},{"location":"Basic/process-management/#kill-terminate-processes","title":"<code>kill</code> - Terminate Processes","text":"<p>The <code>kill</code> command sends signals to processes to terminate them.</p> <p>Basic syntax: <pre><code>kill [signal] PID\n</code></pre></p> <p>Common signals: - SIGTERM (15): Polite termination request (default) - SIGKILL (9): Force termination (cannot be ignored) - SIGSTOP (19): Stop process - SIGCONT (18): Continue stopped process  </p> <p>Examples: <pre><code># Politely ask process 1234 to terminate\nkill 1234\n\n# Force kill process 1234\nkill -9 1234\n\n# Kill using signal name\nkill -SIGTERM 1234\n\n# Kill all processes with a specific name\nkillall firefox\n</code></pre></p>"},{"location":"Basic/process-management/#killall-kill-processes-by-name","title":"<code>killall</code> - Kill Processes by Name","text":"<pre><code># Kill all processes named \"firefox\"\nkillall firefox\n\n# Kill all processes by a specific user\nkillall -u username\n</code></pre>"},{"location":"Basic/process-management/#pkill-kill-processes-by-pattern","title":"<code>pkill</code> - Kill Processes by Pattern","text":"<pre><code># Kill processes whose names contain \"chrome\"\npkill chrome\n\n# Kill processes owned by specific user\npkill -u username\n</code></pre>"},{"location":"Basic/process-management/#process-monitoring-and-information","title":"Process Monitoring and Information","text":""},{"location":"Basic/process-management/#pgrep-find-process-ids","title":"<code>pgrep</code> - Find Process IDs","text":"<pre><code># Find PIDs of processes named \"firefox\"\npgrep firefox\n\n# Find PIDs with more details\npgrep -l firefox\n</code></pre>"},{"location":"Basic/process-management/#lsof-list-open-files","title":"<code>lsof</code> - List Open Files","text":"<p>Shows which files are being used by which processes:</p> <pre><code># List all open files\nlsof\n\n# List files opened by a specific process\nlsof -p 1234\n\n# List processes using a specific file\nlsof /path/to/file\n\n# List processes using a specific port\nlsof -i :8080\n</code></pre>"},{"location":"Basic/process-management/#practical-examples","title":"Practical Examples","text":""},{"location":"Basic/process-management/#example-1-finding-and-killing-a-misbehaving-process","title":"Example 1: Finding and Killing a Misbehaving Process","text":"<pre><code># 1. Find the process\nps aux | grep firefox\n\n# 2. Kill it using PID (replace 1234 with actual PID)\nkill 1234\n\n# Alternative: kill by name\nkillall firefox\n</code></pre>"},{"location":"Basic/process-management/#example-2-running-a-long-task-in-background","title":"Example 2: Running a Long Task in Background","text":"<pre><code># Start a backup process in background\ntar -czf backup.tar.gz /home/user/documents &amp;\n\n# Check if it's still running\njobs\n\n# Monitor system resources\ntop\n</code></pre>"},{"location":"Basic/process-management/#example-3-managing-a-stopped-process","title":"Example 3: Managing a Stopped Process","text":"<pre><code># Start a text editor\nnano myfile.txt\n\n# Stop it with Ctrl+Z\n# [1]+  Stopped                 nano myfile.txt\n\n# List stopped jobs\njobs\n\n# Resume in background\nbg %1\n\n# Bring back to foreground when ready\nfg %1\n</code></pre>"},{"location":"Basic/process-management/#process-priority-and-nice-values","title":"Process Priority and Nice Values","text":"<p>Processes have priority levels that determine how much CPU time they get. The <code>nice</code> command allows you to start processes with different priorities.</p> <pre><code># Start a process with lower priority (nice value 10)\nnice -n 10 long_running_command\n\n# Start a process with higher priority (requires sudo)\nsudo nice -n -10 important_command\n</code></pre> <p>Nice values range from -20 (highest priority) to 19 (lowest priority).</p>"},{"location":"Basic/process-management/#renice-change-priority-of-running-process","title":"<code>renice</code> - Change Priority of Running Process","text":"<pre><code># Change priority of process 1234 to nice value 5\nrenice 5 1234\n\n# Change priority of all processes by user\nrenice 10 -u username\n</code></pre>"},{"location":"Basic/process-management/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always try gentle termination first: Use <code>kill</code> (SIGTERM) before <code>kill -9</code> (SIGKILL)  </p> </li> <li> <p>Monitor resource usage: Use <code>top</code> or <code>htop</code> to identify resource-hungry processes  </p> </li> <li> <p>Use background processes for long tasks: Don't let long-running commands block your terminal  </p> </li> <li> <p>Be careful with <code>kill -9</code>: It forces termination without cleanup, which can cause data loss  </p> </li> <li> <p>Check before killing: Make sure you're killing the right process by checking the PID  </p> </li> </ol>"},{"location":"Basic/process-management/#common-scenarios-and-solutions","title":"Common Scenarios and Solutions","text":""},{"location":"Basic/process-management/#scenario-1-terminal-becomes-unresponsive","title":"Scenario 1: Terminal Becomes Unresponsive","text":"<pre><code># Open another terminal and find the problematic process\nps aux | grep -i stuck_program\n\n# Kill it\nkill -9 PID\n</code></pre>"},{"location":"Basic/process-management/#scenario-2-too-many-background-jobs","title":"Scenario 2: Too Many Background Jobs","text":"<pre><code># List all jobs\njobs\n\n# Kill specific job\nkill %1\n\n# Kill all jobs\nkill $(jobs -p)\n</code></pre>"},{"location":"Basic/process-management/#scenario-3-finding-resource-heavy-processes","title":"Scenario 3: Finding Resource-Heavy Processes","text":"<pre><code># Sort by CPU usage\nps aux --sort=-%cpu | head -10\n\n# Sort by memory usage\nps aux --sort=-%mem | head -10\n</code></pre>"},{"location":"Basic/process-management/#summary","title":"Summary","text":"<p>Process management is a fundamental skill in UNIX systems. The key commands to remember are:</p> <ul> <li><code>ps aux</code>: List all processes  </li> <li><code>top</code>/<code>htop</code>: Monitor processes in real-time  </li> <li><code>jobs</code>: List jobs started from current shell  </li> <li><code>kill PID</code>: Terminate a process  </li> <li><code>killall name</code>: Kill processes by name  </li> <li><code>fg</code>/<code>bg</code>: Move jobs between foreground and background  </li> <li><code>Ctrl+Z</code>: Stop current process  </li> <li><code>Ctrl+C</code>: Terminate current process  </li> </ul> <p>With these commands, you'll be able to effectively manage processes in any UNIX environment. Practice these commands in a safe environment to become comfortable with process management!</p>"},{"location":"Basic/system-information/","title":"System Information in Bash","text":""},{"location":"Basic/system-information/#introduction","title":"IntroductionTable of Content","text":"<p>Understanding your system's configuration, resources, and current state is crucial for system administration, troubleshooting, and optimization. This guide covers essential bash commands for gathering system information in Unix-like environments. These commands will help you monitor hardware, software, processes, and system performance.</p> <ul> <li>System Information in Bash<ul> <li>Introduction</li> <li>System Overview Commands</li> <li>Hardware Information</li> <li>Memory Information</li> <li>Storage Information</li> <li>Process Information</li> <li>System Load and Performance</li> <li>System Services and Processes</li> <li>System Files and Configuration</li> <li>Environment and Shell Information</li> <li>System Monitoring Scripts</li> <li>Practical Examples</li> <li>Common System Information One-liners</li> <li>Tips for Beginners</li> <li>Useful Aliases</li> <li>Security Considerations</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Basic/system-information/#system-overview-commands","title":"System Overview Commands","text":""},{"location":"Basic/system-information/#uname-system-information","title":"<code>uname</code> - System Information","text":"<p>The <code>uname</code> command provides basic system information including kernel name, version, and architecture.</p> <p>Basic usage: <pre><code>uname\n</code></pre> Output: <code>Linux</code></p> <p>Show all information: <pre><code>uname -a\n</code></pre> Example output: <pre><code>Linux ubuntu 5.15.0-72-generic #79-Ubuntu SMP Wed Apr 19 08:22:18 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre></p> <p>Common options: <pre><code>uname -s    # Kernel name (Linux, Darwin, etc.)\nuname -r    # Kernel release version\nuname -v    # Kernel version\nuname -m    # Machine hardware name (x86_64, arm64, etc.)\nuname -p    # Processor type\nuname -o    # Operating system\n</code></pre></p>"},{"location":"Basic/system-information/#hostname-system-name","title":"<code>hostname</code> - System Name","text":"<p>Show hostname: <pre><code>hostname\n</code></pre></p> <p>Show fully qualified domain name: <pre><code>hostname -f\n</code></pre></p> <p>Show IP address: <pre><code>hostname -I\n</code></pre></p>"},{"location":"Basic/system-information/#whoami-and-id-user-information","title":"<code>whoami</code> and <code>id</code> - User Information","text":"<p>Current user: <pre><code>whoami\n</code></pre></p> <p>Detailed user information: <pre><code>id\n</code></pre> Example output: <pre><code>uid=1000(john) gid=1000(john) groups=1000(john),4(adm),24(cdrom),27(sudo),30(dip)\n</code></pre></p> <p>Show all logged-in users: <pre><code>who\n</code></pre></p> <p>Show current user and login time: <pre><code>w\n</code></pre></p>"},{"location":"Basic/system-information/#hardware-information","title":"Hardware Information","text":""},{"location":"Basic/system-information/#lscpu-linux-sysctl-macos-cpu-information","title":"<code>lscpu</code> (Linux) / <code>sysctl</code> (macOS) - CPU Information","text":"<p>Detailed CPU information (Linux): <pre><code>lscpu\n</code></pre></p> <p>Example output: <pre><code>Architecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nCPU(s):                          4\nOn-line CPU(s) list:             0-3\nThread(s) per core:              2\nCore(s) per socket:              2\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           142\nModel name:                      Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz\n</code></pre></p> <p>On macOS, there is no equivalent to <code>lscpu</code>. The command <code>sysctl</code> allows you to obtain more detailed information in isolation. It combines the <code>lscpu</code> and <code>lshw</code> commands under Linux. You can chain commands together to obtain the equivalent :</p> <pre><code>#!/bin/bash\necho \"Architecture      : $(uname -m)\"\necho \"CPU(s)            : $(sysctl -n hw.ncpu)\"\necho \"Core(s) per CPU   : $(sysctl -n machdep.cpu.core_count)\"\necho \"Thread(s) per CPU : $(sysctl -n machdep.cpu.thread_count)\"\necho \"Model name        : $(sysctl -n machdep.cpu.brand_string)\"\necho \"CPU MHz           : $(sysctl -n hw.cpufrequency | awk '{print $1/1000000 \" MHz\"}')\"\necho \"L1 Cache          : $(sysctl -n hw.l1dcachesize) bytes\"\necho \"L2 Cache          : $(sysctl -n hw.l2cachesize) bytes\"\necho \"L3 Cache          : $(sysctl -n hw.l3cachesize 2&gt;/dev/null || echo \"N/A\")\"\necho \"Memory            : $(( $(sysctl -n hw.memsize) / 1024 / 1024 )) MB\"\n</code></pre>"},{"location":"Basic/system-information/#lshw-hardware-information","title":"<code>lshw</code> - Hardware Information","text":"<p>Complete hardware information (requires root): <pre><code>sudo lshw\n</code></pre></p> <p>Short format: <pre><code>sudo lshw -short\n</code></pre></p> <p>Specific hardware class: <pre><code>sudo lshw -class processor\nsudo lshw -class memory\nsudo lshw -class disk\nsudo lshw -class network\n</code></pre></p>"},{"location":"Basic/system-information/#lspci-linux-system_profiler-macos-pci-devices","title":"<code>lspci</code> (Linux) / <code>system_profiler</code> (macOS) - PCI Devices","text":"<p>List all PCI devices: <pre><code>lspci                           # Linux\nsystem_profiler SPPciDataType   # macOS\n</code></pre></p> <p>Verbose output: <pre><code>lspci -v\n</code></pre></p> <p>Tree format: <pre><code>lspci -t\n</code></pre></p>"},{"location":"Basic/system-information/#lsusb-linux-system_profiler-macos-usb-devices","title":"<code>lsusb</code> (Linux) / <code>system_profiler</code> (macOS) - USB Devices","text":"<p>List USB devices: <pre><code>lsusb                           # Linux\nsystem_profiler SPUSBDataType   # macOS\nsystem_profiler SPUSBDataType | grep -E 'Product ID|Vendor ID|Speed|Location ID|Serial Number|Manufacturer'\n</code></pre></p> <p>Verbose output: <pre><code>lsusb -v\n</code></pre></p>"},{"location":"Basic/system-information/#lsblk-linux-diskutil-macos-block-devices","title":"<code>lsblk</code> (Linux) / <code>diskutil</code> (macOS) - Block Devices","text":"<p>List block devices: <pre><code>lsblk               # Linux\ndiskutil list       # macOS\n</code></pre></p> <p>Example output (Linux): <pre><code>NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda      8:0    0   500G  0 disk \n\u251c\u2500sda1   8:1    0   512M  0 part /boot/efi\n\u251c\u2500sda2   8:2    0     1G  0 part /boot\n\u2514\u2500sda3   8:3    0 498.5G  0 part /\n</code></pre></p> <p>Example output (macOS): <pre><code>/dev/disk0 (internal, physical):\n   #:                       TYPE NAME                    SIZE       IDENTIFIER\n   0:      GUID_partition_scheme                        *1.0 TB     disk0\n   1:             Apple_APFS_ISC Container disk1         524.3 MB   disk0s1\n   2:                 Apple_APFS Container disk3        2994.7 GB   disk0s2\n   3:        Apple_APFS_Recovery Container disk2         5.4 GB     disk0s3\n\n/dev/disk3 (synthesized):\n   #:                       TYPE NAME                    SIZE       IDENTIFIER\n   0:      APFS Container Scheme -                      +994.7 GB   disk3\n                                 Physical Store disk0s2\n   1:                APFS Volume Macintosh HD            11.3 GB    disk3s1\n   2:              APFS Snapshot com.apple.os.update-... 11.3 GB    disk3s1s1\n   3:                APFS Volume Preboot                 7.1 GB     disk3s2\n   4:                APFS Volume Recovery                1.0 GB     disk3s3\n   5:                APFS Volume Data                   2901.6 GB   disk3s5\n   6:                APFS Volume VM                      2.1 GB     disk3s6\n</code></pre></p> <p>Show filesystem information: <pre><code>lsblk -f\n</code></pre></p>"},{"location":"Basic/system-information/#memory-information","title":"Memory Information","text":""},{"location":"Basic/system-information/#free-linux-vm_stat-macos-memory-usage","title":"<code>free</code> (Linux) / <code>vm_stat</code> (macOS) - Memory Usage","text":"<p>Show memory usage: <pre><code>free        # Linux\nvm_stat     # macOS\n</code></pre></p> <p>Human-readable format: <pre><code>free -h\n</code></pre></p> <p>Example output: <pre><code>               total        used        free      shared  buff/cache   available\nMem:            7.7Gi       2.1Gi       3.2Gi       234Mi       2.4Gi       5.1Gi\nSwap:           2.0Gi          0B       2.0Gi\n</code></pre></p> <p>Show memory in different units: <pre><code>free -m    # Megabytes\nfree -g    # Gigabytes\nfree -k    # Kilobytes\n</code></pre></p>"},{"location":"Basic/system-information/#procmeminfo-detailed-memory-information","title":"<code>/proc/meminfo</code> - Detailed Memory Information","text":"<p>View detailed memory information: <pre><code>cat /proc/meminfo\n</code></pre></p> <p>Show specific memory information: <pre><code>grep MemTotal /proc/meminfo\ngrep MemAvailable /proc/meminfo\ngrep SwapTotal /proc/meminfo\n</code></pre></p>"},{"location":"Basic/system-information/#storage-information","title":"Storage Information","text":""},{"location":"Basic/system-information/#df-disk-space-usage","title":"<code>df</code> - Disk Space Usage","text":"<p>Show disk usage: <pre><code>df\n</code></pre></p> <p>Human-readable format: <pre><code>df -h\n</code></pre></p> <p>Example output: <pre><code>Filesystem      Size  Used Avail Use% Mounted on\n/dev/sda3       489G  156G  308G  34% /\n/dev/sda1       511M  6.1M  505M   2% /boot/efi\n/dev/sda2       974M  203M  704M  23% /boot\n</code></pre></p> <p>Show filesystem type: <pre><code>df -T\n</code></pre></p> <p>Show inodes usage: <pre><code>df -i\n</code></pre></p>"},{"location":"Basic/system-information/#du-directory-space-usage","title":"<code>du</code> - Directory Space Usage","text":"<p>Show directory sizes: <pre><code>du -h\n</code></pre></p> <p>Show summary of current directory: <pre><code>du -sh\n</code></pre></p> <p>Show sizes of subdirectories: <pre><code>du -h --max-depth=1\n</code></pre></p> <p>Show largest directories: <pre><code>du -h | sort -hr | head -10\n</code></pre></p>"},{"location":"Basic/system-information/#fdisk-disk-partitions","title":"<code>fdisk</code> - Disk Partitions","text":"<p>List disk partitions (requires root): <pre><code>sudo fdisk -l\n</code></pre></p> <p>Show partition table for specific disk: <pre><code>sudo fdisk -l /dev/sda\n</code></pre></p>"},{"location":"Basic/system-information/#process-information","title":"Process Information","text":""},{"location":"Basic/system-information/#ps-process-status","title":"<code>ps</code> - Process Status","text":"<p>Show all processes: <pre><code>ps aux\n</code></pre></p> <p>Show processes in tree format: <pre><code>ps auxf\n</code></pre></p> <p>Show specific user processes: <pre><code>ps -u username\n</code></pre></p> <p>Show processes with specific command: <pre><code>ps aux | grep firefox\n</code></pre></p>"},{"location":"Basic/system-information/#top-real-time-process-monitor","title":"<code>top</code> - Real-time Process Monitor","text":"<p>Interactive process monitor: <pre><code>top\n</code></pre></p> <p>Key shortcuts in top: - <code>q</code> - quit - <code>k</code> - kill process - <code>r</code> - renice process - <code>M</code> - sort by memory usage - <code>P</code> - sort by CPU usage - <code>h</code> - help  </p>"},{"location":"Basic/system-information/#htop-enhanced-process-monitor","title":"<code>htop</code> - Enhanced Process Monitor","text":"<p>Enhanced interactive process monitor: <pre><code>htop\n</code></pre></p> <p>Note: May need to install with <code>sudo apt install htop</code> (Linux) or <code>brew install htop</code> (macOS; need to install homebrew first)</p>"},{"location":"Basic/system-information/#pstree-process-tree","title":"<code>pstree</code> - Process Tree","text":"<p>Show process tree: <pre><code>pstree\n</code></pre></p> <p>Show with PIDs: <pre><code>pstree -p\n</code></pre></p> <p>Show specific user's processes: <pre><code>pstree username\n</code></pre></p>"},{"location":"Basic/system-information/#system-load-and-performance","title":"System Load and Performance","text":""},{"location":"Basic/system-information/#uptime-system-uptime-and-load","title":"<code>uptime</code> - System Uptime and Load","text":"<p>Show system uptime and load: <pre><code>uptime\n</code></pre></p> <p>Example output: <pre><code> 14:23:45 up 2 days,  3:45,  2 users,  load average: 0.15, 0.20, 0.18\n</code></pre></p>"},{"location":"Basic/system-information/#vmstat-linux-vm_stat-macos-virtual-memory-statistics","title":"<code>vmstat</code> (Linux) / <code>vm_stat</code> (macOS) - Virtual Memory Statistics","text":"<p>Show system statistics: <pre><code>vmstat\n</code></pre></p> <p>Continuous monitoring (every 2 seconds): <pre><code>vmstat 2\n</code></pre></p> <p>Show statistics 5 times with 2-second intervals: <pre><code>vmstat 2 5\n</code></pre></p>"},{"location":"Basic/system-information/#iostat-io-statistics","title":"<code>iostat</code> - I/O Statistics","text":"<p>Show I/O statistics: <pre><code>iostat\n</code></pre></p> <p>Continuous monitoring: <pre><code>iostat 2\n</code></pre></p> <p>Note: May need to install with <code>sudo apt install sysstat</code></p>"},{"location":"Basic/system-information/#sar-linux-system-activity-reporter","title":"<code>sar</code> (Linux) - System Activity Reporter","text":"<p>Show CPU usage: <pre><code>sar -u\n</code></pre></p> <p>Show memory usage: <pre><code>sar -r\n</code></pre></p> <p>Show I/O statistics: <pre><code>sar -b\n</code></pre></p>"},{"location":"Basic/system-information/#system-services-and-processes","title":"System Services and Processes","text":""},{"location":"Basic/system-information/#systemctl-linux-launchctl-macos-system-service-manager","title":"<code>systemctl</code> (Linux) / <code>launchctl</code> (macOS) - System Service Manager","text":"<p>On macOS, there is no direct equivalent to systemctl because :     - macOS uses a different init system, based on launchd.     - Services (daemons) are managed via launchctl, which is the command-line tool for interacting with launchd.  </p> <p>List all services: <pre><code>systemctl list-units --type=service\nlaunchctl list\n</code></pre></p> <p>Check service status: <pre><code>systemctl status nginx\nsystemctl status ssh\n</code></pre></p> <p>Show active services: <pre><code>systemctl list-units --type=service --state=active\n</code></pre></p> <p>Show failed services: <pre><code>systemctl list-units --type=service --state=failed\n</code></pre></p>"},{"location":"Basic/system-information/#jobs-background-jobs","title":"<code>jobs</code> - Background Jobs","text":"<p>Show current jobs: <pre><code>jobs\n</code></pre></p> <p>Show jobs with PIDs: <pre><code>jobs -l\n</code></pre></p>"},{"location":"Basic/system-information/#system-files-and-configuration","title":"System Files and Configuration","text":""},{"location":"Basic/system-information/#proc-linux-filesystem-system-information","title":"<code>/proc</code> (Linux) Filesystem - System Information","text":"<p>The <code>/proc</code> filesystem provides real-time system information:</p> <p>CPU information: <pre><code>cat /proc/cpuinfo\n</code></pre></p> <p>Memory information: <pre><code>cat /proc/meminfo\n</code></pre></p> <p>System version: <pre><code>cat /proc/version\n</code></pre></p> <p>Current processes: <pre><code>cat /proc/loadavg\n</code></pre></p> <p>Uptime: <pre><code>cat /proc/uptime\n</code></pre></p> <p>Mounted filesystems: <pre><code>cat /proc/mounts\n</code></pre></p>"},{"location":"Basic/system-information/#system-files","title":"System Files","text":"<p>Operating system information: <pre><code>cat /etc/os-release     # Linux\nsw_vers                 # macOS\n</code></pre></p> <p>Distribution information: <pre><code>cat /etc/issue          # Linux\nsw_vers -productName    # macOS\n</code></pre></p> <p>System timezone: <pre><code>cat /etc/timezone           # Linux\nsystemsetup -gettimezone    # macOS\n</code></pre></p> <p>DNS configuration: <pre><code>cat /etc/resolv.conf        # Linux\nscutil --dns                # macOS\n</code></pre></p> <p>Network interfaces: <pre><code>cat /etc/network/interfaces             # Linux\nnetworksetup -listallnetworkservices    # macOS    \n</code></pre></p>"},{"location":"Basic/system-information/#environment-and-shell-information","title":"Environment and Shell Information","text":""},{"location":"Basic/system-information/#env-environment-variables","title":"<code>env</code> - Environment Variables","text":"<p>Show all environment variables: <pre><code>env\n</code></pre></p> <p>Show specific variable: <pre><code>echo $PATH\necho $HOME\necho $USER\necho $SHELL\n</code></pre></p> <p>Show shell information: <pre><code>echo $0\necho $BASH_VERSION\n</code></pre></p>"},{"location":"Basic/system-information/#which-and-whereis-command-locations","title":"<code>which</code> and <code>whereis</code> - Command Locations","text":"<p>Find command location: <pre><code>which python3\nwhich gcc\nwhich ls\n</code></pre></p> <p>Find command and manual locations: <pre><code>whereis python3\nwhereis gcc\n</code></pre></p>"},{"location":"Basic/system-information/#history-command-history","title":"<code>history</code> - Command History","text":"<p>Show command history: <pre><code>history\n</code></pre></p> <p>Show last 10 commands: <pre><code>history 10\n</code></pre></p>"},{"location":"Basic/system-information/#system-monitoring-scripts","title":"System Monitoring Scripts","text":""},{"location":"Basic/system-information/#creating-a-system-info-script","title":"Creating a System Info Script","text":"<p>Here's a simple script to gather system information:</p> <pre><code>#!/bin/bash\n# system_info.sh - System Information Script\n\necho \"=== SYSTEM INFORMATION ===\"\necho \"Date: $(date)\"\necho \"Hostname: $(hostname)\"\necho \"User: $(whoami)\"\necho \"Uptime: $(uptime)\"\necho\n\necho \"=== HARDWARE INFORMATION ===\"\necho \"CPU: $(lscpu | grep 'Model name' | cut -d':' -f2 | xargs)\"\necho \"Memory: $(free -h | grep 'Mem:' | awk '{print $2}')\"\necho \"Disk: $(df -h / | tail -1 | awk '{print $2}')\"\necho\n\necho \"=== SYSTEM LOAD ===\"\necho \"CPU Usage: $(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | cut -d'%' -f1)\"\necho \"Memory Usage: $(free | grep Mem | awk '{printf(\"%.2f%%\"), $3/$2 * 100.0}')\"\necho \"Disk Usage: $(df -h / | tail -1 | awk '{print $5}')\"\necho\n\necho \"=== NETWORK INFORMATION ===\"\necho \"IP Address: $(hostname -I | awk '{print $1}')\"\necho \"Active Connections: $(netstat -an | grep ESTABLISHED | wc -l)\"\n</code></pre> <p>Save this as <code>system_info.sh</code>, make it executable, and run it:</p> <pre><code>chmod +x system_info.sh\n./system_info.sh\n</code></pre>"},{"location":"Basic/system-information/#practical-examples","title":"Practical Examples","text":""},{"location":"Basic/system-information/#example-1-check-system-resources-before-installing-software","title":"Example 1: Check System Resources Before Installing Software","text":"<pre><code># Check available memory\nfree -h\n\n# Check available disk space\ndf -h\n\n# Check CPU information\nlscpu | grep -E 'Model name|CPU\\(s\\)|Architecture'\n\n# Check if system is 64-bit\nuname -m\n</code></pre>"},{"location":"Basic/system-information/#example-2-monitor-system-performance","title":"Example 2: Monitor System Performance","text":"<pre><code># Check current system load\nuptime\n\n# Monitor processes using most CPU\nps aux --sort=-%cpu | head -10\n\n# Monitor processes using most memory\nps aux --sort=-%mem | head -10\n\n# Check I/O usage\niostat 1 3\n</code></pre>"},{"location":"Basic/system-information/#example-3-troubleshoot-high-resource-usage","title":"Example 3: Troubleshoot High Resource Usage","text":"<pre><code># Find processes using high CPU\ntop -o %CPU\n\n# Find processes using high memory\ntop -o %MEM\n\n# Check disk I/O\niotop  # if available\n\n# Check network connections\nnetstat -tuln\n</code></pre>"},{"location":"Basic/system-information/#example-4-system-health-check","title":"Example 4: System Health Check","text":"<pre><code># Check disk usage\ndf -h | grep -E '(8[0-9]|9[0-9])%'\n\n# Check memory usage\nfree -h | grep -E 'Mem:|Swap:'\n\n# Check system load\nuptime\n\n# Check failed services\nsystemctl --failed\n\n# Check system logs for errors\njournalctl -p err --since today\n</code></pre>"},{"location":"Basic/system-information/#common-system-information-one-liners","title":"Common System Information One-liners","text":"<p>Quick system overview: <pre><code>echo \"$(uname -n) running $(uname -s) $(uname -r) on $(uname -m)\"\n</code></pre></p> <p>Memory usage percentage: <pre><code>free | grep Mem | awk '{printf(\"Memory Usage: %.2f%%\\n\", $3/$2 * 100.0)}'\n</code></pre></p> <p>CPU usage: <pre><code>grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {print usage \"%\"}'\n</code></pre></p> <p>Disk usage of root partition: <pre><code>df -h / | tail -1 | awk '{print \"Root partition: \" $5 \" used of \" $2}'\n</code></pre></p> <p>Number of processes: <pre><code>ps aux | wc -l\n</code></pre></p> <p>System architecture: <pre><code>uname -m\n</code></pre></p>"},{"location":"Basic/system-information/#tips-for-beginners","title":"Tips for Beginners","text":"<ol> <li>Start with basic commands: Begin with <code>uname</code>, <code>whoami</code>, <code>free</code>, and <code>df</code></li> <li>Use help options: Most commands support <code>-h</code> or <code>--help</code> flags</li> <li>Read man pages: Use <code>man command</code> for detailed documentation</li> <li>Practice regularly: Set up a cron job to run system info commands daily</li> <li>Create aliases: Make shortcuts for frequently used command combinations</li> <li>Monitor trends: Run the same commands at different times to understand patterns</li> <li>Combine commands: Use pipes (<code>|</code>) to filter and format output</li> </ol>"},{"location":"Basic/system-information/#useful-aliases","title":"Useful Aliases","text":"<p>Add these to your <code>.bashrc</code> file:</p> <pre><code>alias sysinfo='uname -a &amp;&amp; free -h &amp;&amp; df -h'\nalias meminfo='free -h &amp;&amp; cat /proc/meminfo | head -20'\nalias cpuinfo='lscpu | grep -E \"Model name|CPU\\(s\\)|Architecture\"'\nalias diskinfo='df -h &amp;&amp; lsblk'\nalias netinfo='hostname -I &amp;&amp; netstat -tuln'\n</code></pre>"},{"location":"Basic/system-information/#security-considerations","title":"Security Considerations","text":"<ul> <li>Some commands require root privileges (use <code>sudo</code>)</li> <li>Be careful when running commands that can modify system settings</li> <li>Monitor system logs regularly for security issues</li> <li>Don't share detailed system information publicly</li> <li>Use system monitoring to detect unusual activity</li> </ul>"},{"location":"Basic/system-information/#conclusion","title":"Conclusion","text":"<p>Understanding system information is fundamental for effective system administration and troubleshooting. These commands provide comprehensive insights into your system's hardware, software, performance, and configuration. Practice these commands regularly to become proficient in system monitoring and maintenance.</p> <p>Remember that system information gathering is an ongoing process - the more you understand about your system's normal behavior, the better you'll be at identifying and resolving issues when they arise.</p>"},{"location":"Basic/text-processing/","title":"Text Processing in Bash","text":"<p>Text processing is one of the most powerful features of UNIX systems. Whether you're analyzing data files, processing log files, or manipulating configuration files, mastering text processing commands will dramatically improve your productivity. This guide covers essential tools from basic text manipulation to advanced pattern matching and data extraction.</p> Table of Content <ul> <li>Text Processing in Bash<ul> <li>Basic Text Processing Commands</li> <li>Pattern Matching and Searching</li> <li>Text Sorting and Uniqueness</li> <li>Field and Column Processing</li> <li>Introduction to sed - Stream Editor</li> <li>Introduction to awk - Pattern Scanning and Processing</li> <li>Advanced Text Processing Techniques</li> <li>Practical Examples and Use Cases</li> <li>Performance Tips and Best Practices</li> <li>Summary and Next Steps</li> </ul> </li> </ul>"},{"location":"Basic/text-processing/#basic-text-processing-commands","title":"Basic Text Processing Commands","text":""},{"location":"Basic/text-processing/#cat-concatenate-and-display-files","title":"<code>cat</code> - Concatenate and Display Files","text":"<p>The <code>cat</code> command is the foundation of text processing, allowing you to display and combine text files.</p> <p>Basic usage: <pre><code>cat file.txt                        # Display file contents\ncat file1.txt file2.txt             # Display multiple files\ncat file1.txt file2.txt &gt; combined.txt    # Combine files\n</code></pre></p> <p>Useful options: <pre><code>cat -n file.txt                     # Show line numbers\ncat -A file.txt                     # Show all characters (including hidden ones)\ncat -s file.txt                     # Squeeze multiple blank lines into one\n</code></pre></p> <p>Creating files with cat: <pre><code>cat &gt; new_file.txt                  # Create file and type content (Ctrl+D to finish)\ncat &gt;&gt; existing_file.txt            # Append to existing file\n</code></pre></p>"},{"location":"Basic/text-processing/#head-and-tail-display-file-portions","title":"<code>head</code> and <code>tail</code> - Display File Portions","text":"<p>These commands are essential for examining large files.</p> <p>head - First lines: <pre><code>head file.txt                       # First 10 lines (default)\nhead -n 20 file.txt                 # First 20 lines\nhead -n 5 *.txt                     # First 5 lines of all .txt files\n</code></pre></p> <p>tail - Last lines: <pre><code>tail file.txt                       # Last 10 lines (default)\ntail -n 15 file.txt                 # Last 15 lines\ntail -f logfile.txt                 # Follow file changes (useful for logs)\ntail -F logfile.txt                 # Follow file, handle rotation\n</code></pre></p> <p>Combining head and tail: <pre><code>head -n 50 file.txt | tail -n 10    # Lines 41-50 of file\n</code></pre></p>"},{"location":"Basic/text-processing/#wc-word-line-and-character-count","title":"<code>wc</code> - Word, Line, and Character Count","text":"<p>The <code>wc</code> command provides quick statistics about text files.</p> <pre><code>wc file.txt                         # Lines, words, characters\nwc -l file.txt                      # Count lines only\nwc -w file.txt                      # Count words only\nwc -c file.txt                      # Count characters only\nwc -m file.txt                      # Count characters (multibyte aware)\n</code></pre> <p>Practical examples: <pre><code>wc -l *.txt                         # Count lines in all text files\nls | wc -l                          # Count files in directory\ncat file.txt | wc -w                # Count words using pipes\n</code></pre></p>"},{"location":"Basic/text-processing/#pattern-matching-and-searching","title":"Pattern Matching and Searching","text":""},{"location":"Basic/text-processing/#grep-search-text-patterns","title":"<code>grep</code> - Search Text Patterns","text":"<p>The <code>grep</code> command searches for patterns in text files and is one of the most important text processing tools.</p> <p>Basic syntax: <pre><code>grep \"pattern\" file.txt\n</code></pre></p> <p>Common options: <pre><code>grep -i \"pattern\" file.txt          # Case-insensitive search\ngrep -n \"pattern\" file.txt          # Show line numbers\ngrep -v \"pattern\" file.txt          # Show lines that DON'T match\ngrep -r \"pattern\" directory/        # Recursive search in directory\ngrep -l \"pattern\" *.txt             # Show only filenames that match\ngrep -c \"pattern\" file.txt          # Count matching lines\n</code></pre></p> <p>Advanced pattern matching: <pre><code>grep \"^start\" file.txt              # Lines starting with \"start\"\ngrep \"end$\" file.txt                # Lines ending with \"end\"\ngrep \"^$\" file.txt                  # Empty lines\ngrep -E \"pattern1|pattern2\" file.txt # Multiple patterns (extended regex)\ngrep -w \"word\" file.txt             # Match whole words only\n</code></pre></p> <p>Practical examples: <pre><code># Find all Python files containing a specific function\ngrep -r \"def process_data\" --include=\"*.py\" .\n\n# Find error messages in log files\ngrep -i \"error\" /var/log/*.log\n\n# Find email addresses in text files\ngrep -E \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\" file.txt\n\n# Search for IP addresses\ngrep -E \"([0-9]{1,3}\\.){3}[0-9]{1,3}\" file.txt\n</code></pre></p>"},{"location":"Basic/text-processing/#grep-with-regular-expressions","title":"<code>grep</code> with Regular Expressions","text":"<p>Regular expressions (regex) make <code>grep</code> extremely powerful:</p> <p>Basic regex patterns: <pre><code>grep \"colou\\?r\" file.txt            # Match \"color\" or \"colour\"\ngrep \"test[123]\" file.txt           # Match \"test1\", \"test2\", or \"test3\"\ngrep \"file[0-9]\" file.txt           # Match \"file\" followed by any digit\ngrep \"^[A-Z]\" file.txt              # Lines starting with uppercase letter\ngrep \"[0-9]\\{3\\}\" file.txt          # Three consecutive digits\n</code></pre></p> <p>Extended regex with -E: <pre><code>grep -E \"test[0-9]+\" file.txt       # \"test\" followed by one or more digits\ngrep -E \"(jpg|png|gif)$\" file.txt   # Lines ending with image extensions\ngrep -E \"^[a-zA-Z]+:\" file.txt      # Lines starting with word followed by colon\n</code></pre></p>"},{"location":"Basic/text-processing/#text-sorting-and-uniqueness","title":"Text Sorting and Uniqueness","text":""},{"location":"Basic/text-processing/#sort-sort-lines-of-text","title":"<code>sort</code> - Sort Lines of Text","text":"<p>The <code>sort</code> command arranges lines in various orders.</p> <p>Basic sorting: <pre><code>sort file.txt                       # Sort alphabetically\nsort -r file.txt                    # Reverse sort\nsort -n file.txt                    # Numeric sort\nsort -u file.txt                    # Sort and remove duplicates\n</code></pre></p> <p>Advanced sorting: <pre><code>sort -k2 file.txt                   # Sort by second field\nsort -k2,2 file.txt                 # Sort by second field only\nsort -t: -k3 /etc/passwd            # Sort by third field, using : as delimiter\nsort -k1,1 -k2,2n file.txt          # Sort by first field, then by second numerically\n</code></pre></p> <p>Practical examples: <pre><code># Sort files by size\nls -la | sort -k5 -n                # Sort by file size (5th column)\n\n# Sort IP addresses correctly\nsort -t. -k1,1n -k2,2n -k3,3n -k4,4n ip_addresses.txt\n\n# Sort CSV file by specific column\nsort -t, -k3 data.csv               # Sort CSV by third column\n</code></pre></p>"},{"location":"Basic/text-processing/#uniq-remove-or-report-duplicate-lines","title":"<code>uniq</code> - Remove or Report Duplicate Lines","text":"<p>The <code>uniq</code> command works with sorted input to handle duplicates.</p> <p>Basic usage: <pre><code>sort file.txt | uniq                # Remove duplicate lines\nsort file.txt | uniq -c             # Count occurrences of each line\nsort file.txt | uniq -d             # Show only duplicate lines\nsort file.txt | uniq -u             # Show only unique lines\n</code></pre></p> <p>Practical examples: <pre><code># Find most common words in a file\ntr ' ' '\\n' &lt; file.txt | sort | uniq -c | sort -nr\n\n# Find duplicate IP addresses in log files\ngrep -E \"([0-9]{1,3}\\.){3}[0-9]{1,3}\" access.log | sort | uniq -d\n\n# Count unique visitors\ncut -d' ' -f1 access.log | sort | uniq | wc -l\n</code></pre></p>"},{"location":"Basic/text-processing/#field-and-column-processing","title":"Field and Column Processing","text":""},{"location":"Basic/text-processing/#cut-extract-columns-from-text","title":"<code>cut</code> - Extract Columns from Text","text":"<p>The <code>cut</code> command extracts specific columns or fields from text.</p> <p>Extract by character position: <pre><code>cut -c1-10 file.txt                 # Characters 1-10\ncut -c1,5,10 file.txt               # Characters 1, 5, and 10\ncut -c10- file.txt                  # From character 10 to end\n</code></pre></p> <p>Extract by field (delimiter-separated): <pre><code>cut -d: -f1 /etc/passwd             # First field, colon-separated\ncut -d, -f2,4 data.csv              # Fields 2 and 4, comma-separated\ncut -d$'\\t' -f1-3 file.txt          # First 3 fields, tab-separated\n</code></pre></p> <p>Practical examples: <pre><code># Extract usernames from /etc/passwd\ncut -d: -f1 /etc/passwd\n\n# Extract IP addresses from log files (assuming they're in first column)\ncut -d' ' -f1 access.log | sort | uniq\n\n# Extract specific columns from CSV\ncut -d, -f1,3,5 data.csv &gt; selected_columns.csv\n</code></pre></p>"},{"location":"Basic/text-processing/#tr-translate-characters","title":"<code>tr</code> - Translate Characters","text":"<p>The <code>tr</code> command transforms characters in text streams.</p> <p>Basic character translation: <pre><code>tr 'a' 'b' &lt; file.txt               # Replace 'a' with 'b'\ntr 'a-z' 'A-Z' &lt; file.txt           # Convert lowercase to uppercase\ntr 'A-Z' 'a-z' &lt; file.txt           # Convert uppercase to lowercase\n</code></pre></p> <p>Delete characters: <pre><code>tr -d '0-9' &lt; file.txt              # Delete all digits\ntr -d ' ' &lt; file.txt                # Delete all spaces\ntr -d '\\n' &lt; file.txt               # Delete newlines (join lines)\n</code></pre></p> <p>Squeeze characters: <pre><code>tr -s ' ' &lt; file.txt                # Squeeze multiple spaces into one\ntr -s '\\n' &lt; file.txt               # Remove empty lines\n</code></pre></p> <p>Practical examples: <pre><code># Clean up text files\ntr -d '\\r' &lt; dos_file.txt &gt; unix_file.txt    # Remove carriage returns\n\n# Convert spaces to underscores\ntr ' ' '_' &lt; file.txt\n\n# Extract words (replace non-letters with newlines)\ntr -cs 'A-Za-z' '\\n' &lt; file.txt\n</code></pre></p>"},{"location":"Basic/text-processing/#introduction-to-sed-stream-editor","title":"Introduction to <code>sed</code> - Stream Editor","text":"<p><code>sed</code> is a powerful stream editor that can perform complex text transformations. We'll start with simple examples and build up to more advanced usage.</p>"},{"location":"Basic/text-processing/#basic-sed-operations","title":"Basic <code>sed</code> Operations","text":"<p>Substitution (find and replace): <pre><code>sed 's/old/new/' file.txt           # Replace first occurrence per line\nsed 's/old/new/g' file.txt          # Replace all occurrences\nsed 's/old/new/2' file.txt          # Replace second occurrence per line\n</code></pre></p> <p>Case-insensitive substitution: <pre><code>sed 's/old/new/gi' file.txt         # Global, case-insensitive replacement\n</code></pre></p> <p>In-place editing: <pre><code>sed -i 's/old/new/g' file.txt       # Modify file directly\nsed -i.bak 's/old/new/g' file.txt   # Create backup before modifying\n</code></pre></p>"},{"location":"Basic/text-processing/#sed-line-operations","title":"<code>sed</code> Line Operations","text":"<p>Delete lines: <pre><code>sed '3d' file.txt                   # Delete line 3\nsed '2,5d' file.txt                 # Delete lines 2-5\nsed '/pattern/d' file.txt           # Delete lines containing pattern\nsed '/^$/d' file.txt                # Delete empty lines\n</code></pre></p> <p>Print specific lines: <pre><code>sed -n '5p' file.txt                # Print only line 5\nsed -n '10,20p' file.txt            # Print lines 10-20\nsed -n '/pattern/p' file.txt        # Print lines containing pattern\n</code></pre></p> <p>Add/insert lines: <pre><code>sed '3i\\New line' file.txt          # Insert line before line 3\nsed '3a\\New line' file.txt          # Add line after line 3\nsed '/pattern/a\\New line' file.txt  # Add line after pattern match\n</code></pre></p>"},{"location":"Basic/text-processing/#practical-sed-examples","title":"Practical <code>sed</code> Examples","text":"<p>Configuration file editing: <pre><code># Comment out lines containing a pattern\nsed 's/^/#/' config.txt             # Comment out all lines\nsed '/database/s/^/#/' config.txt   # Comment out lines with \"database\"\n\n# Uncomment lines\nsed 's/^#//' config.txt             # Remove # from beginning of lines\nsed '/server/s/^#//' config.txt     # Uncomment lines containing \"server\"\n</code></pre></p> <p>Log file processing: <pre><code># Extract specific parts of log entries\nsed 's/.*\\[\\([^]]*\\)\\].*/\\1/' access.log    # Extract timestamp from [timestamp]\n\n# Clean up log files\nsed 's/[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}/IP_ADDRESS/g' access.log\n</code></pre></p> <p>Data cleaning: <pre><code># Remove trailing whitespace\nsed 's/[[:space:]]*$//' file.txt\n\n# Convert multiple spaces to single space\nsed 's/  */ /g' file.txt\n\n# Remove Windows line endings\nsed 's/\\r$//' file.txt\n</code></pre></p>"},{"location":"Basic/text-processing/#introduction-to-awk-pattern-scanning-and-processing","title":"Introduction to <code>awk</code> - Pattern Scanning and Processing","text":"<p><code>awk</code> is a powerful programming language designed for text processing. It excels at processing structured text files, especially those with columns.</p>"},{"location":"Basic/text-processing/#basic-awk-concepts","title":"Basic <code>awk</code> Concepts","text":"<p>Structure of awk: <pre><code>awk 'pattern { action }' file.txt\n</code></pre></p> <p>Built-in variables: - <code>NR</code>: Number of records (lines) processed - <code>NF</code>: Number of fields in current record - <code>$0</code>: Entire current record - <code>$1, $2, ...</code>: Individual fields - <code>FS</code>: Field separator (default: space/tab)  </p>"},{"location":"Basic/text-processing/#simple-awk-examples","title":"Simple <code>awk</code> Examples","text":"<p>Print specific fields: <pre><code>awk '{print $1}' file.txt           # Print first field\nawk '{print $1, $3}' file.txt       # Print first and third fields\nawk '{print $NF}' file.txt          # Print last field\nawk '{print NR, $0}' file.txt       # Print line number and entire line\n</code></pre></p> <p>Field separators: <pre><code>awk -F: '{print $1}' /etc/passwd    # Use colon as field separator\nawk -F, '{print $2}' data.csv       # Use comma as field separator\nawk 'BEGIN{FS=\",\"} {print $1}' data.csv    # Set field separator in BEGIN block\n</code></pre></p> <p>Pattern matching: <pre><code>awk '/pattern/ {print}' file.txt    # Print lines containing pattern\nawk '$1 == \"value\" {print}' file.txt # Print lines where first field equals \"value\"\nawk '$3 &gt; 100 {print}' file.txt     # Print lines where third field &gt; 100\n</code></pre></p>"},{"location":"Basic/text-processing/#awk-calculations-and-processing","title":"<code>awk</code> Calculations and Processing","text":"<p>Mathematical operations: <pre><code>awk '{sum += $1} END {print sum}' file.txt       # Sum first column\nawk '{print $1 * $2}' file.txt                  # Multiply first two columns\nawk '{avg += $1; count++} END {print avg/count}' file.txt  # Average of first column\n</code></pre></p> <p>String operations: <pre><code>awk '{print length($0)}' file.txt               # Print length of each line\nawk '{print toupper($1)}' file.txt              # Convert first field to uppercase\nawk '{print substr($1, 1, 3)}' file.txt        # Print first 3 characters of first field\n</code></pre></p> <p>Conditional processing: <pre><code>awk '{if ($1 &gt; 50) print \"High:\", $0; else print \"Low:\", $0}' file.txt\nawk '$3 &gt; 100 {print $1, \"is high\"}' file.txt\nawk 'NR &gt; 1 {print}' file.txt                   # Skip header line\n</code></pre></p>"},{"location":"Basic/text-processing/#practical-awk-examples","title":"Practical <code>awk</code> Examples","text":"<p>CSV processing: <pre><code># Calculate total of a column in CSV\nawk -F, '{sum += $3} END {print \"Total:\", sum}' data.csv\n\n# Print rows where a condition is met\nawk -F, '$4 &gt; 1000 {print $1, $2}' sales.csv\n\n# Count records by category\nawk -F, '{count[$2]++} END {for (i in count) print i, count[i]}' data.csv\n</code></pre></p> <p>Log file analysis: <pre><code># Count HTTP status codes\nawk '{print $9}' access.log | sort | uniq -c\n\n# Calculate average response time (assuming time is in field 10)\nawk '{sum += $10; count++} END {print \"Average:\", sum/count}' access.log\n\n# Extract and count unique IP addresses\nawk '{print $1}' access.log | sort | uniq -c | sort -nr\n</code></pre></p> <p>System administration: <pre><code># Find processes using most CPU (from ps output)\nps aux | awk 'NR &gt; 1 {print $3, $11}' | sort -nr | head -10\n\n# Parse /etc/passwd for user information\nawk -F: '{print $1 \" has shell \" $7}' /etc/passwd\n\n# Monitor disk usage\ndf -h | awk 'NR &gt; 1 {print $5, $6}' | sort -nr\n</code></pre></p>"},{"location":"Basic/text-processing/#advanced-text-processing-techniques","title":"Advanced Text Processing Techniques","text":""},{"location":"Basic/text-processing/#combining-commands-with-pipes","title":"Combining Commands with Pipes","text":"<p>The real power of UNIX text processing comes from combining commands:</p> <p>Complex pipelines: <pre><code># Find most common words in a file\ncat file.txt | tr -s ' ' '\\n' | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -nr | head -10\n\n# Analyze web server logs\ncat access.log | awk '{print $1}' | sort | uniq -c | sort -nr | head -10\n\n# Process CSV data\ncat data.csv | sed '1d' | cut -d, -f3 | sort -n | tail -10\n</code></pre></p>"},{"location":"Basic/text-processing/#using-xargs-for-batch-processing","title":"Using <code>xargs</code> for Batch Processing","text":"<p><code>xargs</code> converts input into arguments for other commands:</p> <pre><code># Find and process multiple files\nfind . -name \"*.txt\" | xargs grep \"pattern\"\n\n# Delete files found by find\nfind . -name \"*.tmp\" -print0 | xargs -0 rm\n\n# Process files with custom commands\nls *.txt | xargs -I {} cp {} backup/{}\n</code></pre>"},{"location":"Basic/text-processing/#text-processing-with-paste-and-join","title":"Text Processing with <code>paste</code> and <code>join</code>","text":"<p>paste - Merge lines: <pre><code>paste file1.txt file2.txt           # Merge lines side by side\npaste -d, file1.txt file2.txt       # Use comma as delimiter\npaste -s file.txt                   # Merge all lines into one\n</code></pre></p> <p>join - Join files on common field: <pre><code>join -t, file1.csv file2.csv        # Join CSV files on first field\njoin -1 2 -2 1 file1.txt file2.txt  # Join using field 2 of file1 and field 1 of file2\n</code></pre></p>"},{"location":"Basic/text-processing/#practical-examples-and-use-cases","title":"Practical Examples and Use Cases","text":""},{"location":"Basic/text-processing/#example-1-processing-csv-data","title":"Example 1: Processing CSV Data","text":"<p>Sample CSV file (sales.csv): <pre><code>Date,Product,Quantity,Price\n2024-01-01,Widget A,10,25.50\n2024-01-02,Widget B,5,15.75\n2024-01-03,Widget A,8,25.50\n</code></pre></p> <p>Processing tasks: <pre><code># Remove header and calculate total revenue\ntail -n +2 sales.csv | awk -F, '{revenue += $3 * $4} END {print \"Total Revenue:\", revenue}'\n\n# Find best-selling product\ntail -n +2 sales.csv | awk -F, '{sales[$2] += $3} END {for (p in sales) print p, sales[p]}' | sort -k2 -nr | head -1\n\n# Convert to different format\nawk -F, 'NR &gt; 1 {printf \"%s: %d units of %s at $%.2f each\\n\", $1, $3, $2, $4}' sales.csv\n</code></pre></p>"},{"location":"Basic/text-processing/#example-2-log-file-analysis","title":"Example 2: Log File Analysis","text":"<p>Sample log entry: <pre><code>192.168.1.100 - - [01/Jan/2024:12:00:00 +0000] \"GET /index.html HTTP/1.1\" 200 1024\n</code></pre></p> <p>Analysis tasks: <pre><code># Extract IP addresses and count occurrences\nawk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10\n\n# Find 404 errors\nawk '$9 == 404 {print $1, $7}' access.log\n\n# Calculate total bytes transferred\nawk '{sum += $10} END {print \"Total bytes:\", sum}' access.log\n\n# Find peak traffic hours\nawk '{gsub(/\\[/, \"\", $4); gsub(/:.*/, \"\", $4); print $4}' access.log | sort | uniq -c | sort -nr\n</code></pre></p>"},{"location":"Basic/text-processing/#example-3-configuration-file-management","title":"Example 3: Configuration File Management","text":"<p>Process configuration files: <pre><code># Extract uncommented lines\nsed '/^#/d; /^$/d' config.conf\n\n# Add comments to specific lines\nsed '/database/s/^/# /' config.conf\n\n# Update configuration values\nsed 's/^port=.*/port=8080/' config.conf\n\n# Extract configuration values\nawk -F= '/^[^#]/ {print $1, $2}' config.conf\n</code></pre></p>"},{"location":"Basic/text-processing/#example-4-data-cleaning-and-transformation","title":"Example 4: Data Cleaning and Transformation","text":"<p>Clean messy data: <pre><code># Remove extra whitespace\nsed 's/^[[:space:]]*//; s/[[:space:]]*$//' file.txt\n\n# Standardize date format\nsed 's/\\([0-9]\\{1,2\\}\\)\\/\\([0-9]\\{1,2\\}\\)\\/\\([0-9]\\{4\\}\\)/\\3-\\2-\\1/g' file.txt\n\n# Convert phone numbers to standard format\nsed 's/(\\([0-9]\\{3\\}\\)) \\([0-9]\\{3\\}\\)-\\([0-9]\\{4\\}\\)/\\1-\\2-\\3/g' file.txt\n\n# Extract email addresses\ngrep -E -o \"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\" file.txt\n</code></pre></p>"},{"location":"Basic/text-processing/#performance-tips-and-best-practices","title":"Performance Tips and Best Practices","text":""},{"location":"Basic/text-processing/#efficient-text-processing","title":"Efficient Text Processing","text":"<p>Choose the right tool: - <code>grep</code>: Fast searching and filtering - <code>sed</code>: Simple substitutions and line operations - <code>awk</code>: Field processing and calculations - <code>cut</code>: Simple column extraction - <code>tr</code>: Character transformations</p> <p>Performance considerations: <pre><code># Use grep for simple searches (faster than sed/awk)\ngrep \"pattern\" file.txt\n\n# Use cut for simple column extraction\ncut -d, -f1 file.csv\n\n# Use awk for complex processing\nawk 'complex_logic' file.txt\n\n# Process large files efficiently\n# Instead of: cat huge_file.txt | grep pattern\n# Use: grep pattern huge_file.txt\n</code></pre></p>"},{"location":"Basic/text-processing/#memory-and-processing-tips","title":"Memory and Processing Tips","text":"<p>Handle large files: <pre><code># Process files in chunks\nsplit -l 10000 large_file.txt chunk_\n\n# Use streaming where possible\ntail -f logfile.txt | grep \"ERROR\"\n\n# Avoid loading entire file into memory\n# Good: grep pattern file.txt\n# Avoid: cat file.txt | grep pattern (for large files)\n</code></pre></p>"},{"location":"Basic/text-processing/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":"<p>Quoting and escaping: <pre><code># Problem: Special characters in patterns\ngrep '$100' file.txt              # Wrong: $ has special meaning\ngrep '\\$100' file.txt             # Correct: Escape the $\n\n# Problem: Spaces in filenames\ngrep pattern file name.txt        # Wrong: Treated as two files\ngrep pattern \"file name.txt\"      # Correct: Quote the filename\n</code></pre></p> <p>Field separators: <pre><code># Problem: Inconsistent whitespace\nawk '{print $2}' file.txt         # May not work with multiple spaces\nawk '{print $2}' file.txt         # awk handles multiple spaces correctly\n\n# Problem: Wrong delimiter\ncut -d' ' -f2 file.csv            # Wrong: CSV uses commas\ncut -d, -f2 file.csv              # Correct: Use comma delimiter\n</code></pre></p>"},{"location":"Basic/text-processing/#summary-and-next-steps","title":"Summary and Next Steps","text":"<p>Text processing is a fundamental skill in UNIX environments. Key takeaways:</p> <p>Essential commands to master: - <code>grep</code> for searching and filtering - <code>sed</code> for substitutions and line operations - <code>awk</code> for field processing and calculations - <code>cut</code>, <code>sort</code>, <code>uniq</code> for data manipulation - <code>tr</code> for character transformations  </p> <p>Best practices: - Start with simple commands and build complexity gradually - Use pipes to combine commands for powerful workflows - Choose the right tool for each task - Test commands on small files before processing large datasets - Always backup important files before in-place editing  </p> <p>Continue learning: - Practice with real data files - Learn more advanced regex patterns - Explore shell scripting to automate text processing tasks - Study more advanced <code>awk</code> programming features - Learn about other text processing tools like <code>perl</code> and <code>python</code> </p> <p>The combination of these tools provides incredibly powerful text processing capabilities. With practice, you'll be able to handle complex data manipulation tasks efficiently and elegantly.</p>"},{"location":"Fastq/BWA/","title":"Mapping Reads to a Reference Genome with BWA: Burrows-Wheeler Aligner","text":"Table of Content <ul> <li>Mapping Reads to a Reference Genome with BWA: Burrows-Wheeler Aligner<ul> <li>Overview</li> <li>Scientific Principles</li> <li>Typical Usage</li> </ul> </li> </ul>"},{"location":"Fastq/BWA/#overview","title":"Overview","text":"<p>BWA (Burrows-Wheeler Aligner) is a fast and memory-efficient software package for aligning relatively short nucleotide sequences (reads) against a large reference genome, such as the human genome. It is widely used in bioinformatics pipelines for processing next-generation sequencing (NGS) data.</p> <p>BWA supports three primary algorithms:</p> <ul> <li>BWA-backtrack: Optimized for short reads (\u2264100 bp), typically from early Illumina platforms.</li> <li>BWA-SW: Suitable for longer reads with higher error rates.</li> <li>BWA-MEM: The most commonly used algorithm, designed for reads \u226570 bp. It is accurate, fast, and supports gapped alignment and split-read mapping.</li> </ul>"},{"location":"Fastq/BWA/#scientific-principles","title":"Scientific Principles","text":"<p>BWA is based on the Burrows-Wheeler Transform (BWT) and the FM-index, which enable efficient substring matching in large texts. The BWT rearranges a string into runs of similar characters, facilitating compression and fast search. The FM-index, a compressed suffix array, allows backward searching of patterns in logarithmic time.</p> <p>The alignment process involves:</p> <ol> <li>Indexing the reference genome using BWT and FM-index.</li> <li>Seeding: Finding exact or near-exact matches of substrings (seeds) from the query.</li> <li>Extension: Extending seeds into full alignments using dynamic programming.</li> <li>Scoring: Assigning alignment scores based on match/mismatch, gap penalties, and quality scores.</li> </ol>"},{"location":"Fastq/BWA/#typical-usage","title":"Typical Usage","text":""},{"location":"Fastq/BWA/#indexing-the-reference-genome","title":"Indexing the Reference Genome","text":"<p>Before alignment, the reference genome must be indexed:</p> <pre><code>bwa index reference.fasta\n</code></pre> <p>This generates several auxiliary files (<code>.amb</code>, <code>.ann</code>, <code>.bwt</code>, <code>.pac</code>, <code>.sa</code>) used during alignment.</p>"},{"location":"Fastq/BWA/#preparing-a-list-of-fastq-files-to-be-treated","title":"Preparing a List of Fastq Files To Be Treated","text":"<p>generate_fastq_files_list.sh<pre><code>#!/bin/bash\n\nls -1 raw | grep -E '_1\\.fastq\\.gz' | sed 's/_\\(1\\)\\.fastq\\.gz//g' &gt; info_files/fastq_files.list\n</code></pre> This script scans the raw/ directory for paired-end FASTQ files with names ending in _1.fastq.gz. It extracts the sample names (removing the _1.fastq.gz suffix) and writes one sample name per line to the file info_files/fastq_files.list.</p>"},{"location":"Fastq/BWA/#mapping-reads-on-a-reference-genome-with-bwa-mem-on-a-hpc-cluster","title":"Mapping Reads On a Reference Genome with BWA-MEM on a HPC Cluster","text":""},{"location":"Fastq/BWA/#samples-in-a-unique-pair-of-fastq-files","title":"Samples in a unique pair of Fastq files","text":"<p>Here is a script used for the mapping of paired-end Fastq files from the mosquito Anopheles gambiae. Each sample is in a unique pair of Fastq files (<code>&lt;samplename&gt;_1|R1.fastq.gz; &lt;samplename&gt;_2|R2.fastq.gz</code>):</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --job-name=bwa\n#SBATCH --time=04:00:00\n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 16\n#SBATCH --mem=16G\n#SBATCH --array 1-8\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\n# Recover of fastq file name\nSAMPLENAME=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/fastq_names)\necho ${SAMPLENAME}\n\n# Create ReadGroup for bwa\nREADGROUP=\"@RG\\tID:${SAMPLENAME}\\tSM:${SAMPLENAME}\\tCN:SC\\tPL:ILLUMINA\"\necho ${READGROUP}\n\nREFPATH=\"resources/genomes\"\nTRIMMINGPATH=\"results/01_Trimming\"\nOUTPUTPATH=\"results/02_Mapping\"\nmkdir -p ${OUTPUTPATH}\nREPORTPATH=\"results/11_Reports/bwa\"\nmkdir -p ${REPORTPATH}\n\n\nmodule load bwa/0.7.17\n\n# bwa OPTIONS:\n# -t &lt;INT&gt;: threads\n# -M: Mark shorter split hits as secondary (for Picard compatibility)\n# -T &lt;INT&gt;: Don\u2019t output alignment with score lower than INT. This option only affects output. [default: 30]\n# -v &lt;INT&gt;: Control the verbose level of the stderr: 0 - disable all, 1 - output errors only\n#           2 - output warning and errors, 3 - all normal messages.\n\nbwa mem -M -T 0 -t 16 \\\n  -R ${READGROUP} \\\n  ${REFPATH}/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa\\\n  ${TRIMMINGPATH}/${SAMPLENAME}_trimmomatic_R1.fastq.gz\\\n  ${TRIMMINGPATH}/${SAMPLENAME}_trimmomatic_R2.fastq.gz &gt; ${OUTPUTPATH}/${SAMPLENAME}.sam \\\n  2&gt; ${REPORTPATH}/${SAMPLENAME}-bwa.err\n\nend_time=$(date +%s)  # Capture l'heure de fin\nduration=$((end_time - start_time))  # Calcule la dur\u00e9e\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre> <p>This SLURM batch script is designed to perform parallelized read alignment using BWA-MEM on a high-performance computing (HPC) cluster. It leverages SLURM's job array functionality to process multiple samples in parallel, each defined by a line in a file named fastq_names.</p>"},{"location":"Fastq/BWA/#slurm-configuration","title":"SLURM Configuration","text":"<p><pre><code>#SBATCH --array 1-8\n</code></pre> This line tells SLURM to run the script as an array job with 8 tasks. Each task will process a different sample, determined by its line number in the info_files/fastq_names file.</p> <p>Other key SLURM directives: <pre><code>--cpus-per-task=16: Allocates 16 CPU threads per task.\n--mem=16G: Allocates 16 GB of RAM.\n--time=04:00:00: Sets a 4-hour time limit.\n--mail-type=FAIL: Sends an email if the job fails.\n</code></pre></p>"},{"location":"Fastq/BWA/#sample-name-and-read-group","title":"Sample Name and Read Group","text":"<p>The script extracts the sample name corresponding to the current array task: <pre><code>SAMPLENAME=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/fastq_names)\n</code></pre> It then constructs a Read Group (RG) string, which is essential for downstream tools like GATK: <pre><code>READGROUP=\"@RG\\\\tID:${SAMPLENAME}\\\\tSM:${SAMPLENAME}\\\\tCN:SC\\\\tPL:ILLUMINA\"\n</code></pre></p>"},{"location":"Fastq/BWA/#bwa-mem-alignment","title":"BWA-MEM Alignment","text":"<p>The core of the script is the bwa mem command: <pre><code>bwa mem -M -T 0 -t 16 \\\\\n  -R ${READGROUP} \\\\\n  ${REFPATH}/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa \\\\\n  ${TRIMMINGPATH}/${SAMPLENAME}_trimmomatic_R1.fastq.gz \\\\\n  ${TRIMMINGPATH}/${SAMPLENAME}_trimmomatic_R2.fastq.gz \\\\\n  &gt; ${OUTPUTPATH}/${SAMPLENAME}.sam \\\\\n  2&gt; ${REPORTPATH}/${SAMPLENAME}-bwa.err\n</code></pre></p>"},{"location":"Fastq/BWA/#key-options-explained","title":"Key Options Explained","text":"<ul> <li>-M: Marks shorter split hits as secondary. This is important for compatibility with Picard tools, which expect this flag for proper duplicate marking.  </li> <li>-T 0: Disables the minimum alignment score threshold. By default, BWA filters out alignments with a score below 30. Setting -T 0 ensures that all alignments, even low-quality ones, are retained in the output SAM file. Reads with low quality will be marked as \"MQ0\" by samtools (see figure below). This is particularly useful when using samtools stats after duplicate removal, as it allows reads with mapping quality 0 (MQ0) to be included in the statistics. These reads are often excluded by default but can be informative for assessing alignment quality and coverage.  </li> <li>-t 16: Uses 16 threads to speed up the alignment process.  </li> </ul> <p> Example of MQ0 reads (in orange) in samtools stats when -T option is set to 0</p>"},{"location":"Fastq/BWA/#samples-in-multiple-lanes-multiple-fastq-files","title":"Samples in multiple lanes = multiple Fastq files","text":"<p>Sometimes, a single biological sample is sequenced across multiple lanes of a Illumina flowcell. This is done for a few reasons: - To increase sequencing depth (more data = better coverage) - To balance the load across lanes - To reduce technical biases from a single lane  </p> <p>As a result, you may receive multiple FASTQ files for the same sample, one from each lane. These files usually have lane identifiers in their filenames (e.g., Sample1_<code>L001</code>R1.fastq.gz, Sample1<code>L002</code>_R1.fastq.gz, etc.).</p> <p>It is important to preserve the lane information when mapping reads.</p> <p>Each group of reads from a lane must be mapped separately using a specific Read Group (RG) tag in the aligner (e.g., with @RG in bwa mem), which includes the lane number. This allows downstream tools to track the origin of each read and correct for lane-specific technical variations.</p> <p>In this case,the content of the info_files/fastq_files.list will be:</p> <p>info_files/fastq_files.list<pre><code>Sample_1_EKDN230051723-1A_H2F5FDSXC_L3\nSample_1_EKDN230051723-1A_H2WC5DSXC_L2\nSample_2_EKDN230051724-1A_HFTFWDSX7_L4\nSample_3_EKDN230051725-1A_H2WC5DSXC_L2\nSample_3_EKDN230051725-1A_HFTFWDSX7_L4\n</code></pre> Sample_1 has two lanes (L2/L3), Sample_3 has two lanes (L2/L4) ans Sample_2 has only one lane.</p> <p>Here is a script used for the mapping of paired-end Fastq files from the mosquito Anopheles gambiae, in multiple pair (or not) of Fastq files:</p> <p>BWA mapping script - multiple lanes case<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=bwa\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 16\n#SBATCH --mem=32G\n#SBATCH --array 11-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Recover of fastq file name\nSAMPLELANE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/fastq_files)\necho ${SAMPLELANE}\n\n# Extract Sample name\nSAMPLE=$(echo ${SAMPLELANE} | awk -F'[_]' '{print $1\"_\"$2}')\necho ${SAMPLE}\n\n# Extract Lane number\nLANE=$(echo ${SAMPLELANE} | awk -F'[_]' '{print $5}')\necho ${LANE}\n\n# Create ReadGroup for bwa\nREADGROUP=\"@RG\\tID:${LANE}\\tSM:${SAMPLE}\\tCN:SC\\tPL:ILLUMINA\"\necho ${READGROUP}\n\nmodule load bwa/0.7.17 samtools/1.15.1\n\n# bwa OPTIONS:\n# -t &lt;INT&gt;: threads\n# -M: Mark shorter split hits as secondary (for Picard compatibility)\n# -T &lt;INT&gt;: Don\u2019t output alignment with score lower than INT. This option only affects output. [default: 30]\n# -v &lt;INT&gt;: Control the verbose level of the stderr: 0 - disable all, 1 - output errors only\n#           2 - output warning and errors, 3 - all normal messages.\n\nbwa mem -M -t 16 \\\n  -R ${READGROUP} \\\n  resources/genomes/GCF_035046485.1_AalbF5_genomic.fna \\\n  results/01_Trimming/${SAMPLELANE}_trimmomatic_R1.fastq.gz \\\n  results/01_Trimming/${SAMPLELANE}_trimmomatic_R2.fastq.gz &gt; \\\n  results/02_Mapping/${SAMPLELANE}.sam \\\n  2&gt; results/11_Reports/bwa/${SAMPLELANE}-bwa.err\n</code></pre> </p>"},{"location":"Fastq/BWA/#the-pattern","title":"The pattern","text":"<p>The FASTQ filenames in info_files/fastq_files list follow this pattern: <pre><code>Sample_1_EKDN230051723-1A_H2F5FDSXC_L3\n</code></pre> Structure (underscore-delimited): <pre><code>$1        $2       $3              $4        $5\nSample    1   EKDN230051723-1A   H2F5FDSXC   L3\n</code></pre> </p> <ol> <li>Extracting the Sample Name</li> </ol> <pre><code>SAMPLE=$(echo ${SAMPLELANE} | awk -F'[_]' '{print $1\"_\"$2}')\n</code></pre> <ul> <li><code>-F'[_]'</code>: Sets the field separator to _ (underscore).  </li> <li><code>{print $1\"_\"$2}</code>: Concatenates the first and second fields (e.g., Sample_1). Purpose: This extracts just the biological sample name, discarding sequencing run or lane info.</li> </ul> <p> 2. Extracting the Lane Number <pre><code>LANE=$(echo ${SAMPLELANE} | awk -F'[_]' '{print $5}')\n</code></pre> Uses the same field splitting. - <code>{print $5}</code>: Returns the fifth field, which is the lane ID, e.g., L3. Purpose: Extracts the lane identifier to uniquely label the read group by sequencing origin.</p> <p>These extracted values are used to construct the @RG (Read Group) line passed to BWA: <pre><code>READGROUP=\"@RG\\tID:${LANE}\\tSM:${SAMPLE}\\tCN:SC\\tPL:ILLUMINA\"\n</code></pre></p> <ul> <li><code>ID</code>: the lane (e.g., L3)  </li> <li><code>SM</code>: the sample (e.g., Sample_1)  </li> <li><code>CN</code>: center name (hardcoded here as SC)  </li> <li><code>PL</code>: platform (ILLUMINA)  </li> </ul> <p>This enables tools like <code>GATK</code>, used in downstream analysis, to distinguish reads by lane and sample, correct for batch effects, and handle duplicates correctly. </p> <p>Only after alignment, once the Read Groups are set, the resulting BAM files can be merged into a single file. This ensures that each read retains its correct sequencing context. See The Bam File section for more informations.</p> <p>So, although the sample appears split at the FASTQ level, it is best to map each lane separately and merge the BAMs later \u2014 not the FASTQs \u2014 to preserve data integrity and support proper downstream processing like duplicate marking or base recalibration.</p>"},{"location":"Fastq/FastQC/","title":"Fastq Quality Control With FastQC","text":"Table of Content <ul> <li>Fastq Quality Control With FastQC<ul> <li>Overview</li> <li>Key Features</li> <li>Quality Control Modules</li> <li>Command Line Usage</li> <li>Output Files</li> <li>Interpretation Guidelines</li> <li>Integration with Workflows</li> <li>Performance Considerations</li> <li>Best Practices</li> <li>Limitations</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Fastq/FastQC/#overview","title":"Overview","text":"<p>FastQC is a widely-used bioinformatics tool developed by the Babraham Institute for quality control analysis of high-throughput sequencing data. It provides comprehensive statistical assessments and visual representations of sequence quality metrics, enabling researchers to identify potential issues in their sequencing datasets before downstream analysis.</p>"},{"location":"Fastq/FastQC/#key-features","title":"Key Features","text":""},{"location":"Fastq/FastQC/#platform-compatibility","title":"Platform Compatibility","text":"<ul> <li>Cross-platform: Available for Linux, macOS, and Windows</li> <li>Multiple interfaces: Command-line and graphical user interface (GUI)</li> <li>Batch processing: Simultaneous analysis of multiple files</li> <li>Format support: FASTQ, SAM, BAM, and compressed formats</li> </ul>"},{"location":"Fastq/FastQC/#analysis-modules","title":"Analysis Modules","text":"<p>FastQC performs twelve distinct quality control analyses, each generating specific metrics and visualizations.</p>"},{"location":"Fastq/FastQC/#quality-control-modules","title":"Quality Control Modules","text":""},{"location":"Fastq/FastQC/#1-basic-statistics","title":"1. Basic Statistics","text":"<p>Provides fundamental dataset characteristics:  - Total number of sequences - Sequence length distribution - GC content percentage - Sequence encoding format detection  </p>"},{"location":"Fastq/FastQC/#2-per-base-sequence-quality","title":"2. Per Base Sequence Quality","text":"<p>Evaluates Phred quality scores across all base positions: - Pass: Median quality \u226525 for all positions - Warning: Lower quartile quality &lt;20 or median &lt;25 for any position - Fail: Lower quartile quality &lt;10 or median &lt;20 for any position  </p>"},{"location":"Fastq/FastQC/#3-per-tile-sequence-quality","title":"3. Per Tile Sequence Quality","text":"<p>Assesses quality variation across flow cell tiles (Illumina data): - Identifies problematic tiles or regions - Detects systematic quality degradation patterns - Useful for troubleshooting sequencing run issues  </p>"},{"location":"Fastq/FastQC/#4-per-sequence-quality-scores","title":"4. Per Sequence Quality Scores","text":"<p>Analyzes the distribution of mean quality scores per read: - Pass: Peak at quality score &gt;27 - Warning: Peak between 20-27 - Fail: Peak &lt;20  </p>"},{"location":"Fastq/FastQC/#5-per-base-sequence-content","title":"5. Per Base Sequence Content","text":"<p>Examines nucleotide composition across read positions:  - Pass: Difference between A/T and G/C &lt;10% - Warning: Difference between A/T and G/C 10-20% - Fail: Difference between A/T and G/C &gt;20%  </p>"},{"location":"Fastq/FastQC/#6-per-sequence-gc-content","title":"6. Per Sequence GC Content","text":"<p>Compares observed GC content distribution to theoretical normal distribution: - Pass: Distribution matches expected normal curve - Warning: Sum of deviations &gt;15% of reads - Fail: Sum of deviations &gt;20% of reads  </p>"},{"location":"Fastq/FastQC/#7-per-base-n-content","title":"7. Per Base N Content","text":"<p>Quantifies the proportion of uncalled bases (N) at each position: - Pass: N content &lt;5% at all positions - Warning: N content 5-20% at any position - Fail: N content &gt;20% at any position  </p>"},{"location":"Fastq/FastQC/#8-sequence-length-distribution","title":"8. Sequence Length Distribution","text":"<p>Analyzes the distribution of sequence lengths: - Pass: All sequences same length - Warning: Variable lengths present - Fail: Sequences with zero length detected  </p>"},{"location":"Fastq/FastQC/#9-sequence-duplication-levels","title":"9. Sequence Duplication Levels","text":"<p>Identifies potential PCR amplification artifacts: - Pass: Non-unique sequences &lt;20% - Warning: Non-unique sequences 20-50% - Fail: Non-unique sequences &gt;50%  </p>"},{"location":"Fastq/FastQC/#10-overrepresented-sequences","title":"10. Overrepresented Sequences","text":"<p>Detects sequences comprising &gt;0.1% of the total: - Identifies potential contamination - Detects adapter sequences - Highlights PCR artifacts  </p>"},{"location":"Fastq/FastQC/#11-adapter-content","title":"11. Adapter Content","text":"<p>Searches for common sequencing adapters: - Illumina Universal Adapter - Illumina Small RNA Adapter - Nextera Transposase Sequence - SOLID Small RNA Adapter  </p>"},{"location":"Fastq/FastQC/#12-kmer-content","title":"12. Kmer Content","text":"<p>Identifies enriched kmers that may indicate bias: - Pass: No kmers with positional bias - Warning: Kmers with positional bias &lt;1% - Fail: Kmers with positional bias &gt;1%  </p>"},{"location":"Fastq/FastQC/#command-line-usage","title":"Command Line Usage","text":""},{"location":"Fastq/FastQC/#basic-syntax","title":"Basic Syntax","text":"<pre><code>fastqc [options] seqfile1 seqfile2 .. seqfileN \n</code></pre>"},{"location":"Fastq/FastQC/#common-parameters","title":"Common Parameters","text":"<pre><code># Basic analysis\nfastqc sample.fastq\n\n# Specify output directory\nfastqc -o /path/to/output sample.fastq\n\n# Batch processing\nfastqc *.fastq\n\n# Specify number of threads\nfastqc -t 4 sample.fastq\n\n# Quiet mode (suppress progress output)\nfastqc -q sample.fastq\n\n# Extract results to directory\nfastqc --extract sample.fastq\n</code></pre>"},{"location":"Fastq/FastQC/#advanced-options","title":"Advanced Options","text":"<pre><code># Custom adapter sequences\nfastqc -a adapter_file.txt sample.fastq\n\n# Specify sequence format\nfastqc -f fastq sample.fastq\n\n# Set memory limit (Java heap size)\nfastqc -Xmx2g sample.fastq\n\n# Generate report without images\nfastqc --nogroup sample.fastq\n</code></pre>"},{"location":"Fastq/FastQC/#output-files","title":"Output Files","text":""},{"location":"Fastq/FastQC/#html-report","title":"HTML Report","text":"<ul> <li>Interactive web-based report</li> <li>Graphical visualizations</li> <li>Pass/Warning/Fail status indicators</li> <li>Detailed explanations of each module</li> </ul>"},{"location":"Fastq/FastQC/#text-summary","title":"Text Summary","text":"<ul> <li>Tab-delimited summary statistics</li> <li>Machine-readable format</li> <li>Suitable for automated parsing</li> <li>Integration with pipeline workflows</li> </ul>"},{"location":"Fastq/FastQC/#data-files","title":"Data Files","text":"<ul> <li>Raw data underlying each analysis</li> <li>Extracted when using <code>--extract</code> option</li> <li>Enables custom downstream analysis</li> </ul>"},{"location":"Fastq/FastQC/#interpretation-guidelines","title":"Interpretation Guidelines","text":""},{"location":"Fastq/FastQC/#quality-thresholds","title":"Quality Thresholds","text":"Metric Good Acceptable Poor Per base quality &gt;30 20-30 &lt;20 Per sequence quality Peak &gt;27 Peak 20-27 Peak &lt;20 GC content Expected \u00b15% Expected \u00b110% Expected &gt;\u00b110% Duplication level &lt;20% 20-50% &gt;50%"},{"location":"Fastq/FastQC/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"Fastq/FastQC/#low-quality-scores","title":"Low Quality Scores","text":"<ul> <li>Causes: Sequencing chemistry degradation, over-clustering</li> <li>Solutions: Quality trimming, read filtering</li> </ul>"},{"location":"Fastq/FastQC/#high-duplication-levels","title":"High Duplication Levels","text":"<ul> <li>Causes: PCR amplification bias, low library complexity</li> <li>Solutions: Duplicate removal, library optimization</li> </ul>"},{"location":"Fastq/FastQC/#adapter-contamination","title":"Adapter Contamination","text":"<ul> <li>Causes: Incomplete adapter removal, insert size issues</li> <li>Solutions: Adapter trimming, library size selection</li> </ul>"},{"location":"Fastq/FastQC/#unusual-gc-content","title":"Unusual GC Content","text":"<ul> <li>Causes: Contamination, amplification bias, sample composition</li> <li>Solutions: Contamination screening, bias correction</li> </ul>"},{"location":"Fastq/FastQC/#integration-with-workflows","title":"Integration with Workflows","text":""},{"location":"Fastq/FastQC/#preprocessing-pipelines","title":"Preprocessing Pipelines","text":"<p>FastQC is typically integrated at multiple stages: 1. Raw data assessment: Initial quality evaluation 2. Post-trimming validation: Verify improvement after processing 3. Final validation: Confirm data suitability for analysis  </p>"},{"location":"Fastq/FastQC/#automation-tools","title":"Automation Tools","text":"<ul> <li>MultiQC: Aggregate FastQC reports across samples</li> <li>Nextflow: Workflow management integration</li> <li>Snakemake: Rule-based workflow incorporation</li> <li>Galaxy: Web-based platform integration</li> </ul>"},{"location":"Fastq/FastQC/#performance-considerations","title":"Performance Considerations","text":""},{"location":"Fastq/FastQC/#memory-requirements","title":"Memory Requirements","text":"<ul> <li>Typical usage: 1-2 GB RAM per file</li> <li>Large files: Scale memory allocation accordingly</li> <li>Java heap size: Adjust with <code>-Xmx</code> parameter</li> </ul>"},{"location":"Fastq/FastQC/#processing-speed","title":"Processing Speed","text":"<ul> <li>Typical throughput: 1-10 million reads per minute</li> <li>Factors affecting speed: File size, compression, storage I/O</li> <li>Parallelization: Use multiple threads for batch processing</li> </ul>"},{"location":"Fastq/FastQC/#best-practices","title":"Best Practices","text":""},{"location":"Fastq/FastQC/#routine-quality-control","title":"Routine Quality Control","text":"<ul> <li>Run FastQC on all sequencing datasets</li> <li>Compare results across samples and runs</li> <li>Establish baseline quality metrics for your platform</li> </ul>"},{"location":"Fastq/FastQC/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Correlate quality issues with sequencing run metadata</li> <li>Use FastQC results to optimize library preparation</li> <li>Document quality control decisions for reproducibility</li> </ul>"},{"location":"Fastq/FastQC/#report-interpretation","title":"Report Interpretation","text":"<ul> <li>Consider experiment-specific quality requirements</li> <li>Understand that some \"failures\" may be expected (e.g., RNA-seq bias)</li> <li>Use multiple QC metrics together for comprehensive assessment</li> </ul>"},{"location":"Fastq/FastQC/#limitations","title":"Limitations","text":""},{"location":"Fastq/FastQC/#analysis-scope","title":"Analysis Scope","text":"<ul> <li>Limited to sequence-level quality assessment</li> <li>Does not evaluate biological significance</li> <li>Cannot detect all types of experimental artifacts</li> </ul>"},{"location":"Fastq/FastQC/#threshold-sensitivity","title":"Threshold Sensitivity","text":"<ul> <li>Generic thresholds may not suit all applications</li> <li>Some warnings may be acceptable for specific analyses</li> <li>Requires domain expertise for proper interpretation</li> </ul>"},{"location":"Fastq/FastQC/#conclusion","title":"Conclusion","text":"<p>FastQC serves as an essential first step in sequencing data quality assessment, providing standardized metrics and visualizations that guide preprocessing decisions and identify potential experimental issues. Its comprehensive analysis modules, combined with intuitive reporting, make it an indispensable tool in modern genomics workflows. Proper interpretation of FastQC results, combined with understanding of experimental context, enables researchers to make informed decisions about data processing and analysis strategies.</p>"},{"location":"Fastq/Fastq-screen/","title":"Fastq Contamination Chacking With Fastq-Screen","text":"Table of Content <ul> <li>Fastq Contamination Chacking With Fastq-Screen<ul> <li>Overview</li> <li>Principle of Operation</li> <li>Technical Implementation</li> <li>Output and Interpretation</li> <li>Applications in Genomics</li> <li>Best Practices</li> <li>Limitations and Considerations</li> <li>Command Line Usage</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Fastq/Fastq-screen/#overview","title":"Overview","text":"<p><code>Fastq-screen</code> is a quality control tool designed to screen libraries of short reads in Fastq format against a set of reference databases. The primary purpose of this tool is to identify potential contamination sources and assess the composition of sequencing libraries by aligning reads against multiple reference genomes or databases simultaneously.</p>"},{"location":"Fastq/Fastq-screen/#principle-of-operation","title":"Principle of Operation","text":"<p><code>Fastq-screen</code> operates by taking a subset of reads from input Fastq files and aligning them against user-defined reference databases using fast alignment algorithms. The tool provides quantitative assessment of how many reads align to each reference database, enabling researchers to identify potential contamination, confirm the expected organism composition, and detect unexpected sequences in their datasets.</p>"},{"location":"Fastq/Fastq-screen/#core-methodology","title":"Core Methodology","text":"<p>The screening process follows these key steps:</p> <ol> <li>Read Sampling: A representative subset of reads is extracted from the input FastQ file(s) to reduce computational overhead while maintaining statistical significance  </li> <li>Multi-Database Alignment: Reads are aligned against multiple reference databases simultaneously using configurable alignment parameters  </li> <li>Classification: Reads are classified based on their alignment patterns:  </li> <li>Uniquely mapping reads (align to only one database)  </li> <li>Multi-mapping reads (align to multiple databases)  </li> <li>Unmapped reads (fail to align to any database)  </li> <li>Statistical Reporting: Results are presented as percentages and counts for each reference database  </li> </ol>"},{"location":"Fastq/Fastq-screen/#technical-implementation","title":"Technical Implementation","text":""},{"location":"Fastq/Fastq-screen/#alignment-strategy","title":"Alignment Strategy","text":"<p>FastQ Screen utilizes Bowtie2 as the default aligner, though it supports multiple alignment tools including: - Bowtie2 (default) - Bowtie - BWA - HISAT2  </p> <p>The tool employs a mapping strategy optimized for contamination detection rather than comprehensive genome mapping, using parameters that favor sensitivity over specificity to capture potential contaminants.</p>"},{"location":"Fastq/Fastq-screen/#database-configuration","title":"Database Configuration","text":"<p>Reference databases are configured through a configuration file that specifies: - Database paths and names - Aligner-specific parameters - Organism or contamination source labels - Custom alignment settings per database  </p> <p>Common reference databases include: - Host organism genomes - Common laboratory contaminants (E. coli, yeast, mycoplasma) - Adapter sequences - Vector sequences - PhiX control sequences  </p>"},{"location":"Fastq/Fastq-screen/#performance-optimization","title":"Performance Optimization","text":"<p>To maintain computational efficiency, FastQ Screen implements several optimization strategies: - Subset Analysis: By default, only the first 100,000 reads are analyzed - Parallel Processing: Multi-threading support for simultaneous database screening - Memory Management: Efficient handling of large reference databases - Configurable Parameters: Adjustable alignment stringency and sampling parameters  </p>"},{"location":"Fastq/Fastq-screen/#output-and-interpretation","title":"Output and Interpretation","text":""},{"location":"Fastq/Fastq-screen/#graphical-output","title":"Graphical Output","text":"<p>FastQ Screen generates publication-ready plots showing: - Percentage of reads mapping to each database - Stacked bar charts displaying unique vs. multi-mapping reads - Heat maps for multiple sample comparisons  </p>"},{"location":"Fastq/Fastq-screen/#tabular-results","title":"Tabular Results","text":"<p>Detailed tabular output includes: - Read counts and percentages for each database - Mapping statistics (unique, multi-mapping, unmapped) - Quality metrics and alignment parameters used  </p>"},{"location":"Fastq/Fastq-screen/#interpretation-guidelines","title":"Interpretation Guidelines","text":"<p>Results interpretation focuses on: - Expected Alignments: High percentage alignment to the target organism database - Contamination Detection: Unexpected alignments to bacterial, viral, or other databases - Adapter Content: Presence of sequencing adapters or technical sequences - Cross-contamination: Alignments to multiple related organisms  </p>"},{"location":"Fastq/Fastq-screen/#applications-in-genomics","title":"Applications in Genomics","text":""},{"location":"Fastq/Fastq-screen/#quality-control-workflows","title":"Quality Control Workflows","text":"<p>FastQ Screen serves as a critical component in NGS quality control pipelines by: - Validating sample identity and purity - Detecting cross-contamination between samples - Identifying technical artifacts and adapter sequences - Assessing library preparation quality  </p>"},{"location":"Fastq/Fastq-screen/#contamination-source-identification","title":"Contamination Source Identification","text":"<p>The tool effectively identifies various contamination sources: - Bacterial Contamination: Detection of common laboratory bacterial strains - Human DNA Contamination: Identification of human DNA in non-human samples - Environmental Contamination: Detection of environmental microorganisms - Reagent Contamination: Identification of contaminating sequences from reagents  </p>"},{"location":"Fastq/Fastq-screen/#multi-species-analysis","title":"Multi-Species Analysis","text":"<p>For complex samples containing multiple organisms: - Metagenomic sample composition assessment - Host-pathogen interaction studies - Symbiotic relationship analysis - Environmental sample characterization  </p>"},{"location":"Fastq/Fastq-screen/#best-practices","title":"Best Practices","text":""},{"location":"Fastq/Fastq-screen/#database-selection","title":"Database Selection","text":"<p>Optimal database selection should include: - Target organism genome(s) - Common laboratory contaminants - Potential environmental contaminants specific to sample type - Technical sequences (adapters, vectors, controls)  </p>"},{"location":"Fastq/Fastq-screen/#parameter-optimization","title":"Parameter Optimization","text":"<p>Key parameters to consider: - Number of reads to screen (balance between accuracy and speed) - Alignment stringency settings - Multi-mapping tolerance thresholds - Minimum alignment length requirements  </p>"},{"location":"Fastq/Fastq-screen/#integration-with-pipelines","title":"Integration with Pipelines","text":"<p>FastQ Screen integrates effectively with: - MultiQC for consolidated quality control reporting - Automated bioinformatics pipelines - High-throughput screening workflows - Custom quality control frameworks  </p>"},{"location":"Fastq/Fastq-screen/#limitations-and-considerations","title":"Limitations and Considerations","text":""},{"location":"Fastq/Fastq-screen/#technical-limitations","title":"Technical Limitations","text":"<ul> <li>Database Dependency: Results quality depends on comprehensive and current reference databases  </li> <li>Short Read Specificity: Optimized for short-read sequencing technologies  </li> <li>Alignment Sensitivity: May miss highly divergent sequences or novel organisms  </li> <li>Computational Resources: Memory requirements scale with database size  </li> </ul>"},{"location":"Fastq/Fastq-screen/#biological-considerations","title":"Biological Considerations","text":"<ul> <li>Phylogenetic Relationships: Closely related organisms may show cross-mapping  </li> <li>Repetitive Sequences: Highly repetitive regions may cause multi-mapping artifacts  </li> <li>Sequence Similarity: Conservative interpretation needed for evolutionarily related species  </li> <li>Database Completeness: Absence of alignment doesn't guarantee absence of organism  </li> </ul>"},{"location":"Fastq/Fastq-screen/#command-line-usage","title":"Command Line Usage","text":""},{"location":"Fastq/Fastq-screen/#basic-commands","title":"Basic Commands","text":""},{"location":"Fastq/Fastq-screen/#standard-single-end-screening","title":"Standard Single-End Screening","text":"<pre><code>fastq_screen --conf /path/to/fastq_screen.conf sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#paired-end-screening","title":"Paired-End Screening","text":"<pre><code>fastq_screen --conf /path/to/fastq_screen.conf sample_R1.fastq.gz sample_R2.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#multiple-file-processing","title":"Multiple File Processing","text":"<pre><code>fastq_screen --conf /path/to/fastq_screen.conf *.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"Fastq/Fastq-screen/#custom-subset-size","title":"Custom Subset Size","text":"<pre><code># Screen only the first 50,000 reads for faster processing\nfastq_screen --conf /path/to/fastq_screen.conf --subset 50000 sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#multi-threading","title":"Multi-threading","text":"<pre><code># Use 8 threads for parallel processing\nfastq_screen --conf /path/to/fastq_screen.conf --threads 8 sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#output-directory-specification","title":"Output Directory Specification","text":"<pre><code># Specify custom output directory\nfastq_screen --conf /path/to/fastq_screen.conf --outdir /path/to/results/ sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#force-overwrite-existing-results","title":"Force Overwrite Existing Results","text":"<pre><code># Overwrite existing output files\nfastq_screen --conf /path/to/fastq_screen.conf --force sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#specialized-screening-options","title":"Specialized Screening Options","text":""},{"location":"Fastq/Fastq-screen/#tag-based-output","title":"Tag-based Output","text":"<pre><code># Add custom tag to output files\nfastq_screen --conf /path/to/fastq_screen.conf --tag custom_label sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#aligner-selection","title":"Aligner Selection","text":"<pre><code># Use BWA instead of default Bowtie2\nfastq_screen --conf /path/to/fastq_screen.conf --aligner bwa sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#quiet-mode","title":"Quiet Mode","text":"<pre><code># Suppress standard output messages\nfastq_screen --conf /path/to/fastq_screen.conf --quiet sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#configuration-file-examples","title":"Configuration File Examples","text":""},{"location":"Fastq/Fastq-screen/#basic-configuration-structure","title":"Basic Configuration Structure","text":"<pre><code># Example fastq_screen.conf file content\nDATABASE    Human    /path/to/human_genome/human_index    bowtie2\nDATABASE    Mouse    /path/to/mouse_genome/mouse_index    bowtie2\nDATABASE    E_coli   /path/to/ecoli_genome/ecoli_index    bowtie2\nDATABASE    Yeast    /path/to/yeast_genome/yeast_index    bowtie2\nDATABASE    PhiX     /path/to/phix_genome/phix_index      bowtie2\nDATABASE    Adapters /path/to/adapters/adapter_index     bowtie2\n</code></pre>"},{"location":"Fastq/Fastq-screen/#advanced-configuration-with-custom-parameters","title":"Advanced Configuration with Custom Parameters","text":"<pre><code># Configuration with custom bowtie2 parameters\nDATABASE    Human    /path/to/human_genome/human_index    bowtie2    --very-sensitive --end-to-end\nDATABASE    Bacteria /path/to/bacteria_db/bacteria_index bowtie2    --local --very-fast\n</code></pre>"},{"location":"Fastq/Fastq-screen/#batch-processing-scripts","title":"Batch Processing Scripts","text":""},{"location":"Fastq/Fastq-screen/#shell-script-for-multiple-samples","title":"Shell Script for Multiple Samples","text":"<pre><code>#!/bin/bash\n# Batch processing script\nCONF_FILE=\"/path/to/fastq_screen.conf\"\nINPUT_DIR=\"/path/to/fastq_files/\"\nOUTPUT_DIR=\"/path/to/results/\"\n\nfor file in ${INPUT_DIR}*.fastq.gz; do\n    echo \"Processing $file\"\n    fastq_screen --conf $CONF_FILE --outdir $OUTPUT_DIR --threads 4 $file\ndone\n</code></pre>"},{"location":"Fastq/Fastq-screen/#paired-end-batch-processing","title":"Paired-End Batch Processing","text":"<pre><code>#!/bin/bash\n# Process paired-end files\nCONF_FILE=\"/path/to/fastq_screen.conf\"\nINPUT_DIR=\"/path/to/fastq_files/\"\n\nfor r1_file in ${INPUT_DIR}*_R1.fastq.gz; do\n    r2_file=${r1_file/_R1/_R2}\n    echo \"Processing pair: $r1_file and $r2_file\"\n    fastq_screen --conf $CONF_FILE --threads 8 $r1_file $r2_file\ndone\n</code></pre>"},{"location":"Fastq/Fastq-screen/#fastq-screen-on-a-hpc-cluster","title":"Fastq-screen on a HPC Cluster","text":"<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=fastqscreen  \n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=24G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Recover of fastq file name\nSAMPLELANE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/fastq_files)\n\n# Locate INPUT directory \nINPUT_FOLDER=\"raw\"\n\nLOG_FOLDER=\"results/11_Reports/fastq-screen\"\n\nmodule load fastq-screen/0.15.3 bwa/0.7.17\n\nfastq_screen -q --threads 8 \\\n            --conf info_files/fastq-screen.conf \\\n            --aligner bwa \\\n            --subset 1000 \\\n            --outdir qc/fastq-screen/ \\\n            ${INPUT_FOLDER}/*.fastq.gz \\\n            &amp;&gt; ${LOG_FOLDER}/${SAMPLELANE}-fastq-screen.log\n</code></pre>"},{"location":"Fastq/Fastq-screen/#integration-with-quality-control-pipelines","title":"Integration with Quality Control Pipelines","text":""},{"location":"Fastq/Fastq-screen/#combined-with-fastqc","title":"Combined with FastQC","text":"<pre><code># Run FastQC and FastQ Screen sequentially\nfastqc sample.fastq.gz\nfastq_screen --conf /path/to/fastq_screen.conf sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#integration-with-multiqc","title":"Integration with MultiQC","text":"<pre><code># Generate FastQ Screen reports for MultiQC compilation\nfastq_screen --conf /path/to/fastq_screen.conf *.fastq.gz\nmultiqc .\n</code></pre>"},{"location":"Fastq/Fastq-screen/#troubleshooting-commands","title":"Troubleshooting Commands","text":""},{"location":"Fastq/Fastq-screen/#test-configuration","title":"Test Configuration","text":"<pre><code># Test configuration file validity\nfastq_screen --conf /path/to/fastq_screen.conf --help\n</code></pre>"},{"location":"Fastq/Fastq-screen/#verbose-output-for-debugging","title":"Verbose Output for Debugging","text":"<pre><code># Enable verbose output for troubleshooting\nfastq_screen --conf /path/to/fastq_screen.conf --verbose sample.fastq.gz\n</code></pre>"},{"location":"Fastq/Fastq-screen/#version-information","title":"Version Information","text":"<pre><code># Check FastQ Screen version\nfastq_screen --version\n</code></pre>"},{"location":"Fastq/Fastq-screen/#conclusion","title":"Conclusion","text":"<p>FastQ Screen represents an essential tool in the NGS quality control arsenal, providing rapid and comprehensive contamination screening capabilities. Its ability to simultaneously screen against multiple reference databases makes it invaluable for maintaining data quality and integrity in genomics research. When properly configured and interpreted, FastQ Screen enables researchers to confidently assess the composition and quality of their sequencing libraries, ultimately contributing to more reliable and reproducible genomics analyses.</p>"},{"location":"Fastq/Fastq/","title":"FASTQ Format","text":"Table of Content <ul> <li>FASTQ Format<ul> <li>Overview</li> <li>Format Structure</li> <li>Quality Score Encoding</li> <li>File Extensions and Compression</li> <li>Paired-End Sequencing</li> <li>Best Practices</li> <li>Common Applications</li> <li>Technical Considerations</li> <li>Related Formats</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Fastq/Fastq/#overview","title":"Overview","text":"<p>The FASTQ format is a text-based file format widely used in bioinformatics to store nucleotide sequences along with their corresponding quality scores. Originally developed for the Sanger sequencing platform, FASTQ has become the de facto standard for representing high-throughput sequencing data from modern platforms including Illumina, Oxford Nanopore, and PacBio systems.</p>"},{"location":"Fastq/Fastq/#format-structure","title":"Format Structure","text":"<p>Each sequence record in a FASTQ file consists of exactly four lines:</p> <ol> <li>Header line: Begins with <code>@</code> followed by the sequence identifier and optional description</li> <li>Sequence line: Contains the raw nucleotide sequence (A, T, G, C, N)</li> <li>Separator line: Begins with <code>+</code> and optionally repeats the sequence identifier</li> <li>Quality line: Contains quality scores encoded as ASCII characters</li> </ol>"},{"location":"Fastq/Fastq/#example-record","title":"Example Record","text":"<pre><code>@SEQ_ID_001 description text\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65\n</code></pre>"},{"location":"Fastq/Fastq/#quality-score-encoding","title":"Quality Score Encoding","text":"<p>Quality scores represent the confidence in each base call, typically expressed as Phred quality scores:</p> <p>Phred Quality Score Formula: <pre><code>Q = -10 \u00d7 log\u2081\u2080(P)\n</code></pre></p> <p>Where <code>P</code> is the probability that the base call is incorrect.</p>"},{"location":"Fastq/Fastq/#common-encoding-schemes","title":"Common Encoding Schemes","text":"<p>FASTQ files combine nucleotide sequences and corresponding quality scores. Each quality score is <code>Phred-scaled</code>, meaning it\u2019s a logarithmic measure of the probability that a base was called incorrectly. To store these scores efficiently in text format, they are encoded as <code>ASCII characters</code>, and different sequencing platforms (and versions) have used different encoding schemes.</p> Encoding ASCII Range Phred Score Range Offset Sanger/Illumina 1.8+ 33-126 0-93 33 Illumina 1.3-1.7 64-126 0-62 64 Solexa 59-126 -5-62 64"},{"location":"Fastq/Fastq/#quality-score-interpretation","title":"Quality Score Interpretation","text":"Phred Score Error Probability Accuracy ASCII (Sanger) 10 1 in 10 90% + 20 1 in 100 99% 5 30 1 in 1000 99.9% ? 40 1 in 10000 99.99% I"},{"location":"Fastq/Fastq/#file-extensions-and-compression","title":"File Extensions and Compression","text":""},{"location":"Fastq/Fastq/#standard-extensions","title":"Standard Extensions","text":"<ul> <li><code>.fastq</code> - Uncompressed FASTQ file</li> <li><code>.fq</code> - Alternative uncompressed extension</li> <li><code>.fastq.gz</code> - Gzip-compressed FASTQ file</li> <li><code>.fq.gz</code> - Alternative compressed extension</li> </ul>"},{"location":"Fastq/Fastq/#compression-benefits","title":"Compression Benefits","text":"<p>Compression typically reduces file size by 70-80%, significantly improving storage efficiency and transfer speeds while maintaining data integrity.</p>"},{"location":"Fastq/Fastq/#paired-end-sequencing","title":"Paired-End Sequencing","text":"<p>For paired-end sequencing data, reads are typically stored in separate files:</p> <ul> <li><code>sample_R1.fastq</code> - Forward reads (Read 1)</li> <li><code>sample_R2.fastq</code> - Reverse reads (Read 2)</li> </ul> <p>Corresponding reads maintain identical identifiers, often with <code>/1</code> and <code>/2</code> suffixes or <code>1:</code> and <code>2:</code> flags.</p>"},{"location":"Fastq/Fastq/#best-practices","title":"Best Practices","text":""},{"location":"Fastq/Fastq/#data-integrity","title":"Data Integrity","text":"<ul> <li>Always validate FASTQ format before analysis</li> <li>Verify quality score encoding scheme</li> <li>Check for consistent record structure (4 lines per read)</li> </ul>"},{"location":"Fastq/Fastq/#storage-considerations","title":"Storage Considerations","text":"<ul> <li>Use compression for long-term storage</li> <li>Maintain raw FASTQ files as primary data</li> <li>Implement proper backup strategies</li> </ul>"},{"location":"Fastq/Fastq/#quality-control","title":"Quality Control","text":"<ul> <li>Assess quality score distributions</li> <li>Monitor sequence length distributions</li> <li>Identify adapter contamination</li> <li>Evaluate per-base quality trends</li> </ul>"},{"location":"Fastq/Fastq/#common-applications","title":"Common Applications","text":""},{"location":"Fastq/Fastq/#preprocessing","title":"Preprocessing","text":"<ul> <li>Quality trimming and filtering</li> <li>Adapter removal</li> <li>Contamination screening</li> <li>Error correction</li> </ul>"},{"location":"Fastq/Fastq/#analysis-workflows","title":"Analysis Workflows","text":"<ul> <li>Genome assembly</li> <li>Read mapping and alignment</li> <li>Variant calling</li> <li>RNA-seq expression analysis</li> <li>Metagenomics profiling</li> </ul>"},{"location":"Fastq/Fastq/#technical-considerations","title":"Technical Considerations","text":""},{"location":"Fastq/Fastq/#memory-management","title":"Memory Management","text":"<p>Large FASTQ files require efficient processing strategies: - Stream-based processing for memory efficiency - Parallel processing for performance optimization - Chunked analysis for very large datasets  </p>"},{"location":"Fastq/Fastq/#format-validation","title":"Format Validation","text":"<p>Key validation checks include: - Consistent 4-line record structure - Valid nucleotide characters (A, T, G, C, N) - Matching sequence and quality string lengths - Proper ASCII encoding ranges  </p>"},{"location":"Fastq/Fastq/#related-formats","title":"Related Formats","text":"<ul> <li>FASTA: Sequence-only format without quality scores</li> <li>SAM/BAM: Aligned sequence format with additional mapping information</li> <li>CRAM: Compressed reference-based sequence format</li> </ul>"},{"location":"Fastq/Fastq/#conclusion","title":"Conclusion","text":"<p>The FASTQ format remains fundamental to modern genomics workflows, providing essential quality information alongside sequence data. Understanding its structure, encoding schemes, and best practices is crucial for effective bioinformatics analysis and ensuring data integrity throughout the sequencing-to-analysis pipeline.</p>"},{"location":"Fastq/Trimming/","title":"Trimmomatic: Quality-Based Read Trimming","text":"Table of Content <ul> <li>Trimmomatic: Quality-Based Read Trimming<ul> <li>Overview</li> <li>Principle of Operation</li> <li>Technical Implementation</li> <li>Trimming Operations</li> <li>Command Line Usage</li> <li>Performance Considerations</li> <li>Output Interpretation</li> <li>Best Practices</li> <li>Limitations and Considerations</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Fastq/Trimming/#overview","title":"Overview","text":"<p><code>Trimmomatic</code> is a comprehensive read trimming tool for Illumina NGS data that implements flexible quality-based trimming algorithms. The software performs sequence quality assessment and trimming operations through a modular pipeline architecture, enabling researchers to remove low-quality bases, adapter sequences, and other artifacts that could compromise downstream analysis accuracy.</p>"},{"location":"Fastq/Trimming/#principle-of-operation","title":"Principle of Operation","text":"<p><code>Trimmomatic</code> operates through a multi-step processing pipeline where each trimming operation is implemented as a discrete module. The tool processes reads sequentially through user-defined trimming steps, applying quality-based filtering and trimming algorithms to maximize both read quality and retention rates.</p>"},{"location":"Fastq/Trimming/#core-methodology","title":"Core Methodology","text":"<p>The trimming process is based on several key algorithmic approaches:</p> <ol> <li>Sliding Window Quality Trimming: Implements a sliding window algorithm that evaluates average quality scores across defined window sizes</li> <li>Leading/Trailing Quality Trimming: Removes low-quality bases from read termini based on quality thresholds</li> <li>Adaptive Adapter Trimming: Identifies and removes adapter sequences through seed-based alignment algorithms</li> <li>Length-Based Filtering: Applies minimum length thresholds to ensure read utility for downstream applications</li> </ol>"},{"location":"Fastq/Trimming/#technical-implementation","title":"Technical Implementation","text":""},{"location":"Fastq/Trimming/#quality-assessment-algorithms","title":"Quality Assessment Algorithms","text":""},{"location":"Fastq/Trimming/#sliding-window-trimming","title":"Sliding Window Trimming","text":"<p>The sliding window algorithm evaluates sequence quality through: - Window Size Definition: Configurable window sizes (typically 4-5 bases) - Quality Threshold Application: Average quality score thresholds within each window - Progressive Evaluation: Sequential window advancement along read length - Optimal Cutting Point Identification: Determination of trimming positions that maximize retained sequence quality  </p>"},{"location":"Fastq/Trimming/#phred-quality-score-integration","title":"Phred Quality Score Integration","text":"<p>Trimmomatic utilizes Phred quality scores for decision-making processes: - Quality Score Interpretation: Conversion of ASCII-encoded quality scores to probability values - Threshold-Based Decisions: Application of user-defined quality thresholds for trimming operations - Statistical Quality Assessment: Probabilistic evaluation of base-calling accuracy  </p>"},{"location":"Fastq/Trimming/#adapter-removal-mechanisms","title":"Adapter Removal Mechanisms","text":""},{"location":"Fastq/Trimming/#seed-based-alignment","title":"Seed-Based Alignment","text":"<p>Adapter detection employs sophisticated alignment algorithms: - Seed Identification: Short exact matches between reads and adapter sequences - Extension Algorithms: Extension of seed matches to identify full adapter sequences - Mismatch Tolerance: Configurable mismatch thresholds for adapter identification - Partial Adapter Detection: Identification of adapter sequences at read termini  </p>"},{"location":"Fastq/Trimming/#palindrome-mode","title":"Palindrome Mode","text":"<p>For paired-end sequencing, Trimmomatic implements palindrome detection: - Insert Size Evaluation: Assessment of library insert sizes relative to read length - Complementary Sequence Detection: Identification of adapter-adapter ligation events - Read-Through Artifact Removal: Elimination of sequences resulting from short insert libraries  </p>"},{"location":"Fastq/Trimming/#processing-modes","title":"Processing Modes","text":""},{"location":"Fastq/Trimming/#single-end-processing","title":"Single-End Processing","text":"<p>Single-end mode implements: - Quality-based trimming algorithms - Adapter sequence removal - Length-based filtering - Output quality assessment  </p>"},{"location":"Fastq/Trimming/#paired-end-processing","title":"Paired-End Processing","text":"<p>Paired-end mode maintains read pair integrity through: - Synchronous Processing: Simultaneous processing of forward and reverse reads - Pair Integrity Maintenance: Preservation of read pairing relationships - Orphan Read Handling: Management of reads whose pairs fail quality thresholds - Output Stream Segregation: Separation of paired and unpaired reads post-processing  </p>"},{"location":"Fastq/Trimming/#trimming-operations","title":"Trimming Operations","text":""},{"location":"Fastq/Trimming/#illuminaclip","title":"ILLUMINACLIP","text":"<p>Adapter and quality trimming operation with parameters: - Adapter File: FASTA file containing adapter sequences - Seed Mismatches: Number of mismatches allowed in seed region - Palindrome Clip Threshold: Accuracy threshold for palindrome matches - Simple Clip Threshold: Accuracy threshold for simple adapter matches  </p>"},{"location":"Fastq/Trimming/#slidingwindow","title":"SLIDINGWINDOW","text":"<p>Quality-based trimming using sliding window approach: - Window Size: Number of bases in sliding window - Required Quality: Average quality threshold within window - Trimming Strategy: 3' end trimming based on quality degradation  </p>"},{"location":"Fastq/Trimming/#leadingtrailing","title":"LEADING/TRAILING","text":"<p>Terminal base removal based on quality thresholds: - LEADING: Removal of low-quality bases from 5' end - TRAILING: Removal of low-quality bases from 3' end - Quality Threshold: Minimum acceptable quality score  </p>"},{"location":"Fastq/Trimming/#minlen","title":"MINLEN","text":"<p>Length-based filtering: - Minimum Length: Shortest acceptable read length post-trimming - Read Retention: Ensures reads maintain sufficient length for alignment  </p>"},{"location":"Fastq/Trimming/#cropheadcrop","title":"CROP/HEADCROP","text":"<p>Fixed-length trimming operations: - CROP: Trimming reads to specified maximum length - HEADCROP: Removal of specified number of bases from 5' end  </p>"},{"location":"Fastq/Trimming/#command-line-usage","title":"Command Line Usage","text":""},{"location":"Fastq/Trimming/#basic-commands","title":"Basic Commands","text":""},{"location":"Fastq/Trimming/#single-end-trimming","title":"Single-End Trimming","text":"<pre><code>java -jar trimmomatic-0.39.jar SE -phred33 \\\n    input.fastq.gz output_trimmed.fastq.gz \\\n    ILLUMINACLIP:TruSeq3-SE.fa:2:30:10 \\\n    LEADING:3 TRAILING:3 \\\n    SLIDINGWINDOW:4:15 \\\n    MINLEN:36\n</code></pre>"},{"location":"Fastq/Trimming/#paired-end-trimming","title":"Paired-End Trimming","text":"<pre><code>java -jar trimmomatic-0.39.jar PE -phred33 \\\n    input_R1.fastq.gz input_R2.fastq.gz \\\n    output_R1_paired.fastq.gz output_R1_unpaired.fastq.gz \\\n    output_R2_paired.fastq.gz output_R2_unpaired.fastq.gz \\\n    ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 \\\n    LEADING:3 TRAILING:3 \\\n    SLIDINGWINDOW:4:15 \\\n    MINLEN:36\n</code></pre>"},{"location":"Fastq/Trimming/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"Fastq/Trimming/#high-quality-trimming","title":"High-Quality Trimming","text":"<pre><code># Stringent quality trimming for sensitive applications\njava -jar trimmomatic-0.39.jar PE -phred33 \\\n    sample_R1.fastq.gz sample_R2.fastq.gz \\\n    sample_R1_paired.fastq.gz sample_R1_unpaired.fastq.gz \\\n    sample_R2_paired.fastq.gz sample_R2_unpaired.fastq.gz \\\n    ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads \\\n    LEADING:5 TRAILING:5 \\\n    SLIDINGWINDOW:4:20 \\\n    MINLEN:50\n</code></pre>"},{"location":"Fastq/Trimming/#rna-seq-specific-trimming","title":"RNA-seq Specific Trimming","text":"<pre><code># Optimized for RNA-seq with stranded libraries\njava -jar trimmomatic-0.39.jar PE -phred33 \\\n    RNAseq_R1.fastq.gz RNAseq_R2.fastq.gz \\\n    RNAseq_R1_paired.fastq.gz RNAseq_R1_unpaired.fastq.gz \\\n    RNAseq_R2_paired.fastq.gz RNAseq_R2_unpaired.fastq.gz \\\n    ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:1:true \\\n    LEADING:3 TRAILING:3 \\\n    SLIDINGWINDOW:4:15 \\\n    MINLEN:25\n</code></pre>"},{"location":"Fastq/Trimming/#memory-optimized-processing","title":"Memory-Optimized Processing","text":"<pre><code># Large file processing with memory optimization\njava -Xmx4g -jar trimmomatic-0.39.jar PE -phred33 \\\n    -threads 8 \\\n    large_R1.fastq.gz large_R2.fastq.gz \\\n    large_R1_paired.fastq.gz large_R1_unpaired.fastq.gz \\\n    large_R2_paired.fastq.gz large_R2_unpaired.fastq.gz \\\n    ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 \\\n    LEADING:3 TRAILING:3 \\\n    SLIDINGWINDOW:4:15 \\\n    MINLEN:36\n</code></pre>"},{"location":"Fastq/Trimming/#specialized-trimming-strategies","title":"Specialized Trimming Strategies","text":""},{"location":"Fastq/Trimming/#nextera-adapter-removal","title":"Nextera Adapter Removal","text":"<pre><code># Nextera library preparation adapter trimming\njava -jar trimmomatic-0.39.jar PE -phred33 \\\n    nextera_R1.fastq.gz nextera_R2.fastq.gz \\\n    nextera_R1_paired.fastq.gz nextera_R1_unpaired.fastq.gz \\\n    nextera_R2_paired.fastq.gz nextera_R2_unpaired.fastq.gz \\\n    ILLUMINACLIP:NexteraPE-PE.fa:2:30:10:2:keepBothReads \\\n    LEADING:3 TRAILING:3 \\\n    SLIDINGWINDOW:4:15 \\\n    MINLEN:36\n</code></pre>"},{"location":"Fastq/Trimming/#custom-adapter-trimming","title":"Custom Adapter Trimming","text":"<pre><code># Custom adapter sequences\njava -jar trimmomatic-0.39.jar PE -phred33 \\\n    custom_R1.fastq.gz custom_R2.fastq.gz \\\n    custom_R1_paired.fastq.gz custom_R1_unpaired.fastq.gz \\\n    custom_R2_paired.fastq.gz custom_R2_unpaired.fastq.gz \\\n    ILLUMINACLIP:custom_adapters.fa:2:30:10 \\\n    LEADING:3 TRAILING:3 \\\n    SLIDINGWINDOW:4:15 \\\n    MINLEN:36\n</code></pre>"},{"location":"Fastq/Trimming/#fixed-length-trimming","title":"Fixed-Length Trimming","text":"<pre><code># Uniform read length generation\njava -jar trimmomatic-0.39.jar PE -phred33 \\\n    variable_R1.fastq.gz variable_R2.fastq.gz \\\n    fixed_R1_paired.fastq.gz fixed_R1_unpaired.fastq.gz \\\n    fixed_R2_paired.fastq.gz fixed_R2_unpaired.fastq.gz \\\n    ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 \\\n    HEADCROP:15 \\\n    CROP:100 \\\n    LEADING:3 TRAILING:3 \\\n    MINLEN:75\n</code></pre>"},{"location":"Fastq/Trimming/#batch-processing-scripts","title":"Batch Processing Scripts","text":""},{"location":"Fastq/Trimming/#shell-script-for-multiple-samples","title":"Shell Script for Multiple Samples","text":"<pre><code>#!/bin/bash\n# Batch trimming script for multiple paired-end samples\n\nTRIMMOMATIC_JAR=\"/path/to/trimmomatic-0.39.jar\"\nADAPTER_FILE=\"/path/to/TruSeq3-PE.fa\"\nINPUT_DIR=\"/path/to/raw_fastq/\"\nOUTPUT_DIR=\"/path/to/trimmed_fastq/\"\n\nmkdir -p $OUTPUT_DIR\n\nfor r1_file in ${INPUT_DIR}*_R1.fastq.gz; do\n    sample_name=$(basename $r1_file _R1.fastq.gz)\n    r2_file=${INPUT_DIR}${sample_name}_R2.fastq.gz\n\n    echo \"Processing sample: $sample_name\"\n\n    java -jar $TRIMMOMATIC_JAR PE -phred33 -threads 4 \\\n        $r1_file $r2_file \\\n        ${OUTPUT_DIR}${sample_name}_R1_paired.fastq.gz \\\n        ${OUTPUT_DIR}${sample_name}_R1_unpaired.fastq.gz \\\n        ${OUTPUT_DIR}${sample_name}_R2_paired.fastq.gz \\\n        ${OUTPUT_DIR}${sample_name}_R2_unpaired.fastq.gz \\\n        ILLUMINACLIP:$ADAPTER_FILE:2:30:10 \\\n        LEADING:3 TRAILING:3 \\\n        SLIDINGWINDOW:4:15 \\\n        MINLEN:36\ndone\n</code></pre>"},{"location":"Fastq/Trimming/#quality-control-integration","title":"Quality Control Integration","text":"<pre><code>#!/bin/bash\n# Combined quality assessment and trimming pipeline\n\nSAMPLE_PREFIX=\"sample\"\nFASTQC_OUTPUT=\"/path/to/fastqc_output/\"\nTRIMMED_OUTPUT=\"/path/to/trimmed_output/\"\n\n# Pre-trimming quality assessment\nfastqc -o $FASTQC_OUTPUT ${SAMPLE_PREFIX}_R1.fastq.gz ${SAMPLE_PREFIX}_R2.fastq.gz\n\n# Trimming operation\njava -jar trimmomatic-0.39.jar PE -phred33 \\\n    ${SAMPLE_PREFIX}_R1.fastq.gz ${SAMPLE_PREFIX}_R2.fastq.gz \\\n    ${TRIMMED_OUTPUT}${SAMPLE_PREFIX}_R1_paired.fastq.gz \\\n    ${TRIMMED_OUTPUT}${SAMPLE_PREFIX}_R1_unpaired.fastq.gz \\\n    ${TRIMMED_OUTPUT}${SAMPLE_PREFIX}_R2_paired.fastq.gz \\\n    ${TRIMMED_OUTPUT}${SAMPLE_PREFIX}_R2_unpaired.fastq.gz \\\n    ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 \\\n    LEADING:3 TRAILING:3 \\\n    SLIDINGWINDOW:4:15 \\\n    MINLEN:36\n\n# Post-trimming quality assessment\nfastqc -o $FASTQC_OUTPUT \\\n    ${TRIMMED_OUTPUT}${SAMPLE_PREFIX}_R1_paired.fastq.gz \\\n    ${TRIMMED_OUTPUT}${SAMPLE_PREFIX}_R2_paired.fastq.gz\n</code></pre>"},{"location":"Fastq/Trimming/#batch-processing-on-a-hpc-cluster","title":"Batch Processing On a HPC Cluster","text":"<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=trim\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=16G\n#SBATCH --array 1-56\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Recover of fastq file name\nSAMPLE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/fastq_files)\n\n# Create OUTPUT directory\nOUTPUT_FOLDER=\"results/01_Trimming\"\nmkdir -p \"$OUTPUT_FOLDER\"\n\n# Create directory for log files\nLOG_FOLDER=\"results/11_Reports\"\nmkdir -p \"$LOG_FOLDER\"\n\nmodule load trimmomatic/0.39 fastqc/0.12.1\n\ntrimmomatic PE -threads 16 \\\n    -phred33 -trimlog \"$LOG_FOLDER/trimmomatic_${SAMPLE}.log\" \\\n    \"raw/${SAMPLE}_R1.fastq.gz\" \\\n    \"raw/${SAMPLE}_R2.fastq.gz\" \\\n    \"$OUTPUT_FOLDER/${SAMPLE}_trimmomatic_R1.fastq.gz\" \\\n    \"$OUTPUT_FOLDER/${SAMPLE}_trimmomatic_unpaired_R1.fastq.gz\" \\\n    \"$OUTPUT_FOLDER/${SAMPLE}_trimmomatic_R2.fastq.gz\" \\\n    \"$OUTPUT_FOLDER/${SAMPLE}_trimmomatic_unpaired_R2.fastq.gz\" \\\n    ILLUMINACLIP:resources/adapters/TruSeq2-PE.fa:2:30:15 LEADING:20 TRAILING:3 SLIDINGWINDOW:5:20 AVGQUAL:20 MINLEN:50\n\nfastqc --threads 16 -o qc/fastqc-post-trim/ \"$OUTPUT_FOLDER/${SAMPLE}_trimmomatic_R1.fastq.gz\"\nfastqc --threads 16 -o qc/fastqc-post-trim/ \"$OUTPUT_FOLDER/${SAMPLE}_trimmomatic_R2.fastq.gz\"\n</code></pre>"},{"location":"Fastq/Trimming/#performance-considerations","title":"Performance Considerations","text":""},{"location":"Fastq/Trimming/#computational-requirements","title":"Computational Requirements","text":""},{"location":"Fastq/Trimming/#memory-usage","title":"Memory Usage","text":"<ul> <li>Single-End: Minimal memory requirements (typically &lt;1GB)  </li> <li>Paired-End: Moderate memory usage scaling with file size  </li> <li>Large Files: Memory optimization through streaming algorithms  </li> </ul>"},{"location":"Fastq/Trimming/#threading-support","title":"Threading Support","text":"<ul> <li>Multi-threading: Parallel processing capability for improved performance  </li> <li>I/O Optimization: Efficient file handling for compressed formats  </li> <li>Scalability: Linear performance scaling with thread count  </li> </ul>"},{"location":"Fastq/Trimming/#algorithm-efficiency","title":"Algorithm Efficiency","text":""},{"location":"Fastq/Trimming/#time-complexity","title":"Time Complexity","text":"<ul> <li>Linear Processing: O(n) time complexity relative to input size  </li> <li>Sliding Window: Constant time window evaluation  </li> <li>Adapter Matching: Efficient seed-based alignment algorithms  </li> </ul>"},{"location":"Fastq/Trimming/#output-interpretation","title":"Output Interpretation","text":""},{"location":"Fastq/Trimming/#quality-metrics","title":"Quality Metrics","text":""},{"location":"Fastq/Trimming/#trimming-statistics","title":"Trimming Statistics","text":"<ul> <li>Input Read Count: Total reads processed  </li> <li>Output Read Count: Reads surviving quality filters  </li> <li>Trimming Rates: Percentage of reads requiring trimming  </li> <li>Length Distributions: Pre- and post-trimming read length profiles  </li> </ul>"},{"location":"Fastq/Trimming/#quality-improvements","title":"Quality Improvements","text":"<ul> <li>Quality Score Distributions: Comparison of quality profiles  </li> <li>Adapter Contamination: Reduction in adapter sequence content  </li> <li>Overall Quality Enhancement: Improvement in dataset quality metrics  </li> </ul>"},{"location":"Fastq/Trimming/#best-practices","title":"Best Practices","text":""},{"location":"Fastq/Trimming/#parameter-optimization","title":"Parameter Optimization","text":""},{"location":"Fastq/Trimming/#quality-thresholds","title":"Quality Thresholds","text":"<ul> <li>Conservative Approach: Higher quality thresholds for critical applications  </li> <li>Balanced Strategy: Moderate thresholds preserving read count and quality  </li> <li>Application-Specific: Parameter adjustment based on downstream analysis requirements  </li> </ul>"},{"location":"Fastq/Trimming/#adapter-trimming","title":"Adapter Trimming","text":"<ul> <li>Accurate Adapter Files: Use of appropriate adapter sequences for library preparation method  </li> <li>Stringency Levels: Adjustment of mismatch tolerance based on sequence quality  </li> <li>Palindrome Detection: Optimization for paired-end library characteristics  </li> </ul>"},{"location":"Fastq/Trimming/#integration-strategies","title":"Integration Strategies","text":""},{"location":"Fastq/Trimming/#pipeline-integration","title":"Pipeline Integration","text":"<ul> <li>Quality Control Workflows: Integration with FastQC and MultiQC  </li> <li>Alignment Pipelines: Preprocessing for genome and transcriptome alignment  </li> <li>Variant Calling: Quality optimization for SNP and indel detection  </li> </ul>"},{"location":"Fastq/Trimming/#limitations-and-considerations","title":"Limitations and Considerations","text":""},{"location":"Fastq/Trimming/#technical-limitations","title":"Technical Limitations","text":""},{"location":"Fastq/Trimming/#algorithm-constraints","title":"Algorithm Constraints","text":"<ul> <li>Quality Score Dependency: Reliance on accurate quality score calibration  </li> <li>Adapter Database Requirements: Need for comprehensive adapter sequence databases  </li> <li>Short Read Optimization: Designed primarily for short-read sequencing technologies  </li> </ul>"},{"location":"Fastq/Trimming/#biological-considerations","title":"Biological Considerations","text":"<ul> <li>Over-Trimming Risk: Potential for excessive sequence removal  </li> <li>Bias Introduction: Possible introduction of length or quality biases  </li> <li>Context Sensitivity: Need for application-specific parameter optimization  </li> </ul>"},{"location":"Fastq/Trimming/#quality-control-validation","title":"Quality Control Validation","text":""},{"location":"Fastq/Trimming/#post-trimming-assessment","title":"Post-Trimming Assessment","text":"<ul> <li>Quality Metrics Evaluation: Comprehensive quality assessment post-trimming  </li> <li>Read Length Distributions: Analysis of length profiles and retention rates  </li> <li>Downstream Impact: Evaluation of trimming effects on alignment and analysis quality  </li> </ul>"},{"location":"Fastq/Trimming/#conclusion","title":"Conclusion","text":"<p>Trimmomatic provides a robust and flexible framework for NGS read quality improvement through sophisticated trimming algorithms. Its modular architecture enables researchers to implement customized quality control strategies while maintaining high processing efficiency. When properly configured and validated, Trimmomatic significantly enhances downstream analysis quality by removing low-quality sequences and technical artifacts, ultimately contributing to more accurate and reliable genomics research outcomes.</p>"},{"location":"Fastq/rename-fastq/","title":"Renaming FASTQ Files Downloaded from the Short Read Archive (SRA)","text":"Table of Content <ul> <li>Renaming FASTQ Files Downloaded from the Short Read Archive (SRA)<ul> <li>Introduction to the Short Read Archive (SRA)</li> <li>Downloading FASTQ Files Using sra-tools</li> <li>Downloading FASTQ Files Using sra-tools on a HPC Cluster</li> </ul> </li> </ul>"},{"location":"Fastq/rename-fastq/#introduction-to-the-short-read-archive-sra","title":"Introduction to the Short Read Archive (SRA)","text":"<p>The Short Read Archive (SRA) is a public repository maintained by the National Center for Biotechnology Information (NCBI). It is a valuable resource for researchers who wish to access and reuse sequencing datasets for comparative studies, benchmarking, or meta-analyses.  </p> <p>SRA organizes sequencing data hierarchically into BioProjects, BioSamples, and Runs. A BioProject represents a broader research initiative, while each BioSample corresponds to a specific biological specimen. Under each BioSample, one or more Runs contain the actual sequencing data, typically in FASTQ format. However, the FASTQ files downloaded from SRA are named using the Run accession (e.g., SRR12345678) or sometimes the BioSample ID, which often does not reflect the actual biological or experimental sample name used in the study. This mismatch can make downstream analysis and interpretation difficult.  </p> <p>Fortunately, a CSV file is usually provided alongside the dataset, mapping each Run or BioSample to its corresponding Sample Name\u2014a more meaningful and human-readable identifier. This allows users to rename the FASTQ files accordingly, improving clarity and traceability in data processing pipelines.</p>"},{"location":"Fastq/rename-fastq/#downloading-fastq-files-using-sra-tools","title":"Downloading FASTQ Files Using <code>sra-tools</code>","text":""},{"location":"Fastq/rename-fastq/#download-one-fastq-file","title":"Download one Fastq File","text":"<p>To retrieve sequencing data from SRA, the <code>sra-tools</code> suite is commonly used. One of its key utilities is <code>fasterq-dump</code>, which efficiently converts SRA accessions into FASTQ files.</p> <p>Here is an example command to download paired-end reads:</p> <pre><code>prefetch SRR12345678\nfasterq-dump SRR12345678 --split-files --gzip\n</code></pre> <p>You don\u2019t have to use prefetch before fasterq-dump but:</p> <ul> <li>Using prefetch is more stable for large downloads or on clusters. </li> <li>It Allows you to download .sra files in advance and convert them later. </li> <li>The conversion is faster and the recovery is easier if something goes wrong.</li> </ul> <p>This command downloads and compresses the paired-end reads into two files: - <code>SRR12345678_1.fastq.gz</code> - <code>SRR12345678_2.fastq.gz</code> </p>"},{"location":"Fastq/rename-fastq/#download-an-entire-bioproject","title":"Download an entire BioProject","text":"<p>SRA Accession List This list helps you downloding a batch of Fastq Files: 1. Go the NCBI Short Read Archive web site. 2. Check for a BioProject (ie: PRJNA484104). As a result, there is a list of BioSamples. Just click on the first results and select the BioPreject value in <code>Study</code> item. 3. Click on \"<code>send to:</code>\" on the right corner and select \"<code>Choose Destination</code>\" -&gt; <code>File</code>, \"<code>Format</code>\" -&gt; <code>Accession List</code>. Then click on the \"<code>Create File</code>\" button. 4. Move the <code>SraAccList.csv</code> list in the project directory, where you want to store the Fastq files.  </p> <p>RunInfo Table 5. From the Bioproject page, click on the Number of Links of SRA Experiments tab (in the Project Data summary table).  6. On the SRA page, click on the <code>Send to:</code> on the right corner. 7. Select <code>File</code> and choose <code>RunInfo</code>, then click <code>Create File</code>. 8. Move the <code>SRA_Run_Table.txt</code> list in the project directory, where you want to store the Fastq files. </p>"},{"location":"Fastq/rename-fastq/#download-runs","title":"Download Runs","text":"<ol> <li>Use prefetch command to download the Runs: <pre><code>prefetch --max-size 100G --option-file SraAccList.csv\n</code></pre></li> <li>The download is starting.</li> </ol>"},{"location":"Fastq/rename-fastq/#unzip-sra-data","title":"Unzip SRA Data","text":"<pre><code>for file in $(cat SraAccList.csv); do fasterq-dump --split-files $file done\n</code></pre>"},{"location":"Fastq/rename-fastq/#renaming-fastq-files-based-on-a-csv-file","title":"Renaming FASTQ Files Based on a CSV File","text":"<p>After downloading, it is often useful to rename the FASTQ files using meaningful sample names. The following Bash script automates this process using the .csv file present in the Bioproject that maps SRA run accessions to sample names:</p> <pre><code>#!/bin/bash\n\n# Prompt for the CSV file name\nread -p \"Please enter the name and the path of the CSV file: \" csv_file\n\n# Check if the CSV file exists\nif [ ! -f \"$csv_file\" ]; then\n    echo \"The file $csv_file does not exist.\"\n    exit 1\nfi\n\n# Read the CSV and rename files\nwhile IFS=';' read -r Run SampleName _\ndo\n    if [ -f \"${Run}_1.fastq.gz\" ]; then\n        new_filename=\"${SampleName}_1.fastq.gz\"\n        mv \"${Run}_1.fastq.gz\" \"$new_filename\"\n        echo \"File ${Run}_1.fastq.gz renamed in $new_filename\"\n    elif [ -f \"${Run}_2.fastq.gz\" ]; then\n        new_filename=\"${SampleName}_2.fastq.gz\"\n        mv \"${Run}_2.fastq.gz\" \"$new_filename\"\n        echo \"File ${Run}_2.fastq.gz renamed in $new_filename\"\n    else\n        echo \"The file ${Run}_1.fastq.gz or ${Run}_2.fastq.gz dos not exist.\"\n    fi\ndone &lt; \"$csv_file\"\n</code></pre>"},{"location":"Fastq/rename-fastq/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<ol> <li> <p>Prompt for CSV Input: The script asks the user to enter the name of a CSV file containing SRA run IDs and sample names.</p> </li> <li> <p>File Existence Check: It verifies that the specified CSV file exists before proceeding.</p> </li> <li> <p>CSV Parsing: The script reads the CSV line by line, assuming fields are separated by semicolons (<code>;</code>). It extracts the SRA run ID (<code>Run</code>) and the corresponding sample name (<code>SampleName</code>).</p> </li> <li> <p>File Renaming:</p> </li> <li>If a file named <code>${Run}_1.fastq.gz</code> exists, it is renamed to <code>${SampleName}_1.fastq.gz</code>.</li> <li>If a file named <code>${Run}_2.fastq.gz</code> exists, it is renamed to <code>${SampleName}_2.fastq.gz</code>.</li> <li>If neither file exists, a warning message is printed.</li> </ol> <p>This script is particularly useful when working with large datasets where manual renaming would be time-consuming and error-prone.</p>"},{"location":"Fastq/rename-fastq/#downloading-fastq-files-using-sra-tools-on-a-hpc-cluster","title":"Downloading FASTQ Files Using <code>sra-tools</code> on a HPC Cluster","text":""},{"location":"Fastq/rename-fastq/#download-an-entire-bioproject_1","title":"Download an entire BioProject","text":"<p>SRA Accession List This list helps you downloding a batch of Fastq Files: 1. Go the NCBI Short Read Archive web site. 2. Check for a BioProject (ie: PRJNA484104). As a result, there is a list of BioSamples. Just click on the first results and select the BioPreject value in <code>Study</code> item. 3. Click on \"<code>send to:</code>\" on the right corner and select \"<code>Choose Destination</code>\" -&gt; <code>File</code>, \"<code>Format</code>\" -&gt; <code>Accession List</code>. Then click on the \"<code>Create File</code>\" button. 4. Upload the <code>SraAccList.csv</code> list in the project directory, where you want to store the Fastq files.  </p> <p>RunInfo Table 5. From the Bioproject page, click on the Number of Links of SRA Experiments tab (in the summary table).  6. On the SRA page, click on the <code>Send to:</code> on the right corner. 7. Select <code>File</code> and choose <code>RunInfo</code>, then click <code>Create File</code>. 8. Upload the <code>SRA_Run_Table.txt</code> list in the project directory, where you want to store the Fastq files. </p>"},{"location":"Fastq/rename-fastq/#batch-data-transfer","title":"Batch Data Transfer","text":"<ol> <li>On the HPC cluster, go the the raw data directory of the project directory.</li> <li>Load the sra-tools module and enter in the configuration mode:  <pre><code>module load sra-tools/3.1.1\nvdb-config --interactive\n</code></pre></li> <li>On the <code>Tools</code> tab, select <code>prefetch downloads to * current directory</code>.</li> <li>Choose \"<code>s</code>\" to save and \"<code>x</code>\" to exit config.</li> </ol>"},{"location":"Fastq/rename-fastq/#download-runs_1","title":"Download Runs","text":"<ol> <li>Use prefetch command to download the Runs: <pre><code>prefetch --max-size 100G --option-file SraAccList.csv\n</code></pre></li> <li>The download is starting.</li> </ol>"},{"location":"Fastq/rename-fastq/#unzip-sra-data_1","title":"Unzip SRA Data","text":"<pre><code>for file in $(cat SraAccList.csv); do fasterq-dump --split-files $file done\n</code></pre>"},{"location":"Fastq/rename-fastq/#rename-fastq-files","title":"Rename Fastq Files","text":"<p>Use the <code>SRA_Run_Table.txt</code> and the following script to rename automatically the Fastq files:</p> <pre><code>#!/bin/bash\n\n# Prompt for the CSV file name\nread -p \"Please enter the name and the path of the CSV file: \" csv_file\n\n# Check if the CSV file exists\nif [ ! -f \"$csv_file\" ]; then\n    echo \"The file $csv_file does not exist.\"\n    exit 1\nfi\n\n# Read the CSV and rename files\nwhile IFS=';' read -r Run SampleName _\ndo\n    if [ -f \"${Run}_1.fastq.gz\" ]; then\n        new_filename=\"${SampleName}_1.fastq.gz\"\n        mv \"${Run}_1.fastq.gz\" \"$new_filename\"\n        echo \"File ${Run}_1.fastq.gz renamed in $new_filename\"\n    elif [ -f \"${Run}_2.fastq.gz\" ]; then\n        new_filename=\"${SampleName}_2.fastq.gz\"\n        mv \"${Run}_2.fastq.gz\" \"$new_filename\"\n        echo \"File ${Run}_2.fastq.gz renamed in $new_filename\"\n    else\n        echo \"The file ${Run}_1.fastq.gz or ${Run}_2.fastq.gz dos not exist.\"\n    fi\ndone &lt; \"$csv_file\"\n</code></pre>"},{"location":"Vcf/Vcf/","title":"The VCF Format: A Comprehensive Guide","text":"Table of Content <ul> <li>The VCF Format: A Comprehensive Guide<ul> <li>Introduction</li> <li>VCF Format Structure</li> <li>Detailed Column Specifications</li> <li>Genotype Representation</li> <li>Variant Types in VCF</li> <li>Quality Metrics and Annotations</li> <li>Advanced VCF Features</li> <li>VCF Validation and Standards</li> <li>Tools and Software</li> <li>Best Practices</li> <li>Emerging Developments</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Vcf/Vcf/#introduction","title":"Introduction","text":"<p>The Variant Call Format (VCF) is the standard file format for storing genomic variant data in bioinformatics. Originally developed by the 1000 Genomes Project, VCF has become the universal language for representing genetic variations including single nucleotide polymorphisms (SNPs), insertions and deletions (indels), and structural variants. This standardized format enables interoperability between different variant calling tools, databases, and analysis pipelines.</p> <p>The VCF format addresses the fundamental challenge of representing complex genomic variations in a structured, machine-readable format while maintaining human readability. Its widespread adoption across the genomics community has made it an essential component of modern genomic analysis workflows.</p>"},{"location":"Vcf/Vcf/#vcf-format-structure","title":"VCF Format Structure","text":""},{"location":"Vcf/Vcf/#file-organization","title":"File Organization","text":"<p>VCF files are organized into two main sections:</p> <ol> <li>Header section: Contains metadata about the file and data format</li> <li>Data section: Contains the actual variant records</li> </ol>"},{"location":"Vcf/Vcf/#header-section","title":"Header Section","text":"<p>The header section begins with lines starting with <code>##</code> and contains essential metadata:</p> <pre><code>##fileformat=VCFv4.2\n##fileDate=20231015\n##source=GATK4.2.6.1\n##reference=hg38.fa\n##contig=&lt;ID=chr1,length=248956422&gt;\n##contig=&lt;ID=chr2,length=242193529&gt;\n##INFO=&lt;ID=AC,Number=A,Type=Integer,Description=\"Allele count\"&gt;\n##INFO=&lt;ID=AF,Number=A,Type=Float,Description=\"Allele Frequency\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth\"&gt;\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  Sample1 Sample2\n</code></pre>"},{"location":"Vcf/Vcf/#key-header-elements","title":"Key Header Elements","text":"<ul> <li>fileformat: Specifies the VCF version (e.g., VCFv4.2, VCFv4.3)</li> <li>fileDate: Date of file creation</li> <li>source: Software used to generate the file</li> <li>reference: Reference genome used for variant calling</li> <li>contig: Chromosome/contig information with lengths</li> <li>INFO: Definitions of information fields</li> <li>FORMAT: Definitions of sample-specific format fields</li> </ul>"},{"location":"Vcf/Vcf/#data-section","title":"Data Section","text":"<p>The data section contains tab-delimited records with the following mandatory columns:</p> <ol> <li>CHROM: Chromosome or contig identifier</li> <li>POS: Position on the chromosome (1-based)</li> <li>ID: Variant identifier (often rsID from dbSNP)</li> <li>REF: Reference allele sequence</li> <li>ALT: Alternative allele sequence(s)</li> <li>QUAL: Quality score for the variant</li> <li>FILTER: Filter status (PASS or filter codes)</li> <li>INFO: Additional variant information</li> <li>FORMAT: Format of sample-specific data</li> <li>Sample columns: One column per sample with genotype data</li> </ol>"},{"location":"Vcf/Vcf/#detailed-column-specifications","title":"Detailed Column Specifications","text":""},{"location":"Vcf/Vcf/#chrom-column","title":"CHROM Column","text":"<p>The chromosome column specifies the reference sequence identifier:</p> <pre><code>chr1, chr2, ..., chrX, chrY, chrM\n</code></pre> <p>Standards and conventions: - Human genome: Typically \"chr1\", \"chr2\", etc., or \"1\", \"2\", etc. - Model organisms: Species-specific naming conventions - Consistency: Must match reference genome contig names - Special chromosomes: Sex chromosomes (X, Y) and mitochondrial (M/MT)</p>"},{"location":"Vcf/Vcf/#pos-column","title":"POS Column","text":"<p>Position indicates the reference coordinate of the variant:</p> <ul> <li>1-based indexing: First base of chromosome is position 1</li> <li>Reference position: For SNPs, the position of the variant base</li> <li>Indel positioning: For insertions/deletions, the position before the first altered base</li> <li>Coordinate system: Must be consistent with reference genome</li> </ul>"},{"location":"Vcf/Vcf/#id-column","title":"ID Column","text":"<p>Variant identifier provides database cross-references:</p> <pre><code>rs123456789    # dbSNP identifier\n.              # No identifier available\nCOSV123456     # COSMIC identifier\n</code></pre> <p>Common identifier sources: - dbSNP: rs identifiers for known SNPs - COSMIC: Somatic mutation identifiers - ClinVar: Clinical variant identifiers - Custom: Project-specific identifiers</p>"},{"location":"Vcf/Vcf/#ref-and-alt-columns","title":"REF and ALT Columns","text":"<p>Reference and alternative alleles define the variant:</p>"},{"location":"Vcf/Vcf/#snp-examples","title":"SNP Examples","text":"<pre><code>REF: A    ALT: G     # A to G transition\nREF: C    ALT: T     # C to T transition\n</code></pre>"},{"location":"Vcf/Vcf/#indel-examples","title":"Indel Examples","text":"<pre><code>REF: GATC   ALT: G       # 3-base deletion\nREF: G      ALT: GATC    # 3-base insertion\nREF: ATG    ALT: CCC     # Complex substitution\n</code></pre>"},{"location":"Vcf/Vcf/#multiple-alternatives","title":"Multiple Alternatives","text":"<pre><code>REF: A    ALT: G,T     # Biallelic site (A\u2192G or A\u2192T)\nREF: C    ALT: G,T,A   # Triallelic site\n</code></pre>"},{"location":"Vcf/Vcf/#qual-column","title":"QUAL Column","text":"<p>Quality score represents confidence in the variant call:</p> <ul> <li>Phred scale: -10 \u00d7 log\u2081\u2080(probability of error)</li> <li>Higher values: Greater confidence in variant call</li> <li>Typical range: 0-1000+ (practical upper limit varies)</li> <li>Interpretation: QUAL=30 means 99.9% confidence</li> </ul>"},{"location":"Vcf/Vcf/#filter-column","title":"FILTER Column","text":"<p>Filter status indicates quality assessment results:</p> <pre><code>PASS           # Variant passes all filters\nLowQual        # Low quality variant\nFAIL           # Failed quality filters\n.              # No filters applied\n</code></pre> <p>Common filter codes: - PASS: Variant meets all quality criteria - LowQual: Below quality threshold - LowGQ: Low genotype quality - LowDP: Insufficient read depth - StrandBias: Evidence of strand bias</p>"},{"location":"Vcf/Vcf/#info-column","title":"INFO Column","text":"<p>The INFO field contains semicolon-separated key-value pairs:</p> <pre><code>AC=5;AF=0.357;AN=14;DP=255;FS=0.723;MQ=60.0;QD=25.4\n</code></pre>"},{"location":"Vcf/Vcf/#common-info-fields","title":"Common INFO Fields","text":"<ul> <li>AC: Allele count in genotypes</li> <li>AF: Allele frequency</li> <li>AN: Total number of alleles in called genotypes</li> <li>DP: Approximate read depth</li> <li>FS: Fisher strand bias test</li> <li>MQ: Root mean square mapping quality</li> <li>QD: Variant confidence/quality by depth</li> </ul>"},{"location":"Vcf/Vcf/#format-column","title":"FORMAT Column","text":"<p>The FORMAT field specifies the structure of sample-specific data:</p> <pre><code>GT:AD:DP:GQ:PL\n</code></pre>"},{"location":"Vcf/Vcf/#standard-format-fields","title":"Standard FORMAT Fields","text":"<ul> <li>GT: Genotype (0/0, 0/1, 1/1, etc.)</li> <li>AD: Allelic depths for REF and ALT alleles</li> <li>DP: Approximate read depth</li> <li>GQ: Genotype quality</li> <li>PL: Phred-scaled genotype likelihoods</li> </ul>"},{"location":"Vcf/Vcf/#sample-columns","title":"Sample Columns","text":"<p>Sample columns contain colon-separated values matching the FORMAT specification:</p> <pre><code>Sample1: 0/1:15,12:27:99:350,0,450\nSample2: 1/1:2,25:27:75:750,225,0\n</code></pre>"},{"location":"Vcf/Vcf/#genotype-representation","title":"Genotype Representation","text":""},{"location":"Vcf/Vcf/#genotype-encoding","title":"Genotype Encoding","text":"<p>Genotypes are represented using allele indices:</p> <ul> <li>0: Reference allele</li> <li>1: First alternative allele</li> <li>2: Second alternative allele</li> <li>N: Nth alternative allele</li> </ul>"},{"location":"Vcf/Vcf/#diploid-examples","title":"Diploid Examples","text":"<pre><code>0/0  or  0|0    # Homozygous reference\n0/1  or  0|1    # Heterozygous\n1/1  or  1|1    # Homozygous alternative\n1/2  or  1|2    # Heterozygous with two alternative alleles\n</code></pre>"},{"location":"Vcf/Vcf/#phasing-information","title":"Phasing Information","text":"<p>The separator indicates phasing status:</p> <ul> <li>/ (forward slash): Unphased genotype</li> <li>| (pipe): Phased genotype</li> <li>Phased: Alleles are ordered by parental chromosome</li> <li>Unphased: Allele order is arbitrary</li> </ul>"},{"location":"Vcf/Vcf/#special-genotype-cases","title":"Special Genotype Cases","text":""},{"location":"Vcf/Vcf/#missing-data","title":"Missing Data","text":"<pre><code>./.  or  .|.    # Missing genotype\n.    or  .      # Missing allele\n</code></pre>"},{"location":"Vcf/Vcf/#polyploid-organisms","title":"Polyploid Organisms","text":"<pre><code>0/0/1    # Triploid genotype\n0/1/1/2  # Tetraploid genotype\n</code></pre>"},{"location":"Vcf/Vcf/#variant-types-in-vcf","title":"Variant Types in VCF","text":""},{"location":"Vcf/Vcf/#single-nucleotide-polymorphisms-snps","title":"Single Nucleotide Polymorphisms (SNPs)","text":"<p>SNPs are the simplest variant type:</p> <pre><code>chr1    123456    rs123    A    G    60    PASS    AF=0.5    GT:DP:GQ    0/1:30:99\n</code></pre> <p>Characteristics: - REF and ALT: Single nucleotide each - Position: Exact position of the variant - Transitions: A\u2194G, C\u2194T (more common) - Transversions: A\u2194C, A\u2194T, G\u2194C, G\u2194T (less common)</p>"},{"location":"Vcf/Vcf/#insertions-and-deletions-indels","title":"Insertions and Deletions (Indels)","text":"<p>Indels require special representation:</p>"},{"location":"Vcf/Vcf/#insertion-example","title":"Insertion Example","text":"<pre><code>chr1    123456    .    G    GATC    60    PASS    AF=0.3    GT:DP:GQ    0/1:25:95\n</code></pre>"},{"location":"Vcf/Vcf/#deletion-example","title":"Deletion Example","text":"<pre><code>chr1    123456    .    GATC    G    60    PASS    AF=0.3    GT:DP:GQ    0/1:25:95\n</code></pre> <p>Key principles: - Left normalization: Indels are represented at the leftmost possible position - Padding base: Always include at least one matching base - Minimal representation: Use the shortest possible representation</p>"},{"location":"Vcf/Vcf/#complex-variants","title":"Complex Variants","text":""},{"location":"Vcf/Vcf/#multi-nucleotide-polymorphisms-mnps","title":"Multi-nucleotide Polymorphisms (MNPs)","text":"<pre><code>chr1    123456    .    ATG    GCC    60    PASS    AF=0.2    GT:DP:GQ    0/1:28:90\n</code></pre>"},{"location":"Vcf/Vcf/#block-substitutions","title":"Block Substitutions","text":"<pre><code>chr1    123456    .    ATCG    GCTA    60    PASS    AF=0.1    GT:DP:GQ    0/1:32:85\n</code></pre>"},{"location":"Vcf/Vcf/#quality-metrics-and-annotations","title":"Quality Metrics and Annotations","text":""},{"location":"Vcf/Vcf/#variant-quality-assessment","title":"Variant Quality Assessment","text":""},{"location":"Vcf/Vcf/#quality-score-qual","title":"Quality Score (QUAL)","text":"<p>The QUAL field provides overall confidence in the variant call:</p> <ul> <li>Calculation: Based on likelihood ratios</li> <li>Scale: Phred-scaled probability</li> <li>Interpretation: Higher values indicate greater confidence</li> <li>Thresholds: Project-specific quality cutoffs</li> </ul>"},{"location":"Vcf/Vcf/#genotype-quality-gq","title":"Genotype Quality (GQ)","text":"<p>Individual genotype confidence scores:</p> <ul> <li>Per-sample metric: Confidence in assigned genotype</li> <li>Calculation: Difference between most and second-most likely genotype</li> <li>Range: 0-99 (capped at 99)</li> <li>Filtering: Commonly filtered at GQ \u2265 20 or GQ \u2265 30</li> </ul>"},{"location":"Vcf/Vcf/#depth-and-coverage-metrics","title":"Depth and Coverage Metrics","text":""},{"location":"Vcf/Vcf/#read-depth-dp","title":"Read Depth (DP)","text":"<p>Total sequencing depth at the variant position:</p> <ul> <li>Calculation: Sum of all reads covering the position</li> <li>Filtering: Minimum and maximum depth thresholds</li> <li>Considerations: Extremely high depth may indicate repetitive regions</li> </ul>"},{"location":"Vcf/Vcf/#allelic-depth-ad","title":"Allelic Depth (AD)","text":"<p>Read counts supporting each allele:</p> <ul> <li>Format: Reference count, alternative count(s)</li> <li>Example: AD=15,12 (15 reference reads, 12 alternative reads)</li> <li>Applications: Allelic balance assessment, contamination detection</li> </ul>"},{"location":"Vcf/Vcf/#statistical-annotations","title":"Statistical Annotations","text":""},{"location":"Vcf/Vcf/#allele-frequency-af","title":"Allele Frequency (AF)","text":"<p>Population frequency of alternative alleles:</p> <ul> <li>Calculation: AC/AN (allele count / total alleles)</li> <li>Range: 0.0 to 1.0</li> <li>Applications: Population genetics, filtering rare variants</li> </ul>"},{"location":"Vcf/Vcf/#hardy-weinberg-equilibrium","title":"Hardy-Weinberg Equilibrium","text":"<p>Statistical test for population genetics assumptions:</p> <ul> <li>HWE p-value: Probability of observing genotype frequencies</li> <li>Deviations: May indicate population structure or technical issues</li> <li>Filtering: Commonly filter variants with HWE p &lt; 0.001</li> </ul>"},{"location":"Vcf/Vcf/#advanced-vcf-features","title":"Advanced VCF Features","text":""},{"location":"Vcf/Vcf/#structural-variants","title":"Structural Variants","text":"<p>Large-scale genomic rearrangements require special handling:</p>"},{"location":"Vcf/Vcf/#deletion","title":"Deletion","text":"<pre><code>chr1    123456    .    N    &lt;DEL&gt;    60    PASS    SVTYPE=DEL;SVLEN=-5000;END=128456\n</code></pre>"},{"location":"Vcf/Vcf/#insertion","title":"Insertion","text":"<pre><code>chr1    123456    .    N    &lt;INS&gt;    60    PASS    SVTYPE=INS;SVLEN=5000\n</code></pre>"},{"location":"Vcf/Vcf/#breakend-notation","title":"Breakend Notation","text":"<pre><code>chr1    123456    .    G    G]chr2:234567]    60    PASS    SVTYPE=BND\n</code></pre>"},{"location":"Vcf/Vcf/#symbolic-alleles","title":"Symbolic Alleles","text":"<p>Special notation for complex variants:</p> <ul> <li>: Deletion <li>: Insertion <li>: Duplication <li>: Copy number variant <li>: Inversion"},{"location":"Vcf/Vcf/#multi-sample-considerations","title":"Multi-sample Considerations","text":""},{"location":"Vcf/Vcf/#population-level-information","title":"Population-level Information","text":"<pre><code>##INFO=&lt;ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes\"&gt;\n##INFO=&lt;ID=AF,Number=A,Type=Float,Description=\"Allele frequency\"&gt;\n##INFO=&lt;ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles\"&gt;\n</code></pre>"},{"location":"Vcf/Vcf/#sample-specific-annotations","title":"Sample-specific Annotations","text":"<pre><code>##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=\"Allelic depths\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth\"&gt;\n</code></pre>"},{"location":"Vcf/Vcf/#vcf-validation-and-standards","title":"VCF Validation and Standards","text":""},{"location":"Vcf/Vcf/#format-compliance","title":"Format Compliance","text":"<p>VCF files must adhere to specification standards:</p>"},{"location":"Vcf/Vcf/#header-validation","title":"Header Validation","text":"<ul> <li>Required fields: fileformat, contig information</li> <li>Field definitions: All INFO and FORMAT fields must be defined</li> <li>Consistency: Column headers must match sample names</li> </ul>"},{"location":"Vcf/Vcf/#data-validation","title":"Data Validation","text":"<ul> <li>Coordinate consistency: Positions must be valid for reference genome</li> <li>Allele representation: REF alleles must match reference sequence</li> <li>Format compliance: Sample data must match FORMAT specification</li> </ul>"},{"location":"Vcf/Vcf/#common-validation-issues","title":"Common Validation Issues","text":""},{"location":"Vcf/Vcf/#coordinate-problems","title":"Coordinate Problems","text":"<ul> <li>Invalid positions: Positions beyond chromosome length</li> <li>Zero-based indexing: Accidentally using 0-based coordinates</li> <li>Negative positions: Invalid coordinate values</li> </ul>"},{"location":"Vcf/Vcf/#allele-representation-issues","title":"Allele Representation Issues","text":"<ul> <li>Reference mismatches: REF allele doesn't match reference genome</li> <li>Improper normalization: Indels not left-normalized</li> <li>Missing padding: Indels without required padding bases</li> </ul>"},{"location":"Vcf/Vcf/#tools-and-software","title":"Tools and Software","text":""},{"location":"Vcf/Vcf/#vcf-manipulation-tools","title":"VCF Manipulation Tools","text":""},{"location":"Vcf/Vcf/#vcftools","title":"VCFtools","text":"<p>Comprehensive toolkit for VCF file manipulation:</p> <pre><code># Basic statistics\nvcftools --vcf input.vcf --freq --out output\n\n# Filtering by quality\nvcftools --vcf input.vcf --minQ 30 --recode --out filtered\n\n# Population genetics statistics\nvcftools --vcf input.vcf --het --out heterozygosity\n</code></pre>"},{"location":"Vcf/Vcf/#bcftools","title":"BCFtools","text":"<p>High-performance VCF processing:</p> <pre><code># Query specific regions\nbcftools query -r chr1:1000000-2000000 input.vcf.gz\n\n# Merge VCF files\nbcftools merge file1.vcf.gz file2.vcf.gz -O z -o merged.vcf.gz\n\n# Convert to BCF format\nbcftools view input.vcf -O b -o output.bcf\n</code></pre>"},{"location":"Vcf/Vcf/#gatk-tools","title":"GATK Tools","text":"<p>Specialized variant processing:</p> <pre><code># Select variants\ngatk SelectVariants -V input.vcf -O snps.vcf --select-type SNP\n\n# Filter variants\ngatk VariantFiltration -V input.vcf -O filtered.vcf --filter-expression \"QD &lt; 2.0\"\n</code></pre>"},{"location":"Vcf/Vcf/#format-conversion","title":"Format Conversion","text":""},{"location":"Vcf/Vcf/#vcf-to-other-formats","title":"VCF to Other Formats","text":"<pre><code># VCF to PLINK\nvcftools --vcf input.vcf --plink --out plink_format\n\n# VCF to BED\nvcftools --vcf input.vcf --bed --out bed_format\n\n# VCF to HapMap\nvcftools --vcf input.vcf --hapmap --out hapmap_format\n</code></pre>"},{"location":"Vcf/Vcf/#best-practices","title":"Best Practices","text":""},{"location":"Vcf/Vcf/#file-organization_1","title":"File Organization","text":""},{"location":"Vcf/Vcf/#compression-and-indexing","title":"Compression and Indexing","text":"<ul> <li>Compression: Use bgzip for block compression</li> <li>Indexing: Create tabix indices for efficient access</li> <li>Storage: Compressed files reduce storage requirements</li> </ul> <pre><code># Compress and index\nbgzip input.vcf\ntabix -p vcf input.vcf.gz\n</code></pre>"},{"location":"Vcf/Vcf/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Descriptive names: Include sample information, date, and processing steps</li> <li>Version control: Track different analysis versions</li> <li>Documentation: Maintain metadata about file contents</li> </ul>"},{"location":"Vcf/Vcf/#quality-control","title":"Quality Control","text":""},{"location":"Vcf/Vcf/#pre-processing-checks","title":"Pre-processing Checks","text":"<ul> <li>Format validation: Verify VCF specification compliance</li> <li>Reference consistency: Ensure reference genome compatibility</li> <li>Sample integrity: Verify expected samples are present</li> </ul>"},{"location":"Vcf/Vcf/#post-processing-validation","title":"Post-processing Validation","text":"<ul> <li>Variant counts: Check expected variant numbers</li> <li>Quality distributions: Examine quality score patterns</li> <li>Allele frequencies: Verify reasonable population genetics parameters</li> </ul>"},{"location":"Vcf/Vcf/#data-management","title":"Data Management","text":""},{"location":"Vcf/Vcf/#backup-strategies","title":"Backup Strategies","text":"<ul> <li>Multiple copies: Maintain redundant copies of important files</li> <li>Version control: Track analysis iterations</li> <li>Documentation: Record processing steps and parameters</li> </ul>"},{"location":"Vcf/Vcf/#access-control","title":"Access Control","text":"<ul> <li>Permissions: Appropriate file system permissions</li> <li>Sharing: Secure methods for data sharing</li> <li>Privacy: Compliance with data protection regulations</li> </ul>"},{"location":"Vcf/Vcf/#emerging-developments","title":"Emerging Developments","text":""},{"location":"Vcf/Vcf/#vcf-specification-evolution","title":"VCF Specification Evolution","text":""},{"location":"Vcf/Vcf/#recent-updates","title":"Recent Updates","text":"<ul> <li>VCFv4.3: Latest specification with enhanced features</li> <li>Structural variants: Improved representation of complex variants</li> <li>Metadata: Enhanced header information standards</li> </ul>"},{"location":"Vcf/Vcf/#future-directions","title":"Future Directions","text":"<ul> <li>Cloud integration: Improved cloud storage compatibility</li> <li>Streaming formats: Efficient processing of large files</li> <li>Compression: Advanced compression algorithms</li> </ul>"},{"location":"Vcf/Vcf/#integration-with-other-formats","title":"Integration with Other Formats","text":""},{"location":"Vcf/Vcf/#genomic-data-standards","title":"Genomic Data Standards","text":"<ul> <li>HGVS: Human Genome Variation Society nomenclature</li> <li>FHIR: Fast Healthcare Interoperability Resources</li> <li>GA4GH: Global Alliance for Genomics and Health standards</li> </ul>"},{"location":"Vcf/Vcf/#interoperability","title":"Interoperability","text":"<ul> <li>API standards: Programmatic access to variant data</li> <li>Cloud platforms: Integration with cloud genomics services</li> <li>Database connectivity: Direct database import/export</li> </ul>"},{"location":"Vcf/Vcf/#conclusion","title":"Conclusion","text":"<p>The VCF format represents a fundamental standard in genomics that enables interoperability, reproducibility, and scalability in variant analysis. Its comprehensive structure accommodates diverse variant types while maintaining computational efficiency and human readability.</p> <p>Understanding VCF format intricacies is essential for genomics researchers, bioinformaticians, and clinicians working with genomic variant data. Proper implementation of VCF standards ensures data quality, facilitates collaboration, and enables integration with the broader genomics ecosystem.</p> <p>As genomics continues to evolve toward larger datasets and more complex analyses, the VCF format will undoubtedly continue to adapt and improve, maintaining its central role in genomic data representation and exchange. Mastery of VCF format principles provides a solid foundation for current genomics work and future developments in the field.</p>"},{"location":"Vcf/accessibility/","title":"VCF Filtering by Genomic Accessibility","text":"Table of Content <ul> <li>VCF Filtering by Genomic Accessibility<ul> <li>Overview</li> <li>Genomic Accessibility: Euchromatin vs. Heterochromatin</li> <li>Accessibility Masks</li> <li>Implementation</li> <li>Key Parameters and Workflow</li> <li>Benefits of Accessibility Filtering</li> <li>Considerations and Limitations</li> <li>Best Practices</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Vcf/accessibility/#overview","title":"Overview","text":"<p>Variant Call Format (VCF) filtering by genomic accessibility is a critical quality control step in genomic analysis pipelines. This process involves restricting variant calls to regions of the genome that are reliably sequenced and assembled, thereby improving the accuracy and reliability of downstream analyses.</p>"},{"location":"Vcf/accessibility/#genomic-accessibility-euchromatin-vs-heterochromatin","title":"Genomic Accessibility: Euchromatin vs. Heterochromatin","text":""},{"location":"Vcf/accessibility/#euchromatic-regions","title":"Euchromatic Regions","text":"<p>Euchromatic regions represent the transcriptionally active, gene-rich portions of the genome characterized by:</p> <ul> <li>Open chromatin structure: Loosely packed DNA that is accessible to sequencing technologies</li> <li>High mappability: Unique sequences that allow for unambiguous read alignment</li> <li>Consistent sequencing coverage: Uniform depth distribution across the region</li> <li>Low repetitive content: Minimal presence of tandem repeats, transposable elements, and segmental duplications</li> </ul>"},{"location":"Vcf/accessibility/#heterochromatic-regions","title":"Heterochromatic Regions","text":"<p>Heterochromatic regions pose significant challenges for sequencing and variant calling due to:</p> <ul> <li>Condensed chromatin structure: Tightly packed DNA that may be inaccessible to sequencing enzymes</li> <li>Repetitive sequences: High density of repetitive elements that cause mapping ambiguity</li> <li>Segmental duplications: Large duplicated sequences (&gt;1kb, &gt;90% identity) that lead to misalignment</li> <li>Centromeric and pericentromeric regions: Highly repetitive sequences that are difficult to assemble</li> <li>Heterozygous deletions: Regions with copy number variations affecting sequencing depth</li> </ul>"},{"location":"Vcf/accessibility/#accessibility-masks","title":"Accessibility Masks","text":"<p>Accessibility masks are binary genomic tracks that define high-confidence regions for variant calling. These masks are typically generated by:</p> <ol> <li>Mappability analysis: Identifying regions where short reads can be uniquely mapped</li> <li>Coverage uniformity: Excluding regions with extreme coverage variations</li> <li>Repeat masking: Filtering out known repetitive elements and low-complexity sequences</li> <li>Manual curation: Removing problematic regions identified through empirical analysis</li> </ol>"},{"location":"Vcf/accessibility/#implementation","title":"Implementation","text":"<p>The following SLURM script demonstrates VCF filtering using bcftools with an accessibility mask:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=16:00:00\n#SBATCH --job-name=accmerge\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16G\n#SBATCH --array=1-5\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chrom.list)\nINPUT_DIR=\"vcfs/\"\n\n# R\u00e9pertoire de sortie pour les fichiers\nOUTPUT_DIR=\"vcfs_filtered/\"\nmkdir -p ${OUTPUT_DIR}\n\n# Load module\nmodule load bcftools/1.15.1\n\nbcftools view --threads 8 \\\n         ${INPUT_DIR}/combined.chr${CHROM}.vcf.gz \\\n         -T /shared/projects/invalbo/bwambae/Accessibility/accessibility.pass.${CHROM}.vcf.gz \\\n         -Oz -o ${OUTPUT_DIR}/combined.chr${CHROM}.acc.vcf.gz\n\nbcftools index --threads 8 ${OUTPUT_DIR}/combined.chr${CHROM}.acc.vcf.gz\n\nend_time=$(date +%s)  # Capture l'heure de fin\nduration=$((end_time - start_time))  # Calcule la dur\u00e9e\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/accessibility/#key-parameters-and-workflow","title":"Key Parameters and Workflow","text":""},{"location":"Vcf/accessibility/#bcftools-parameters","title":"BCFtools Parameters","text":"<ul> <li><code>--threads 8</code>: Utilizes 8 CPU cores for parallel processing</li> <li><code>-T</code>: Specifies the target regions file (accessibility mask)</li> <li><code>-Oz</code>: Outputs compressed VCF format</li> <li><code>-o</code>: Defines the output file path</li> </ul>"},{"location":"Vcf/accessibility/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Chromosome selection: The script processes chromosomes sequentially using SLURM array jobs</li> <li>Region filtering: BCFtools applies the accessibility mask to retain only high-confidence regions</li> <li>Compression and indexing: The filtered VCF is compressed and indexed for efficient downstream access</li> <li>Quality control: Runtime monitoring ensures successful completion</li> </ol>"},{"location":"Vcf/accessibility/#benefits-of-accessibility-filtering","title":"Benefits of Accessibility Filtering","text":""},{"location":"Vcf/accessibility/#improved-variant-quality","title":"Improved Variant Quality","text":"<ul> <li>Reduced false positives: Eliminates spurious variants from problematic genomic regions</li> <li>Enhanced specificity: Increases confidence in variant calls by focusing on reliable regions</li> <li>Consistent genotyping: Provides uniform variant calling across samples</li> </ul>"},{"location":"Vcf/accessibility/#downstream-analysis-advantages","title":"Downstream Analysis Advantages","text":"<ul> <li>Population genetics: More accurate allele frequency estimates and demographic inferences</li> <li>Association studies: Reduced noise in genome-wide association studies (GWAS)</li> <li>Phylogenetic analysis: Improved resolution for evolutionary studies</li> </ul>"},{"location":"Vcf/accessibility/#considerations-and-limitations","title":"Considerations and Limitations","text":""},{"location":"Vcf/accessibility/#data-loss","title":"Data Loss","text":"<p>Accessibility filtering removes variants from potentially important genomic regions, including: - Regulatory elements in heterochromatin - Structural variant breakpoints - Copy number variable regions  </p>"},{"location":"Vcf/accessibility/#mask-selection","title":"Mask Selection","text":"<p>The choice of accessibility mask significantly impacts results and should be: - Species-specific: Tailored to the organism's genome characteristics - Technology-appropriate: Matched to the sequencing platform and read length - Analysis-specific: Customized for the research question and population studied  </p>"},{"location":"Vcf/accessibility/#best-practices","title":"Best Practices","text":"<ol> <li>Document mask provenance: Clearly record the source and generation method of accessibility masks  </li> <li>Validate filtering impact: Assess the proportion of variants retained and their genomic distribution  </li> <li>Consider analysis goals: Balance between data quality and genomic coverage based on research objectives  </li> <li>Benchmark performance: Compare results with and without accessibility filtering to understand the impact  </li> </ol>"},{"location":"Vcf/accessibility/#conclusion","title":"Conclusion","text":"<p>VCF filtering by genomic accessibility represents a fundamental quality control measure that enhances the reliability of variant calling by restricting analysis to high-confidence genomic regions. While this approach may reduce genomic coverage, it significantly improves the accuracy of downstream analyses by eliminating variants from problematic regions where sequencing artifacts are common. The implementation of accessibility filtering should be carefully considered based on the specific research goals and the characteristics of the study population.</p>"},{"location":"Vcf/concatenate/","title":"Concatenate Variants and Invariants into a Unified VCF File","text":"Table of Content <ul> <li>Concatenate Variants and Invariants into a Unified VCF File</li> </ul> <p>After separate processing and filtering of variant and invariant genomic sites, a critical step in the variant calling pipeline is to recombine these data into a single VCF file per chromosome. </p> <p>This step is referred to as concatenation, where two VCF files\u2014typically from the same set of individuals but representing different sets of genomic positions (e.g., variants and invariants)\u2014are stacked vertically, combining their records into a single file. This contrasts with a merge, which combines VCFs from different samples or individuals, aligning them by genomic position to produce a unified multisample VCF. </p> <p>This unified file enables consistent downstream analysis across all genomic positions, regardless of their variant status. </p> <p>To achieve this, a concatenation step is performed using <code>bcftools concat</code>, followed by sorting the concatenated VCF by genomic coordinates with <code>bcftools sort</code>. This ensures that the resulting file is both complete and properly ordered for compatibility with standard genomic analysis tools.</p> <p>Below is the SLURM batch script used to perform these operations in parallel across chromosomes on an HPC cluster:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=01:00:00\n#SBATCH --job-name=cctABAG\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16G\n#SBATCH --array=1-5\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Retrieve the chromosome name from the list using the SLURM array index\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chroms_list)\n\n# Define the output directory\nOUTPUT_DIR=results/05_Variants\nLOG_DIR=\"results/11_Reports/concat\"\nmkdir -p ${LOG_DIR}\n\n# Load bcftools module\nmodule load bcftools/1.15.1\n\n# Index both variant and invariant VCFs\nbcftools index --threads 8 ${OUTPUT_DIR}/AB.AG.HC.acc.VARIANTS.flag.pass.gq20.${CHROM}.vcf.gz\nbcftools index --threads 8 ${OUTPUT_DIR}/AB.AG.HC.acc.INVARIANTS.flag.pass.${CHROM}.vcf.gz\n\n# Concatenate the two VCF files (variants and invariants)\nbcftools concat --threads 8 \\\n        ${OUTPUT_DIR}/AB.AG.HC.acc.VARIANTS.flag.pass.gq20.${CHROM}.vcf.gz \\\n        ${OUTPUT_DIR}/AB.AG.HC.acc.INVARIANTS.flag.pass.${CHROM}.vcf.gz \\\n        --allow-overlaps \\\n        -Oz -o ${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.${CHROM}.vcf.gz \\\n        2&gt;&amp;1 &gt;&gt; ${LOG_DIR}/AB.AG.HC.acc.pass.gq20.${CHROM}.log\n\n# Sort the concatenated VCF by genomic position\nbcftools sort -T tmp/ \\\n        ${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.${CHROM}.vcf.gz \\\n        -Oz -o ${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.sort.${CHROM}.vcf.gz \\\n        2&gt;&amp;1 &gt;&gt; ${LOG_DIR}/AB.AG.HC.acc.pass.gq20.sort.${CHROM}.log\n\n# Clean up the unsorted merged file\nrm ${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.${CHROM}.vcf.gz\n\n# Index the final sorted VCF\nbcftools index --threads 8 ${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.sort.${CHROM}.vcf.gz\n</code></pre>"},{"location":"Vcf/genotype-quality/","title":"Filtering VCF Files by Genotype Quality (GQ)","text":"Table of Content <ul> <li>Filtering VCF Files by Genotype Quality (GQ)<ul> <li>Genotype Quality (GQ)</li> <li>Filtering a VCF File by GQ</li> <li>Filtering Criteria Analysis</li> </ul> </li> </ul>"},{"location":"Vcf/genotype-quality/#genotype-quality-gq","title":"Genotype Quality (GQ)","text":"<p>Genotype Quality (<code>GQ</code>) is a crucial metric in variant calling that estimates the confidence in the genotype assigned to a sample at a given site. GQ values are phred-scaled, meaning that higher values reflect a lower probability of genotyping errors. Filtering variants based on GQ allows for the exclusion of low-confidence genotype calls, which helps improve the reliability of downstream analyses such as population genetics or genotype-phenotype associations.</p> <p>In a VCF file, the GQ value is found in the sample-specific <code>FORMAT</code> fields, typically following the GT (Genotype) field. Here\u2019s an example of a simplified VCF line with the GQ value highlighted:</p> <pre><code>#CHROM   POS     ID      REF     ALT     QUAL    FILTER  INFO                    FORMAT            Sample1\n2L       123456  .       A       G       99      PASS    DP=520;AF=0.5           GT:GQ:DP          0/1:35:520\n</code></pre> In this example: <code>The FORMAT field is GT:GQ:DP</code> indicating that each sample entry will provide values for Genotype (GT), Genotype Quality (GQ), and Read Depth (DP).   <code>For Sample1, the corresponding values are 0/1:35:520, which means:</code> GT = 0/1 (heterozygous),   GQ = 35 \u2192 this is the genotype quality,   DP = 520 (number of reads covering the site).   <p>Thus, the GQ value (35) indicates a reasonably high confidence in the assigned heterozygous genotype at that position.</p>"},{"location":"Vcf/genotype-quality/#filtering-a-vcf-file-by-gq","title":"Filtering a VCF File by GQ","text":"<p>In this project, a GQ threshold of 20 was applied to retain only genotypes with a high confidence level (i.e., an error probability of less than 1%). The filtering was performed using <code>VCFtools</code>, a widely used utility for processing VCF files. The script below illustrates how this filtering step was implemented for each chromosome on a high-performance computing cluster using SLURM:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=00:45:00\n#SBATCH --job-name=GQABAG\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8G\n#SBATCH --array=2\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chroms_list)\n\n# R\u00e9pertoire de sortie pour les fichiers\nOUTPUT_DIR=results/05_Variants\n\nLOG_DIR=\"results/11_Reports/gq20\"\nmkdir -p ${LOG_DIR}\n\n# Load module\nmodule load vcftools/0.1.16 htslib/1.14\n\nvcftools --gzvcf ${OUTPUT_DIR}/AB.AG.HC.acc.VARIANTS.flag.pass.${CHROM}.vcf.gz --minGQ 20 --recode --stdout | bgzip -c &gt; ${OUTPUT_DIR}/AB.AG.HC.acc.VARIANTS.flag.pass.gq20.${CHROM}.vcf.gz\n\n\nend_time=$(date +%s)  # Capture l'heure de fin\nduration=$((end_time - start_time))  # Calcule la dur\u00e9e\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/genotype-quality/#filtering-criteria-analysis","title":"Filtering Criteria Analysis","text":"<ul> <li>--gzvcf: Specifies the path to the input compressed VCF file (.vcf.gz). Here, it processes a variant file specific to one chromosome (${CHROM}), already filtered by flags (e.g., DP, MQ, FS).</li> <li>--minGQ 20: Filters out genotypes with GQ &lt; 20, retaining only high-confidence genotypes (error probability &lt; 1%).</li> <li>--recode: Instructs VCFtools to reconstruct a valid VCF file with only the remaining entries after filtering.</li> <li>--stdout: Sends the output to standard output, allowing piping to another program.</li> <li>bgzip -c: Compresses the output using bgzip (a block compression tool compatible with tabix indexing).</li> <li>&gt; ...vcf.gz: Redirects the compressed output to a new VCF file, named to reflect that it includes only variants with GQ \u2265 20.</li> </ul>"},{"location":"Vcf/gt-indv/","title":"Genotype and Individual-Based Missing Data Filtering in VCF Files","text":"Table of Content <ul> <li>Genotype and Individual-Based Missing Data Filtering in VCF Files<ul> <li>Horizontal Filtering: Site-Level Missing Data</li> <li>Vertical Filtering: Individual-Level Missing Data</li> <li>Conclusion</li> </ul> </li> </ul> <p>Accurate variant analysis requires not only high-confidence genotypes but also careful handling of missing data. Two complementary filtering strategies are commonly applied to Variant Call Format (VCF) files to improve data quality and downstream interpretability: site-based (horizontal) filtering and individual-based (vertical) filtering.</p>"},{"location":"Vcf/gt-indv/#horizontal-filtering-site-level-missing-data","title":"Horizontal Filtering: Site-Level Missing Data","text":"<p>This type of filtering focuses on the genomic positions (sites) themselves. It removes sites for which a defined by user proportion of samples lack genotype calls. Excessive missingness at a site may result from poor alignment, low coverage, or systematic sequencing issues. Filtering such positions helps reduce noise and avoid biases in population-level statistics. </p> <p>In this project, we retained only those positions where at least 80% of sites had a valid genotype call.</p> <p>The script below shows how this filtering step was performed per chromosome using <code>vcftools</code>:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=02:30:00\n#SBATCH --job-name=GTsABAG\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8G\n#SBATCH --array=1-5\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chroms_list)\nOUTPUT_DIR=results/05_Variants\n\nmodule load vcftools/0.1.16 htslib/1.14\n\nvcftools \\\n --gzvcf ${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.sort.${CHROM}.vcf.gz \\\n --max-missing 0.8 \\\n --recode \\\n --recode-INFO-all \\\n --stdout \\\n | bgzip -c &gt; ${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.gt80.${CHROM}.vcf.gz \\\n &amp;&amp; tabix ${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.gt80.${CHROM}.vcf.gz\n\nend_time=$(date +%s)\nduration=$((end_time - start_time))\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/gt-indv/#vertical-filtering-individual-level-missing-data","title":"Vertical Filtering: Individual-Level Missing Data","text":"<p>Once low-quality sites are removed, the next step is to evaluate the quality of each sample (individual) based on the proportion of missing genotype calls across retained sites. Samples with extensive missing data may reflect poor sequencing, contamination, or technical failure. Removing such individuals prevents them from skewing population genetic inferences or association analyses. Here, we excluded individuals with more than 70% missing genotypes.</p> <p>The script below identifies and filters out these low-quality individuals:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=02:00:00\n#SBATCH --job-name=ifltABAG\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8G\n#SBATCH --array=1-5\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chroms_list)\nOUTPUT_DIR=results/05_Variants\nLOG_DIR=\"results/11_Reports/ind70\"\nmkdir -p ${LOG_DIR}\n\nmodule load vcftools/0.1.16 htslib/1.14\n\nSUBSET_VCF=${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.gt80.${CHROM}.vcf.gz\nOUT=${OUTPUT_DIR}/AB.AG.HC.vcf.subset.${CHROM}.indv\n\n# Calculate missingness per individual\nvcftools \\\n --gzvcf ${SUBSET_VCF} \\\n --missing-indv \\\n --out ${OUT}\n\n# Identify individuals with &gt;70% missing data\nmawk '$5 &gt; \"0.7\"' ${OUT}.imiss | cut -f1 &gt; ${OUTPUT_DIR}/AB.AG.HC.lowDP.indv\n\nVCF_IN=${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.gt80.${CHROM}.vcf.gz\nVCF_OUT=${OUTPUT_DIR}/AB.AG.HC.acc.pass.gq20.gt80.ind70.${CHROM}.vcf.gz\n\n# Remove those individuals from the VCF\nvcftools --gzvcf ${VCF_IN} \\\n        --remove-indels \\\n        --remove ${OUTPUT_DIR}/AB.AG.HC.lowDP.indv \\\n        --recode --stdout | bgzip -c &gt; \\\n        ${VCF_OUT}\n\nend_time=$(date +%s)\nduration=$((end_time - start_time))\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/gt-indv/#conclusion","title":"Conclusion","text":"<p>Together, these two layers of missing data filtering\u2014horizontal (site-based) and vertical (individual-based)\u2014enhance the overall reliability of the genotype dataset. This is especially important in large-scale studies where incomplete or uneven data can lead to biases in genetic diversity estimates, population structure, or association mapping. Filtering thresholds (e.g., 80% completeness per site and 70% per individual) should be selected based on study design, sample size, and sequencing depth.</p>"},{"location":"Vcf/homopolymer/","title":"Homopolymer Run Filtering in VCF Files","text":"Table of Content <ul> <li>Homopolymer Run Filtering in VCF Files<ul> <li>Context: Post-Variant Calling Quality Control</li> <li>Understanding Homopolymer Runs</li> <li>Technical Challenges in Homopolymer Regions</li> <li>Rationale for Homopolymer Run Filtering</li> <li>Implementation Workflow</li> <li>Technical Implementation Details</li> <li>Best Practices and Considerations</li> <li>Limitations and Considerations</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Vcf/homopolymer/#context-post-variant-calling-quality-control","title":"Context: Post-Variant Calling Quality Control","text":"<p>This filtering step is particularly relevant in the context of post-variant calling quality control, especially when variants have been called using <code>legacy tools such as GATK's UnifiedGenotyper</code>. While modern variant callers like HaplotypeCaller incorporate sophisticated algorithms that better handle problematic genomic regions including homopolymer runs during the variant calling process, UnifiedGenotyper produces inferior results for indel calling compared to HaplotypeCaller, necessitating additional post-processing steps for homopolymer-associated variants.</p>"},{"location":"Vcf/homopolymer/#understanding-homopolymer-runs","title":"Understanding Homopolymer Runs","text":""},{"location":"Vcf/homopolymer/#definition-and-characteristics","title":"Definition and Characteristics","text":"<p>A homopolymer run (also called homopolymer tract) is a genomic sequence consisting of consecutive identical nucleotides. These sequences are characterized by:</p> <ul> <li>Repetitive nucleotide composition: Stretches of single nucleotides (e.g., AAAAA, TTTTTT, GGGGGG, CCCCCC)</li> <li>Variable length: Typically ranging from 3 to over 20 consecutive identical bases</li> <li>Genome-wide distribution: Present throughout all genomic regions but with varying frequencies</li> <li>Sequence context dependency: Often flanked by complex secondary structures</li> </ul>"},{"location":"Vcf/homopolymer/#examples-of-homopolymer-runs","title":"Examples of Homopolymer Runs","text":"<pre><code>Reference: ATCG[AAAAA]TGCAT\nVariant:   ATCG[AAAAAA]TGCAT  (insertion of one A)\nVariant:   ATCG[AAAA]TGCAT    (deletion of one A)\n</code></pre>"},{"location":"Vcf/homopolymer/#technical-challenges-in-homopolymer-regions","title":"Technical Challenges in Homopolymer Regions","text":""},{"location":"Vcf/homopolymer/#sequencing-technology-limitations","title":"Sequencing Technology Limitations","text":"<p>Illumina Sequencing Issues: - Phasing errors: Synchronization loss during sequencing-by-synthesis - Signal intensity variations: Difficulty in accurately measuring fluorescent signals - Cluster density effects: Overlapping signals from adjacent clusters  </p> <p>Ion Torrent Specific Problems: - Homopolymer length miscalls: Inherent limitation of pH-based detection - Systematic length bias: Tendency to under-call or over-call homopolymer lengths - Flow signal saturation: Signal plateau effects in long homopolymer runs  </p>"},{"location":"Vcf/homopolymer/#bioinformatics-challenges","title":"Bioinformatics Challenges","text":"<p>Alignment Artifacts: - Mapping ambiguity: Multiple valid alignments for reads spanning homopolymer regions - Indel calling errors: Misinterpretation of sequencing errors as true variants - Strand bias: Asymmetric representation of variants on forward and reverse strands  </p> <p>Variant Calling Complications: - False positive indels: Sequencing errors interpreted as genuine insertions/deletions - Allelic imbalance: Preferential amplification of certain alleles - Genotyping errors: Incorrect assignment of homozygous vs. heterozygous states  </p>"},{"location":"Vcf/homopolymer/#rationale-for-homopolymer-run-filtering","title":"Rationale for Homopolymer Run Filtering","text":""},{"location":"Vcf/homopolymer/#quality-control-necessity","title":"Quality Control Necessity","text":"<p>The removal of variants in homopolymer runs is justified by several factors:</p> <ol> <li>High false positive rate: Homopolymer regions exhibit elevated rates of spurious variant calls</li> <li>Systematic sequencing bias: Technology-specific artifacts that cannot be easily corrected</li> <li>Reduced genotyping confidence: Lower quality scores and unreliable allele frequency estimates</li> <li>Downstream analysis interference: Contamination of population genetic and association studies</li> </ol>"},{"location":"Vcf/homopolymer/#impact-on-variant-interpretation","title":"Impact on Variant Interpretation","text":"<p>Clinical Implications: - Diagnostic accuracy: Reduced false positive rates in medical genetic testing - Treatment decisions: More reliable variant classification for therapeutic targeting - Genetic counseling: Improved confidence in variant pathogenicity assessment  </p> <p>Research Applications: - Population genetics: Cleaner datasets for demographic inference - Genome-wide association studies: Reduced noise in association testing - Evolutionary genomics: More accurate phylogenetic reconstructions  </p>"},{"location":"Vcf/homopolymer/#implementation-workflow","title":"Implementation Workflow","text":"<p>The following SLURM script demonstrates the annotation and identification of homopolymer runs using GATK tools:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=11-10:00:00\n#SBATCH --job-name=hmplmr\n#SBATCH -p long\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16G\n#SBATCH --array=2\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chrom.list)\nINPUT_DIR=\"vcfs_filtered/\"\n\n# R\u00e9pertoire de sortie pour les fichiers\nOUTPUT_DIR=\"vcfs_flt_HRun\"\nmkdir -p ${OUTPUT_DIR}\n\n# Load module\nmodule load gatk4/4.2.6.1\nmodule load gatk/3.8\n\ngatk IndexFeatureFile -I ${INPUT_DIR}/combined.chr${CHROM}.acc.vcf.gz\n\ngatk3 -T VariantAnnotator \\\n    -V ${INPUT_DIR}/combined.chr${CHROM}.acc.vcf.gz \\\n    -R /shared/projects/invalbo/bwambae/resources/genome/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa \\\n    -A HomopolymerRun \\\n    -o ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.vcf.gz\n\nend_time=$(date +%s)\nduration=$((end_time - start_time))\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/homopolymer/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"Vcf/homopolymer/#gatk-variantannotator-parameters","title":"GATK VariantAnnotator Parameters","text":"<p>Core Arguments: - <code>-T VariantAnnotator</code>: Specifies the GATK tool for adding annotations to variants - <code>-V</code>: Input VCF file path (already filtered by accessibility) - <code>-R</code>: Reference genome file in FASTA format - <code>-A HomopolymerRun</code>: Annotation module that identifies homopolymer contexts - <code>-o</code>: Output VCF file with homopolymer annotations  </p> <p>Resource Requirements: - Memory allocation: 16GB to handle large VCF files and reference genome - CPU utilization: 8 cores for parallel processing - Runtime: Extended time allocation (11 days) for large-scale genomic data  </p>"},{"location":"Vcf/homopolymer/#homopolymerrun-annotation","title":"HomopolymerRun Annotation","text":"<p>The <code>HomopolymerRun</code> annotation adds the following information to the VCF INFO field:</p> <ul> <li>HRun: Length of the longest homopolymer run in the variant context  </li> <li>Context window: Typically examines \u00b110 bases around the variant position  </li> <li>Strand consideration: Evaluates both forward and reverse strand contexts  </li> </ul>"},{"location":"Vcf/homopolymer/#subsequent-filtering-steps","title":"Subsequent Filtering Steps","text":"<p>After annotation, variants can be filtered based on homopolymer length thresholds:</p> <pre><code># Example filtering command (not in original script)\ngatk VariantFiltration \\\n    -V combined.chr${CHROM}.acc.HRun.vcf.gz \\\n    -filter \"HRun &gt; 4\" \\\n    --filter-name \"HomopolymerRun\" \\\n    -O combined.chr${CHROM}.acc.HRun.filtered.vcf.gz\n</code></pre>"},{"location":"Vcf/homopolymer/#best-practices-and-considerations","title":"Best Practices and Considerations","text":""},{"location":"Vcf/homopolymer/#threshold-selection","title":"Threshold Selection","text":"<p>Conservative Approach (HRun &gt; 3): - Removes most homopolymer-associated artifacts - Maintains high variant quality - May remove some true variants  </p> <p>Moderate Approach (HRun &gt; 5): - Balances quality and sensitivity - Suitable for most research applications - Commonly used in population studies  </p> <p>Lenient Approach (HRun &gt; 7): - Retains more variants for analysis - Higher false positive rate - Requires additional quality control  </p>"},{"location":"Vcf/homopolymer/#validation-strategies","title":"Validation Strategies","text":"<ol> <li>Sanger sequencing confirmation: Validate a subset of filtered variants  </li> <li>Comparison with high-quality datasets: Benchmark against reference populations  </li> <li>Technology-specific validation: Use orthogonal sequencing platforms  </li> <li>Functional annotation: Assess impact on protein-coding regions  </li> </ol>"},{"location":"Vcf/homopolymer/#limitations-and-considerations","title":"Limitations and Considerations","text":""},{"location":"Vcf/homopolymer/#potential-data-loss","title":"Potential Data Loss","text":"<p>Functional Impact: - Removal of true variants in homopolymer regions - Loss of clinically relevant mutations - Reduced power for association studies  </p> <p>Genomic Context: - Regulatory elements within homopolymer tracts - Evolutionary important variations - Species-specific adaptation signals  </p>"},{"location":"Vcf/homopolymer/#alternative-approaches","title":"Alternative Approaches","text":"<p>Improved Sequencing Technologies: - Long-read sequencing (PacBio, Oxford Nanopore) - Linked-read technologies (10x Genomics) - Hybrid assembly approaches  </p> <p>Advanced Bioinformatics Methods: - Machine learning-based variant calling - Consensus calling from multiple algorithms - Probabilistic models for homopolymer regions  </p>"},{"location":"Vcf/homopolymer/#conclusion","title":"Conclusion","text":"<p>Homopolymer run filtering represents a critical quality control step in variant calling pipelines, particularly for data generated using legacy tools or technologies prone to homopolymer-associated artifacts. While this filtering approach may result in the loss of some true variants, it significantly improves the overall quality and reliability of variant calls by removing systematic biases and false positives. The implementation should be tailored to the specific sequencing technology, analysis objectives, and downstream applications, with careful consideration of the trade-offs between sensitivity and specificity.</p> <p>Future developments in sequencing technologies and bioinformatics algorithms continue to improve the accuracy of variant calling in challenging genomic regions, potentially reducing the need for such aggressive filtering approaches. However, for current standard practices, homopolymer run filtering remains an essential component of robust variant calling pipelines.</p>"},{"location":"Vcf/variants-invariants/","title":"Variant and Invariant Site Filtering for Population Genomics","text":"Table of Content <ul> <li>Variant and Invariant Site Filtering for Population Genomics<ul> <li>Overview</li> <li>Rationale for Variant-Invariant Separation</li> <li>Implementation Workflow</li> <li>Population Genomics Applications</li> <li>Best Practices and Recommendations</li> <li>Limitations and Considerations</li> <li>Future Developments</li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"Vcf/variants-invariants/#overview","title":"Overview","text":"<p>In population genomics analyses, the accurate characterization of genetic diversity requires comprehensive assessment of both polymorphic (variant) and monomorphic (invariant) sites across the genome. Traditional variant calling pipelines focus primarily on identifying and filtering variable positions, often discarding invariant sites that are crucial for calculating accurate population genetic parameters such as nucleotide diversity (\u03c0), Tajima's D, and demographic inference statistics.</p>"},{"location":"Vcf/variants-invariants/#rationale-for-variant-invariant-separation","title":"Rationale for Variant-Invariant Separation","text":""},{"location":"Vcf/variants-invariants/#the-importance-of-invariant-sites","title":"The Importance of Invariant Sites","text":"<p>Invariant sites represent genomic positions where all sequenced individuals share the same allele. These sites are essential for:</p> <p>Population Genetic Statistics: - Nucleotide diversity calculations: Accurate \u03c0 estimates require knowledge of both polymorphic and monomorphic sites - Site frequency spectrum: Complete allele frequency distributions need invariant site counts - Demographic inference: Coalescent-based methods require total sequence length information - Selection analysis: Identifying regions under purifying selection through reduced diversity  </p> <p>Comparative Genomics: - Divergence estimates: Calculating substitution rates between populations or species - Conservation analysis: Identifying functionally constrained genomic regions - Mutation rate estimation: Determining background mutation rates across different genomic contexts  </p>"},{"location":"Vcf/variants-invariants/#differential-filtering-requirements","title":"Differential Filtering Requirements","text":"<p>Variants and invariants require distinct filtering strategies due to their fundamental differences:  </p> <p>Variant Sites: - Higher susceptibility to technical artifacts - Complex allelic configurations requiring sophisticated quality metrics - Strand bias and mapping quality considerations - Allele balance and Hardy-Weinberg equilibrium assessment  </p> <p>Invariant Sites: - Simpler quality assessment based primarily on coverage depth - Lower risk of systematic bias - Focused on sequencing completeness rather than allelic accuracy - Reduced computational complexity for quality control  </p>"},{"location":"Vcf/variants-invariants/#implementation-workflow","title":"Implementation Workflow","text":""},{"location":"Vcf/variants-invariants/#step-1-variant-and-invariant-separation","title":"Step 1: Variant and Invariant Separation","text":"<p>The initial step involves separating variant and invariant sites into distinct VCF files for independent processing:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=12:00:00\n#SBATCH --job-name=var_invar\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16G\n#SBATCH --array=1-4\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chrom.list)\n\n# R\u00e9pertoire de sortie pour les fichiers\nINPUT_DIR=\"vcfs_flt_HRun\"\n\nOUTPUT_DIR=\"vcfs_flt_HRun_slct_vrt\"\nmkdir -p ${OUTPUT_DIR}\n\nLOG_DIR=\"logs\"\nmkdir -p ${LOG_DIR}\n\n# Load module\nmodule load gatk4/4.2.6.1\n\ngatk IndexFeatureFile -I ${INPUT_DIR}/combined.chr${CHROM}.acc.HRun.vcf.gz\n\ngatk SelectVariants \\\n -V ${INPUT_DIR}/combined.chr${CHROM}.acc.HRun.vcf.gz \\\n -select-type SNP \\\n -xl-select-type INDEL \\\n -O ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.VARIANTS.vcf.gz \\\n 2&gt;&amp;1 &gt;&gt; ${LOG_DIR}/combined.chr${CHROM}.acc.HRun.VARIANTS.log\n\ngatk SelectVariants \\\n -V ${INPUT_DIR}/combined.chr${CHROM}.acc.HRun.vcf.gz \\\n -select-type NO_VARIATION \\\n -xl-select-type INDEL \\\n -O ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.INVARIANTS.vcf.gz \\\n 2&gt;&amp;1 &gt;&gt; ${LOG_DIR}/combined.chr${CHROM}.acc.HRun.INVARIANTS.log\n\nend_time=$(date +%s)  # Capture l'heure de fin\nduration=$((end_time - start_time))  # Calcule la dur\u00e9e\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/variants-invariants/#technical-details","title":"Technical Details","text":"<p>GATK SelectVariants Parameters: - <code>-select-type SNP</code>: Selects only single nucleotide polymorphisms for variant file - <code>-select-type NO_VARIATION</code>: Selects invariant sites for invariant file - <code>-xl-select-type INDEL</code>: Excludes insertion/deletion variants from both outputs - <code>-V</code>: Input VCF file path - <code>-O</code>: Output VCF file path  </p> <p>File Organization: - Separate logging for variants and invariants facilitates troubleshooting - Chromosome-specific processing enables parallel execution - Compressed VCF output reduces storage requirements  </p>"},{"location":"Vcf/variants-invariants/#step-2-intermediate-statistics-generation","title":"Step 2: Intermediate Statistics Generation","text":"<p>Quality assessment is performed on both variant and invariant datasets to inform filtering decisions:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=01:30:00\n#SBATCH --job-name=stats\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8G\n#SBATCH --array=1-5\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chrom.list)\n\n# R\u00e9pertoire de sortie pour les fichiers\nOUTPUT_DIR=\"vcfs_flt_HRun_slct_vrt\"\n\n# Load module\nmodule load bcftools/1.15.1\n\nbcftools stats ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.INVARIANTS.vcf.gz &gt; ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.INVARIANTS.stats\n\nbcftools stats ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.VARIANTS.vcf.gz &gt; ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.VARIANTS.stats\n\nend_time=$(date +%s)  # Capture l'heure de fin\nduration=$((end_time - start_time))  # Calcule la dur\u00e9e\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/variants-invariants/#statistical-analysis","title":"Statistical Analysis","text":"<p>BCFtools stats output includes: - Site counts: Total number of variants and invariants - Quality distributions: QUAL score histograms for threshold selection - Depth statistics: Coverage depth distributions across sites - Allele frequency spectra: Distribution of minor allele frequencies (variants only) - Transition/transversion ratios: Mutation pattern assessment (variants only)  </p>"},{"location":"Vcf/variants-invariants/#step-3-differential-filtering-and-flagging","title":"Step 3: Differential Filtering and Flagging","text":"<p>Variants and invariants are subjected to distinct filtering criteria reflecting their different quality requirements.</p> <p>Before applying any filtering criteria, it is interesting to examine the distribution of DP (depth of coverage) values in each VCF file. This data-driven approach helps determine appropriate thresholds for filtering both variant and invariant sites, ensuring that quality filters are adapted to the characteristics of the dataset.</p> <pre><code>pip install pysam numpy\nimport pysam\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load VCF file\nvcf_file = \"combined.chr2L.acc.HRun.VARIANTS.vcf.gz\"\n\n# Define intervals\nbins = list(range(0, 1551, 50))\ninterval_labels = [f\"{bins[i]}-{bins[i+1]-1}\" for i in range(len(bins)-1)]\n\n# Initialize dict to count frequencies\ndp_counts = {label: 0 for label in interval_labels}\n\n# Read VCF file\nwith pysam.VariantFile(vcf_file, \"r\") as vcf:\n    for record in vcf:\n        dp = record.info.get(\"DP\")\n        if dp is not None:\n            for i, (low, high) in enumerate(zip(bins[:-1], bins[1:])):\n                if low &lt;= dp &lt; high:\n                    dp_counts[interval_labels[i]] += 1\n                    break\n\n# Prepare data for graph\nintervals = list(dp_counts.keys())\nfrequencies = list(dp_counts.values())\n\n# Draw graph\nplt.figure(figsize=(12, 6))\nplt.bar(intervals, frequencies, color=\"skyblue\", edgecolor=\"black\")\nplt.xticks(rotation=45)\nplt.xlabel(\"Intervalle de DP\")\nplt.ylabel(\"Fr\u00e9quence\")\nplt.title(\"Distribution des valeurs de DP dans le champ INFO\")\nplt.tight_layout()\n\n# Save and show graph\nplt.savefig(\"combined.HRun.acc.VARIANTS.2L.distribution_DP.png\")\nplt.show()\n</code></pre> <p>Variants and invariants are filtered using different criteria, reflecting their distinct error profiles and interpretation requirements. For example, thresholds for MQ, FS, and DP are applied only to variants, while a simpler DP filter is applied to invariant sites. The script below performs these filtering steps using GATK\u2019s VariantFiltration after indexing the input VCFs. The DP&lt;300 cutoff, among others, was chosen based on the DP distribution plots generated with the Python script:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=01:00:00\n#SBATCH --job-name=vfiltABAG\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=16G\n#SBATCH --array=2\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Les VCFs ont \u00e9t\u00e9 filtr\u00e9s par accessibilit\u00e9. Il est donc inutile de filtrer les HighDP.\n\nstart_time=$(date +%s)\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chrom.list)\n\n# R\u00e9pertoire de sortie pour les fichiers\nOUTPUT_DIR=\"vcfs_flt_HRun_slct_vrt\"\n\nLOG_DIR=\"Cluster_logs\"\nmkdir -p ${LOG_DIR}\n\n# Load module\nmodule load gatk4/4.2.6.1\n\ngatk IndexFeatureFile -I ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.VARIANTS.vcf.gz\ngatk IndexFeatureFile -I ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.INVARIANTS.vcf.gz\n\ngatk VariantFiltration \\\n -R resources/genome/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa \\\n -O ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.VARIANTS.flag.vcf.gz \\\n -V ${OUTPUT_DIR}/combined.chr${CHROM}.acc.HRun.VARIANTS.vcf.gz \\\n --filter-expression \"MQ&lt;40.0\" \\\n --filter-name \"MQfilter\" \\\n --filter-expression \"DP&lt;300\" \\\n --filter-name \"LowDP\" \\\n --filter-expression \"FS&gt;60.0\" \\\n --filter-name \"FSfilter\" \\\n --missing-values-evaluate-as-failing \\\n 2&gt;&amp;1 &gt;&gt; ${LOG_DIR}/combined.chr${CHROM}.acc.HRun.VARIANTS.flag.log\n\ngatk VariantFiltration \\\n -R resources/genome/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa \\\n -O ${OUTPUT_DIR}/AB.AG.HC.acc.INVARIANTS.flag.${CHROM}.vcf.gz \\\n -V ${OUTPUT_DIR}/AB.AG.HC.acc.INVARIANTS.${CHROM}.vcf.gz \\\n --filter-expression \"DP&lt;300\" \\\n --filter-name \"LowDP\" \\\n --missing-values-evaluate-as-failing \\\n 2&gt;&amp;1 &gt;&gt; ${LOG_DIR}/AB.AG.HC.acc.INVARIANTS.flag.${CHROM}.log\n\nend_time=$(date +%s)  # Capture l'heure de fin\nduration=$((end_time - start_time))  # Calcule la dur\u00e9e\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/variants-invariants/#filtering-criteria-analysis","title":"Filtering Criteria Analysis","text":"<p>Variant Site Filters: - MQ &lt; 40.0: Removes sites with poor mapping quality, indicating potential alignment artifacts - DP &lt; 300: Excludes undercovered sites with insufficient statistical power for genotype calling - FS &gt; 60.0: Filters sites with excessive strand bias, suggesting technical artifacts - --missing-values-evaluate-as-failing: Treats missing annotation values as failing filters  </p> <p>Invariant Site Filters: - DP &lt; 300: Applies only depth filtering, as other quality metrics are less relevant for monomorphic sites - Simplified criteria: Reflects the lower complexity of invariant site assessment  </p>"},{"location":"Vcf/variants-invariants/#quality-metric-explanations","title":"Quality Metric Explanations","text":"<p>Mapping Quality (MQ): - Root mean square of mapping qualities of reads supporting the variant - Values &lt; 40 indicate poor uniqueness of genomic position - Critical for excluding paralogous sequence alignments  </p> <p>Fisher Strand Bias (FS): - Phred-scaled probability of strand bias in variant calls - Values &gt; 60 suggest systematic sequencing artifacts - Particularly important for distinguishing true variants from technical noise  </p> <p>Depth (DP): - Total read depth across all samples at the site - Minimum threshold ensures adequate statistical power - Balances sensitivity with computational efficiency  </p>"},{"location":"Vcf/variants-invariants/#step-4-final-filtering-and-site-selection","title":"Step 4: Final Filtering and Site Selection","text":"<p>The final step removes all flagged sites, retaining only high-quality variants and invariants:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=01:00:00\n#SBATCH --job-name=snpfltABAG\n#SBATCH -p fast\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16G\n#SBATCH --array=2\n#SBATCH --output=Cluster_logs/%x-%j-%N.out\n#SBATCH --error=Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nstart_time=$(date +%s)\n\n# Retrait des SNPs n'\u00e9tant pas au statut FILTER=PASS\n\n# R\u00e9cup\u00e9ration du nom du fichier BAM \u00e0 partir du fichier de liste\nCHROM=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/chroms_list)\n\n# R\u00e9pertoire de sortie pour les fichiers\nOUTPUT_DIR=results/05_Variants\n\nLOG_DIR=\"results/11_Reports/snpfiltering\"\nmkdir -p ${LOG_DIR}\n\n# Load module\nmodule load gatk4/4.2.6.1\n\ngatk SelectVariants \\\n -V ${OUTPUT_DIR}/AB.AG.HC.acc.VARIANTS.flag.${CHROM}.vcf.gz \\\n --exclude-filtered \\\n -O ${OUTPUT_DIR}/AB.AG.HC.acc.VARIANTS.flag.pass.${CHROM}.vcf.gz \\\n 2&gt;&amp;1 &gt;&gt; ${LOG_DIR}/AB.AG.HC.acc.VARIANTS.flag.pass.${CHROM}.log\n\ngatk SelectVariants \\\n -V ${OUTPUT_DIR}//AB.AG.HC.acc.INVARIANTS.flag.${CHROM}.vcf.gz \\\n --exclude-filtered \\\n -O ${OUTPUT_DIR}/AB.AG.HC.acc.INVARIANTS.flag.pass.${CHROM}.vcf.gz \\\n 2&gt;&amp;1 &gt;&gt; ${LOG_DIR}/AB.AG.HC.acc.INVARIANTS.flag.pass.${CHROM}.log\n\nend_time=$(date +%s)  # Capture l'heure de fin\nduration=$((end_time - start_time))  # Calcule la dur\u00e9e\necho \"The script completed successfully in $((duration / 60)) minutes and $((duration % 60)) seconds.\"\n</code></pre>"},{"location":"Vcf/variants-invariants/#final-selection-parameters","title":"Final Selection Parameters","text":"<p>--exclude-filtered: Removes all sites that failed any filter criterion, retaining only sites marked as \"PASS\"  </p> <p>Quality Control Verification: Final datasets should be assessed for: - Retained site counts and genomic distribution - Quality metric distributions post-filtering - Transition/transversion ratios (variants) - Coverage uniformity across chromosomes  </p>"},{"location":"Vcf/variants-invariants/#population-genomics-applications","title":"Population Genomics Applications","text":""},{"location":"Vcf/variants-invariants/#downstream-analysis-compatibility","title":"Downstream Analysis Compatibility","text":"<p>Nucleotide Diversity Calculations: <pre><code># Example calculation using both datasets\ntotal_sites=$(bcftools view -H variants.pass.vcf.gz | wc -l) + $(bcftools view -H invariants.pass.vcf.gz | wc -l)\npolymorphic_sites=$(bcftools view -H variants.pass.vcf.gz | wc -l)\npi=$(echo \"scale=6; $polymorphic_sites / $total_sites\" | bc)\n</code></pre></p> <p>Site Frequency Spectrum Analysis: - Variants provide polymorphic site frequencies - Invariants contribute to the monomorphic class (frequency = 0 or 1) - Combined datasets enable accurate demographic inference  </p> <p>Selective Sweep Detection: - Reduced diversity in invariant sites indicates purifying selection - Elevated diversity in variant sites suggests balancing selection - Comparative analysis identifies selection signatures  </p>"},{"location":"Vcf/variants-invariants/#computational-considerations","title":"Computational Considerations","text":"<p>Storage Requirements: - Invariant sites typically outnumber variants by 10-100 fold - Compressed VCF formats essential for storage efficiency - Indexed files enable rapid region-specific access  </p> <p>Processing Efficiency: - Parallel chromosome processing reduces computational time - Separate filtering pipelines optimize resource utilization - Modular workflow facilitates parameter optimization  </p>"},{"location":"Vcf/variants-invariants/#best-practices-and-recommendations","title":"Best Practices and Recommendations","text":""},{"location":"Vcf/variants-invariants/#filter-threshold-selection","title":"Filter Threshold Selection","text":"<p>Conservative Approach: - Higher depth requirements (DP &gt; 500) for critical analyses - Stricter quality thresholds (MQ &gt; 50) for publication-quality datasets - Additional filters (BaseQRankSum, ReadPosRankSum) for variant sites  </p> <p>Moderate Approach: - Balanced sensitivity and specificity (current implementation) - Suitable for most population genetic analyses  - Adequate for demographic inference and diversity estimation  </p> <p>Lenient Approach: - Lower thresholds (DP &gt; 100) for sample-limited studies - Relaxed quality criteria for exploratory analyses   - Requires additional downstream validation  </p>"},{"location":"Vcf/variants-invariants/#quality-control-validation","title":"Quality Control Validation","text":"<p>Statistical Assessment: 1. Ti/Tv ratios: Expected values ~2.0-2.1 for human, variable for other species 2. Depth distributions: Should follow expected coverage patterns 3. Allele frequency spectra: Should match population genetic expectations 4. Genomic distribution: Uniform distribution across accessible regions  </p> <p>Comparative Analysis: - Benchmark against high-quality reference datasets - Cross-platform validation using orthogonal technologies - Population-specific optimization based on demographic history  </p>"},{"location":"Vcf/variants-invariants/#integration-strategies","title":"Integration Strategies","text":"<p>Downstream Merging: <pre><code># Combine filtered variants and invariants for analysis\nbcftools concat \\\n  variants.pass.vcf.gz \\\n  invariants.pass.vcf.gz \\\n  -a -D -Oz -o combined.filtered.vcf.gz\n</code></pre></p> <p>Analysis-Specific Subsets: - Coding regions: Enhanced filtering for functional impact assessment - Intergenic regions: Relaxed filtering for neutral evolution studies - Regulatory elements: Intermediate filtering for selection analysis  </p>"},{"location":"Vcf/variants-invariants/#limitations-and-considerations","title":"Limitations and Considerations","text":""},{"location":"Vcf/variants-invariants/#technical-limitations","title":"Technical Limitations","text":"<p>Ascertainment Bias: - Variant discovery influenced by reference genome quality - Population-specific variants may be systematically missed - Affects demographic inference accuracy  </p> <p>Coverage Heterogeneity: - Uneven sequencing depth across genomic regions - Systematic bias in variant detection sensitivity - Requires careful normalization in comparative studies  </p>"},{"location":"Vcf/variants-invariants/#biological-considerations","title":"Biological Considerations","text":"<p>Population Structure: - Filtering thresholds may need population-specific adjustment - Admixed populations require modified quality criteria - Demographic history affects optimal parameter selection  </p> <p>Evolutionary Constraints: - Functional regions may require different filtering approaches - Selective pressure influences variant quality distributions - Conservation levels affect optimal sensitivity/specificity balance  </p>"},{"location":"Vcf/variants-invariants/#future-developments","title":"Future Developments","text":""},{"location":"Vcf/variants-invariants/#methodological-improvements","title":"Methodological Improvements","text":"<p>Machine Learning Approaches: - Automated filter threshold optimization - Context-specific quality assessment - Population-adapted filtering strategies  </p> <p>Long-Read Integration: - Improved structural variant detection - Enhanced repetitive region characterization - Reduced reference bias in variant calling  </p>"},{"location":"Vcf/variants-invariants/#analytical-advances","title":"Analytical Advances","text":"<p>Comprehensive Genomic Datasets: - Integration of regulatory annotations - Functional impact prediction - Evolutionary constraint incorporation  </p> <p>Population-Specific Resources: - Ethnically diverse reference panels - Population-specific quality metrics - Adaptive filtering frameworks  </p>"},{"location":"Vcf/variants-invariants/#conclusion","title":"Conclusion","text":"<p>The separation and differential filtering of variants and invariants represents a sophisticated approach to population genomic analysis that maximizes both data quality and analytical power. By recognizing the distinct characteristics and requirements of polymorphic and monomorphic sites, this workflow enables accurate estimation of population genetic parameters while maintaining computational efficiency.</p> <p>The implementation described here provides a robust framework for population genomic studies, with modular components that can be adapted to specific research questions and sample characteristics. The careful balance between sensitivity and specificity, combined with comprehensive quality control measures, ensures the reliability of downstream analyses while preserving the statistical power necessary for demographic inference and selection analysis.</p> <p>Success in population genomics depends critically on the quality of the underlying data, and the differential treatment of variants and invariants represents a key advancement in achieving this goal. As sequencing technologies and analytical methods continue to evolve, the principles outlined in this workflow will remain essential for extracting meaningful biological insights from genomic variation data.</p>"}]}